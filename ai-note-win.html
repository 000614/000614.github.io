<html><head><meta http-equiv="Content-Type" content="text/html; charset=utf-8"/><title>ai学习笔记（win系统）</title><style>
/* cspell:disable-file */
/* webkit printing magic: print all background colors */
html {
	-webkit-print-color-adjust: exact;
}
* {
	box-sizing: border-box;
	-webkit-print-color-adjust: exact;
}

html,
body {
	margin: 0;
	padding: 0;
}
@media only screen {
	body {
		margin: 2em auto;
		max-width: 900px;
		color: rgb(55, 53, 47);
	}
}

body {
	line-height: 1.5;
	white-space: pre-wrap;
}

a,
a.visited {
	color: inherit;
	text-decoration: underline;
}

.pdf-relative-link-path {
	font-size: 80%;
	color: #444;
}

h1,
h2,
h3 {
	letter-spacing: -0.01em;
	line-height: 1.2;
	font-weight: 600;
	margin-bottom: 0;
}

.page-title {
	font-size: 2.5rem;
	font-weight: 700;
	margin-top: 0;
	margin-bottom: 0.75em;
}

h1 {
	font-size: 1.875rem;
	margin-top: 1.875rem;
}

h2 {
	font-size: 1.5rem;
	margin-top: 1.5rem;
}

h3 {
	font-size: 1.25rem;
	margin-top: 1.25rem;
}

.source {
	border: 1px solid #ddd;
	border-radius: 3px;
	padding: 1.5em;
	word-break: break-all;
}

.callout {
	border-radius: 3px;
	padding: 1rem;
}

figure {
	margin: 1.25em 0;
	page-break-inside: avoid;
}

figcaption {
	opacity: 0.5;
	font-size: 85%;
	margin-top: 0.5em;
}

mark {
	background-color: transparent;
}

.indented {
	padding-left: 1.5em;
}

hr {
	background: transparent;
	display: block;
	width: 100%;
	height: 1px;
	visibility: visible;
	border: none;
	border-bottom: 1px solid rgba(55, 53, 47, 0.09);
}

img {
	max-width: 100%;
}

@media only print {
	img {
		max-height: 100vh;
		object-fit: contain;
	}
}

@page {
	margin: 1in;
}

.collection-content {
	font-size: 0.875rem;
}

.column-list {
	display: flex;
	justify-content: space-between;
}

.column {
	padding: 0 1em;
}

.column:first-child {
	padding-left: 0;
}

.column:last-child {
	padding-right: 0;
}

.table_of_contents-item {
	display: block;
	font-size: 0.875rem;
	line-height: 1.3;
	padding: 0.125rem;
}

.table_of_contents-indent-1 {
	margin-left: 1.5rem;
}

.table_of_contents-indent-2 {
	margin-left: 3rem;
}

.table_of_contents-indent-3 {
	margin-left: 4.5rem;
}

.table_of_contents-link {
	text-decoration: none;
	opacity: 0.7;
	border-bottom: 1px solid rgba(55, 53, 47, 0.18);
}

table,
th,
td {
	border: 1px solid rgba(55, 53, 47, 0.09);
	border-collapse: collapse;
}

table {
	border-left: none;
	border-right: none;
}

th,
td {
	font-weight: normal;
	padding: 0.25em 0.5em;
	line-height: 1.5;
	min-height: 1.5em;
	text-align: left;
}

th {
	color: rgba(55, 53, 47, 0.6);
}

ol,
ul {
	margin: 0;
	margin-block-start: 0.6em;
	margin-block-end: 0.6em;
}

li > ol:first-child,
li > ul:first-child {
	margin-block-start: 0.6em;
}

ul > li {
	list-style: disc;
}

ul.to-do-list {
	padding-inline-start: 0;
}

ul.to-do-list > li {
	list-style: none;
}

.to-do-children-checked {
	text-decoration: line-through;
	opacity: 0.375;
}

ul.toggle > li {
	list-style: none;
}

ul {
	padding-inline-start: 1.7em;
}

ul > li {
	padding-left: 0.1em;
}

ol {
	padding-inline-start: 1.6em;
}

ol > li {
	padding-left: 0.2em;
}

.mono ol {
	padding-inline-start: 2em;
}

.mono ol > li {
	text-indent: -0.4em;
}

.toggle {
	padding-inline-start: 0em;
	list-style-type: none;
}

/* Indent toggle children */
.toggle > li > details {
	padding-left: 1.7em;
}

.toggle > li > details > summary {
	margin-left: -1.1em;
}

.selected-value {
	display: inline-block;
	padding: 0 0.5em;
	background: rgba(206, 205, 202, 0.5);
	border-radius: 3px;
	margin-right: 0.5em;
	margin-top: 0.3em;
	margin-bottom: 0.3em;
	white-space: nowrap;
}

.collection-title {
	display: inline-block;
	margin-right: 1em;
}

.page-description {
	margin-bottom: 2em;
}

.simple-table {
	margin-top: 1em;
	font-size: 0.875rem;
	empty-cells: show;
}
.simple-table td {
	height: 29px;
	min-width: 120px;
}

.simple-table th {
	height: 29px;
	min-width: 120px;
}

.simple-table-header-color {
	background: rgb(247, 246, 243);
	color: black;
}
.simple-table-header {
	font-weight: 500;
}

time {
	opacity: 0.5;
}

.icon {
	display: inline-block;
	max-width: 1.2em;
	max-height: 1.2em;
	text-decoration: none;
	vertical-align: text-bottom;
	margin-right: 0.5em;
}

img.icon {
	border-radius: 3px;
}

.user-icon {
	width: 1.5em;
	height: 1.5em;
	border-radius: 100%;
	margin-right: 0.5rem;
}

.user-icon-inner {
	font-size: 0.8em;
}

.text-icon {
	border: 1px solid #000;
	text-align: center;
}

.page-cover-image {
	display: block;
	object-fit: cover;
	width: 100%;
	max-height: 30vh;
}

.page-header-icon {
	font-size: 3rem;
	margin-bottom: 1rem;
}

.page-header-icon-with-cover {
	margin-top: -0.72em;
	margin-left: 0.07em;
}

.page-header-icon img {
	border-radius: 3px;
}

.link-to-page {
	margin: 1em 0;
	padding: 0;
	border: none;
	font-weight: 500;
}

p > .user {
	opacity: 0.5;
}

td > .user,
td > time {
	white-space: nowrap;
}

input[type="checkbox"] {
	transform: scale(1.5);
	margin-right: 0.6em;
	vertical-align: middle;
}

p {
	margin-top: 0.5em;
	margin-bottom: 0.5em;
}

.image {
	border: none;
	margin: 1.5em 0;
	padding: 0;
	border-radius: 0;
	text-align: center;
}

.code,
code {
	background: rgba(135, 131, 120, 0.15);
	border-radius: 3px;
	padding: 0.2em 0.4em;
	border-radius: 3px;
	font-size: 85%;
	tab-size: 2;
}

code {
	color: #eb5757;
}

.code {
	padding: 1.5em 1em;
}

.code-wrap {
	white-space: pre-wrap;
	word-break: break-all;
}

.code > code {
	background: none;
	padding: 0;
	font-size: 100%;
	color: inherit;
}

blockquote {
	font-size: 1.25em;
	margin: 1em 0;
	padding-left: 1em;
	border-left: 3px solid rgb(55, 53, 47);
}

.bookmark {
	text-decoration: none;
	max-height: 8em;
	padding: 0;
	display: flex;
	width: 100%;
	align-items: stretch;
}

.bookmark-title {
	font-size: 0.85em;
	overflow: hidden;
	text-overflow: ellipsis;
	height: 1.75em;
	white-space: nowrap;
}

.bookmark-text {
	display: flex;
	flex-direction: column;
}

.bookmark-info {
	flex: 4 1 180px;
	padding: 12px 14px 14px;
	display: flex;
	flex-direction: column;
	justify-content: space-between;
}

.bookmark-image {
	width: 33%;
	flex: 1 1 180px;
	display: block;
	position: relative;
	object-fit: cover;
	border-radius: 1px;
}

.bookmark-description {
	color: rgba(55, 53, 47, 0.6);
	font-size: 0.75em;
	overflow: hidden;
	max-height: 4.5em;
	word-break: break-word;
}

.bookmark-href {
	font-size: 0.75em;
	margin-top: 0.25em;
}

.sans { font-family: ui-sans-serif, -apple-system, BlinkMacSystemFont, "Segoe UI Variable Display", "Segoe UI", Helvetica, "Apple Color Emoji", Arial, sans-serif, "Segoe UI Emoji", "Segoe UI Symbol"; }
.code { font-family: "SFMono-Regular", Menlo, Consolas, "PT Mono", "Liberation Mono", Courier, monospace; }
.serif { font-family: Lyon-Text, Georgia, ui-serif, serif; }
.mono { font-family: iawriter-mono, Nitti, Menlo, Courier, monospace; }
.pdf .sans { font-family: Inter, ui-sans-serif, -apple-system, BlinkMacSystemFont, "Segoe UI Variable Display", "Segoe UI", Helvetica, "Apple Color Emoji", Arial, sans-serif, "Segoe UI Emoji", "Segoe UI Symbol", 'Twemoji', 'Noto Color Emoji', 'Noto Sans CJK JP'; }
.pdf:lang(zh-CN) .sans { font-family: Inter, ui-sans-serif, -apple-system, BlinkMacSystemFont, "Segoe UI Variable Display", "Segoe UI", Helvetica, "Apple Color Emoji", Arial, sans-serif, "Segoe UI Emoji", "Segoe UI Symbol", 'Twemoji', 'Noto Color Emoji', 'Noto Sans CJK SC'; }
.pdf:lang(zh-TW) .sans { font-family: Inter, ui-sans-serif, -apple-system, BlinkMacSystemFont, "Segoe UI Variable Display", "Segoe UI", Helvetica, "Apple Color Emoji", Arial, sans-serif, "Segoe UI Emoji", "Segoe UI Symbol", 'Twemoji', 'Noto Color Emoji', 'Noto Sans CJK TC'; }
.pdf:lang(ko-KR) .sans { font-family: Inter, ui-sans-serif, -apple-system, BlinkMacSystemFont, "Segoe UI Variable Display", "Segoe UI", Helvetica, "Apple Color Emoji", Arial, sans-serif, "Segoe UI Emoji", "Segoe UI Symbol", 'Twemoji', 'Noto Color Emoji', 'Noto Sans CJK KR'; }
.pdf .code { font-family: Source Code Pro, "SFMono-Regular", Menlo, Consolas, "PT Mono", "Liberation Mono", Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK JP'; }
.pdf:lang(zh-CN) .code { font-family: Source Code Pro, "SFMono-Regular", Menlo, Consolas, "PT Mono", "Liberation Mono", Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK SC'; }
.pdf:lang(zh-TW) .code { font-family: Source Code Pro, "SFMono-Regular", Menlo, Consolas, "PT Mono", "Liberation Mono", Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK TC'; }
.pdf:lang(ko-KR) .code { font-family: Source Code Pro, "SFMono-Regular", Menlo, Consolas, "PT Mono", "Liberation Mono", Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK KR'; }
.pdf .serif { font-family: PT Serif, Lyon-Text, Georgia, ui-serif, serif, 'Twemoji', 'Noto Color Emoji', 'Noto Serif CJK JP'; }
.pdf:lang(zh-CN) .serif { font-family: PT Serif, Lyon-Text, Georgia, ui-serif, serif, 'Twemoji', 'Noto Color Emoji', 'Noto Serif CJK SC'; }
.pdf:lang(zh-TW) .serif { font-family: PT Serif, Lyon-Text, Georgia, ui-serif, serif, 'Twemoji', 'Noto Color Emoji', 'Noto Serif CJK TC'; }
.pdf:lang(ko-KR) .serif { font-family: PT Serif, Lyon-Text, Georgia, ui-serif, serif, 'Twemoji', 'Noto Color Emoji', 'Noto Serif CJK KR'; }
.pdf .mono { font-family: PT Mono, iawriter-mono, Nitti, Menlo, Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK JP'; }
.pdf:lang(zh-CN) .mono { font-family: PT Mono, iawriter-mono, Nitti, Menlo, Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK SC'; }
.pdf:lang(zh-TW) .mono { font-family: PT Mono, iawriter-mono, Nitti, Menlo, Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK TC'; }
.pdf:lang(ko-KR) .mono { font-family: PT Mono, iawriter-mono, Nitti, Menlo, Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK KR'; }
.highlight-default {
	color: rgba(50, 48, 44, 1);
}
.highlight-gray {
	color: rgba(120, 119, 116, 1);
	fill: rgba(120, 119, 116, 1);
}
.highlight-brown {
	color: rgba(159, 107, 83, 1);
	fill: rgba(159, 107, 83, 1);
}
.highlight-orange {
	color: rgba(217, 115, 13, 1);
	fill: rgba(217, 115, 13, 1);
}
.highlight-yellow {
	color: rgba(203, 145, 47, 1);
	fill: rgba(203, 145, 47, 1);
}
.highlight-teal {
	color: rgba(68, 131, 97, 1);
	fill: rgba(68, 131, 97, 1);
}
.highlight-blue {
	color: rgba(51, 126, 169, 1);
	fill: rgba(51, 126, 169, 1);
}
.highlight-purple {
	color: rgba(144, 101, 176, 1);
	fill: rgba(144, 101, 176, 1);
}
.highlight-pink {
	color: rgba(193, 76, 138, 1);
	fill: rgba(193, 76, 138, 1);
}
.highlight-red {
	color: rgba(212, 76, 71, 1);
	fill: rgba(212, 76, 71, 1);
}
.highlight-default_background {
	color: rgba(50, 48, 44, 1);
}
.highlight-gray_background {
	background: rgba(248, 248, 247, 1);
}
.highlight-brown_background {
	background: rgba(244, 238, 238, 1);
}
.highlight-orange_background {
	background: rgba(251, 236, 221, 1);
}
.highlight-yellow_background {
	background: rgba(251, 243, 219, 1);
}
.highlight-teal_background {
	background: rgba(237, 243, 236, 1);
}
.highlight-blue_background {
	background: rgba(231, 243, 248, 1);
}
.highlight-purple_background {
	background: rgba(248, 243, 252, 1);
}
.highlight-pink_background {
	background: rgba(252, 241, 246, 1);
}
.highlight-red_background {
	background: rgba(253, 235, 236, 1);
}
.block-color-default {
	color: inherit;
	fill: inherit;
}
.block-color-gray {
	color: rgba(120, 119, 116, 1);
	fill: rgba(120, 119, 116, 1);
}
.block-color-brown {
	color: rgba(159, 107, 83, 1);
	fill: rgba(159, 107, 83, 1);
}
.block-color-orange {
	color: rgba(217, 115, 13, 1);
	fill: rgba(217, 115, 13, 1);
}
.block-color-yellow {
	color: rgba(203, 145, 47, 1);
	fill: rgba(203, 145, 47, 1);
}
.block-color-teal {
	color: rgba(68, 131, 97, 1);
	fill: rgba(68, 131, 97, 1);
}
.block-color-blue {
	color: rgba(51, 126, 169, 1);
	fill: rgba(51, 126, 169, 1);
}
.block-color-purple {
	color: rgba(144, 101, 176, 1);
	fill: rgba(144, 101, 176, 1);
}
.block-color-pink {
	color: rgba(193, 76, 138, 1);
	fill: rgba(193, 76, 138, 1);
}
.block-color-red {
	color: rgba(212, 76, 71, 1);
	fill: rgba(212, 76, 71, 1);
}
.block-color-default_background {
	color: inherit;
	fill: inherit;
}
.block-color-gray_background {
	background: rgba(248, 248, 247, 1);
}
.block-color-brown_background {
	background: rgba(244, 238, 238, 1);
}
.block-color-orange_background {
	background: rgba(251, 236, 221, 1);
}
.block-color-yellow_background {
	background: rgba(251, 243, 219, 1);
}
.block-color-teal_background {
	background: rgba(237, 243, 236, 1);
}
.block-color-blue_background {
	background: rgba(231, 243, 248, 1);
}
.block-color-purple_background {
	background: rgba(248, 243, 252, 1);
}
.block-color-pink_background {
	background: rgba(252, 241, 246, 1);
}
.block-color-red_background {
	background: rgba(253, 235, 236, 1);
}
.select-value-color-uiBlue { background-color: undefined; }
.select-value-color-pink { background-color: rgba(225, 136, 179, 0.27); }
.select-value-color-purple { background-color: rgba(168, 129, 197, 0.27); }
.select-value-color-green { background-color: rgba(123, 183, 129, 0.27); }
.select-value-color-gray { background-color: rgba(84, 72, 49, 0.15); }
.select-value-color-transparentGray { background-color: undefined; }
.select-value-color-translucentGray { background-color: undefined; }
.select-value-color-orange { background-color: rgba(224, 124, 57, 0.27); }
.select-value-color-brown { background-color: rgba(210, 162, 141, 0.35); }
.select-value-color-red { background-color: rgba(244, 171, 159, 0.4); }
.select-value-color-yellow { background-color: rgba(236, 191, 66, 0.39); }
.select-value-color-blue { background-color: rgba(93, 165, 206, 0.27); }
.select-value-color-pageGlass { background-color: undefined; }
.select-value-color-washGlass { background-color: undefined; }

.checkbox {
	display: inline-flex;
	vertical-align: text-bottom;
	width: 16;
	height: 16;
	background-size: 16px;
	margin-left: 2px;
	margin-right: 5px;
}

.checkbox-on {
	background-image: url("data:image/svg+xml;charset=UTF-8,%3Csvg%20width%3D%2216%22%20height%3D%2216%22%20viewBox%3D%220%200%2016%2016%22%20fill%3D%22none%22%20xmlns%3D%22http%3A%2F%2Fwww.w3.org%2F2000%2Fsvg%22%3E%0A%3Crect%20width%3D%2216%22%20height%3D%2216%22%20fill%3D%22%2358A9D7%22%2F%3E%0A%3Cpath%20d%3D%22M6.71429%2012.2852L14%204.9995L12.7143%203.71436L6.71429%209.71378L3.28571%206.2831L2%207.57092L6.71429%2012.2852Z%22%20fill%3D%22white%22%2F%3E%0A%3C%2Fsvg%3E");
}

.checkbox-off {
	background-image: url("data:image/svg+xml;charset=UTF-8,%3Csvg%20width%3D%2216%22%20height%3D%2216%22%20viewBox%3D%220%200%2016%2016%22%20fill%3D%22none%22%20xmlns%3D%22http%3A%2F%2Fwww.w3.org%2F2000%2Fsvg%22%3E%0A%3Crect%20x%3D%220.75%22%20y%3D%220.75%22%20width%3D%2214.5%22%20height%3D%2214.5%22%20fill%3D%22white%22%20stroke%3D%22%2336352F%22%20stroke-width%3D%221.5%22%2F%3E%0A%3C%2Fsvg%3E");
}
	
</style></head><body><article id="1afa551d-035a-80b0-9ad2-c463b57612e3" class="page sans"><header><h1 class="page-title">ai学习笔记（win系统）</h1><p class="page-description"></p></header><div class="page-body"><h1 id="1b4a551d-035a-80cc-8ba2-fee399663c4c" class="">WK1-ai部署</h1><h2 id="1afa551d-035a-801e-a543-dc104213ad24" class="">Day01-api的使用-openai库+上下文记忆+gradio界面</h2><p id="1afa551d-035a-8055-a790-f049b48194f4" class="">（背景介绍部分未总结）</p><ul id="1afa551d-035a-80e6-903c-e7e6101dadf4" class="toggle"><li><details open=""><summary>折叠day01课程笔记</summary><h2 id="1afa551d-035a-80ac-9792-c5fb8a9e9c8a" class="">openai库的调用（直接调用+autogen使用）</h2><ul id="1afa551d-035a-8091-aaf8-ec9fc46d2906" class="toggle"><li><details open=""><summary>折叠长代码块</summary><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="1afa551d-035a-80da-8d56-f099784cee00" class="code"><code class="language-Python">import openai
import os
openai.api_key = os.getenv(&quot;GUIJI_API&quot;)

openai.api_base = &quot;https://api.siliconflow.cn/v1&quot;

model_id = &quot;deepseek-ai/DeepSeek-R1-Distill-Qwen-7B&quot;

messages = [
    {&quot;role&quot;: &quot;system&quot;, &quot;content&quot;: &quot;你是一个老师&quot;},
    {&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;你好，你是谁&quot;},
]
response = openai.chat.completions.create(
    model=model_id,
    messages=messages,
    stream=True)
for i in response:
    print(i.choices[0][&quot;delta&quot;][&quot;content&quot;], end=&quot;&quot;)</code></pre><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="1afa551d-035a-80bf-b0d6-d245309b4ed5" class="code"><code class="language-Python">import autogen
import os
from autogen import ConversableAgent

api_key = os.getenv(&quot;GUIJI_API&quot;)

# 引入两个模型，分别为千问和蒸馏的 deepseek，并设置标签稍后便于选用
llm_config = {
    &quot;config_list&quot;: [
        {
            &quot;model&quot;: &quot;deepseek-ai/DeepSeek-R1-Distill-Qwen-7B&quot;,
            &quot;base_url&quot;: &quot;https://api.siliconflow.cn/v1&quot;,
            &quot;api_key&quot;: api_key,
            &quot;tags&quot;: [&quot;deepseek&quot;],
        },
        {
            &quot;model&quot;: &quot;Qwen/Qwen2.5-7B-Instruct&quot;,
            &quot;base_url&quot;: &quot;https://api.siliconflow.cn/v1&quot;,
            &quot;api_key&quot;: api_key,
            &quot;tags&quot;: [&quot;qianwen&quot;],
        }
    ]
}

# 选用标签为 &quot;deepseek&quot; 的模型作为 agent
filter_model = {&quot;tags&quot;: [&quot;deepseek&quot;]}  # 修正为 deepseek
config_model = autogen.filter_config(llm_config[&quot;config_list&quot;], filter_model)
agent_deepseek = ConversableAgent(
    name=&quot;deepseek&quot;,
    llm_config={&quot;config_list&quot;: config_model}
)

# 使用 agent 输入 message，返回回答
reply = agent_deepseek.generate_reply(
    messages=[
        {
            &quot;role&quot;: &quot;user&quot;,  # 用户输入
            &quot;content&quot;:&quot;&quot;
        }
    ]
)
print(reply)</code></pre></details></li></ul><h2 id="1afa551d-035a-80ed-b80d-edf8c7cfee0d" class="">如何增加记忆-循环传递上下文逻辑</h2><ul id="1afa551d-035a-8058-8c57-dc5de8180ceb" class="toggle"><li><details open=""><summary>折叠长代码块</summary><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="1afa551d-035a-8040-8cb5-cb36fdc28a4d" class="code"><code class="language-Python">import autogen
from openai import OpenAI
import os
from autogen import ConversableAgent

api_key = os.getenv(&#x27;GUIJI_API&#x27;)

# 引入两个模型，分别为千问和蒸馏的deepseek，并设置标签稍后便于选用
llm_config = {&quot;config_list&quot;:
                  [{&quot;model&quot;: &quot;Qwen/Qwen2.5-7B-Instruct&quot;,
                    &quot;base_url&quot;: &quot;https://api.siliconflow.cn/v1&quot;,
                    &quot;api_key&quot;: api_key,
                    &quot;tags&quot;: [&quot;qianwen&quot;],
                    },
                   {&quot;model&quot;: &quot;deepseek-ai/DeepSeek-R1-Distill-Qwen-7B&quot;,
                    &quot;base_url&quot;: &quot;https://api.siliconflow.cn/v1&quot;,
                    &quot;api_key&quot;: api_key,
                    &quot;tags&quot;: [&quot;deepseek&quot;],
                    }
                   ]
              }

# 选用标签为“deepseek”的模型作为agent
filter_model = {&quot;tags&quot;: [&quot;deepseek&quot;]}
config_model = autogen.filter_config(llm_config[&quot;config_list&quot;], filter_model)
agent = ConversableAgent(
    name=&quot;deepseek&quot;,
    llm_config={&quot;config_list&quot;: config_model}
)


# 使用 agent 输入 message，返回回答
def generate_response(prompt, history):
    # 初始化输入记录
    messages = []

    if history:
        messages.extend(history)
    messages.append({&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: prompt})

    # 生成回复
    reply = agent.generate_reply(
        messages=messages
    )

    if not history:
        history = []
    # 更新历史记录
    history.append({&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: prompt})
    history.append({&quot;role&quot;: &quot;assistant&quot;, &quot;content&quot;: reply})

    return reply, history  # 返回 reply 和更新后的 history


if __name__ == &#x27;__main__&#x27;:
    conversation_history = []

    while True:
        user_input = input(&quot;你，说话！（退出 为退出指令）&quot;)
        if user_input == &quot;退出&quot;:
            break

        reply, conversation_history = generate_response(user_input, conversation_history)
        print(reply)
</code></pre></details></li></ul><h2 id="1afa551d-035a-80c9-9431-fa6c3f62b618" class="">用gradio库包装简单界面</h2><ul id="1afa551d-035a-8035-a876-e1975c2e7a86" class="toggle"><li><details open=""><summary>折叠长代码块</summary><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="1afa551d-035a-80f2-ab53-f49f1c2b5071" class="code"><code class="language-Python">import autogen
from openai import OpenAI
import os
from autogen import ConversableAgent
import gradio
import os
import json

config_file = &#x27;llm_config.json&#x27;

default_config = {
    &quot;config_list&quot;: [
        {
            &quot;model&quot;: &quot;Qwen/Qwen2.5-7B-Instruct&quot;,
            &quot;base_url&quot;: &quot;https://api.siliconflow.cn/v1&quot;,
            &quot;tags&quot;: [&quot;qianwen&quot;]
        },
        {
            &quot;model&quot;: &quot;deepseek-ai/DeepSeek-R1-Distill-Qwen-7B&quot;,
            &quot;base_url&quot;: &quot;https://api.siliconflow.cn/v1&quot;,
            &quot;tags&quot;: [&quot;deepseek&quot;]
        }
    ]
}

if not os.path.exists(config_file):
    with open(config_file, &#x27;w&#x27;) as file:
        json.dump(default_config, file)
        print(f&quot;已创建默认配置文件 {config_file}&quot;)

with open(config_file, &#x27;r&#x27;) as file:
    config_data = json.load(file)

# 从环境变量中获取 API 密钥，并提供默认值或抛出异常
api_key = os.getenv(&#x27;GUIJI_API&#x27;)
if not api_key:
    raise ValueError(&quot;API key is not set in environment variables&quot;)

# 读取配置文件以获取模型配置
def load_llm_config(config_file=&#x27;llm_config.json&#x27;):
    try:
        with open(config_file, &#x27;r&#x27;) as file:
            config_data = json.load(file)
            if &quot;config_list&quot; not in config_data:
                # 如果缺少config_list键，则使用默认配置
                print(f&quot;Warning: Missing &#x27;config_list&#x27; key in configuration file {config_file}. Using default configuration.&quot;)
                return default_config
            return config_data
    except FileNotFoundError:
        raise FileNotFoundError(f&quot;Configuration file {config_file} not found&quot;)
    except json.JSONDecodeError:
        raise ValueError(f&quot;Invalid JSON format in configuration file {config_file}&quot;)

# 示例配置文件内容 (llm_config.json)
&quot;&quot;&quot;
{
    &quot;config_list&quot;: [
        {
            &quot;model&quot;: &quot;Qwen/Qwen2.5-7B-Instruct&quot;,
            &quot;base_url&quot;: &quot;https://api.siliconflow.cn/v1&quot;,
            &quot;tags&quot;: [&quot;qianwen&quot;]
        },
        {
            &quot;model&quot;: &quot;deepseek-ai/DeepSeek-R1-Distill-Qwen-7B&quot;,
            &quot;base_url&quot;: &quot;https://api.siliconflow.cn/v1&quot;,
            &quot;tags&quot;: [&quot;deepseek&quot;]
        }
    ]
}
&quot;&quot;&quot;

# 加载配置并添加 API 密钥
llm_config = load_llm_config()
for config in llm_config[&quot;config_list&quot;]:
    config[&quot;api_key&quot;] = api_key


#选用标签为“deepseek”的模型作为agent
filter_model = {&quot;tags&quot;: [&quot;deepseek&quot;]}
config_model = autogen.filter_config(llm_config[&quot;config_list&quot;], filter_model)
agent = ConversableAgent(
    name = &quot;deepseek&quot;,
    llm_config = {&quot;config_list&quot;: config_model}
)

# 使用 agent 输入 message，返回回答
def generate_response(prompt, history):
    # 初始化输入记录
    messages = []

    if history:
        messages.extend(history)
    messages.append({&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: prompt})

    # 生成回复
    reply = agent.generate_reply(messages=messages)

    if not history:
        history = []
    # 用append更新历史记录
    history.append({&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: prompt})
    history.append({&quot;role&quot;: &quot;assistant&quot;, &quot;content&quot;: reply})

    return reply, history  # 返回 reply 和更新后的 history


def chat_ui_gr(user_input, conversation_state = gradio.State([]), api_key = api_key, base_url = &quot;https://api.siliconflow.cn/v1&quot;, model_id = &quot;deepseek-ai/DeepSeek-R1-Distill-Qwen-7B&quot;):
    conversation_history = conversation_state #获取对话聊天状态
    reply,conversation_history = generate_response(user_input, conversation_history) #对代码稍作修改，聊天函数更改第二个参api，删掉，后两个参：url和model
    return f&quot;机器人：{reply}&quot;,conversation_history
    # conversation_history.append({&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: user_input}) #更新会话历史
    # conversation_history.append({&quot;role&quot;: &quot;assistant&quot;, &quot;content&quot;: reply})#本段冗余，直接删掉
#删掉后改为使用generate_response返回的history进行记忆


iface = gradio.Interface(
    fn = chat_ui_gr,
    inputs = [
        gradio.Textbox(lines = 2, placeholder = &quot;说啊？说词儿啊？&quot;, label = &quot;my问题&quot;),
        gradio.State([]) #储存会话历史
    ],
    outputs = [
        gradio.Textbox(label = &quot;机器人回答&quot;),
        gradio.State() #会话历史（未输出给用户）
    ],
    title = &quot;机器人窗口&quot;,
    description = &quot;基于蒸馏版deepseek的聊天机器人&quot;,
    examples = [
        [&quot;你是什么模型？&quot;],[&quot;1+1=多少？&quot;],[&quot;刚才的计算答案等于多少？&quot;]
    ]
)

iface.launch(share = True)
</code></pre></details></li></ul></details></li></ul><hr id="1afa551d-035a-80f1-b2c6-ffa0a81f2940"/><h2 id="1afa551d-035a-8053-934f-ec630539983a" class="">Day02-ollama本地部署+docker+webui界面可视化</h2><ul id="1afa551d-035a-80ae-af0c-fc51bc577574" class="toggle"><li><details open=""><summary>折叠讲义（预览需时间，建议回顾的时候下载下来再用）（不折叠写笔记的时候也太卡了….）</summary><figure id="1afa551d-035a-80f2-b425-d5c7333b0477"><div class="source"><a href="DeepSeek-R1Ollama%E5%8F%AF%E8%A7%86%E5%8C%96%E6%9C%AC%E5%9C%B0%E9%83%A8%E7%BD%B2-%E8%AE%B2%E4%B9%89.pdf">attachment:4ffba669-7ad4-435b-b41b-0f43c064729d:DeepSeek-R1Ollama可视化本地部署-讲义.pdf</a></div><figcaption>DeepSeek-R1+Ollama可视化本地部署-讲义</figcaption></figure></details></li></ul><ul id="1afa551d-035a-8096-b16a-e746349de5ec" class="toggle"><li><details open=""><summary>折叠day02课程笔记</summary><h2 id="1afa551d-035a-8045-ba2a-e1e91935e204" class="">ollama+docker内的pull准备</h2><h3 id="1afa551d-035a-8031-80e3-e53c0641bbb9" class="">ollama拉取模型+通过环境变量设置下载目录</h3><p id="1afa551d-035a-80f0-9f96-dea5377ef83b" class="">ollama常见指令介绍：</p><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="1afa551d-035a-80ee-b850-da270e1b46bc" class="code"><code class="language-Python">ollama serve         
ollama create        
ollama show          
ollama run           
ollama pull          
ollama push</code></pre><p id="1afa551d-035a-8032-9b88-f14c26e5d5df" class="">此处使用如下命令拉取deepseek镜像，稍后使用</p><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="1afa551d-035a-80ec-8714-f5f5ddfb76fe" class="code"><code class="language-Python">ollama pull deepseek-r1:7b</code></pre><p id="1afa551d-035a-802b-8f2b-df99e8633ba3" class="">如需修改下载位置，则添加环境变量OLLAMA_MODELS，值为D:\ollama\models，也就是目标下载目录</p><h3 id="1afa551d-035a-8078-b481-ca529567e11f" class="">docker拉取webui镜像</h3><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="1afa551d-035a-80a5-bd0b-f5ff205dac73" class="code"><code class="language-Python">docker pull ghcr.io/open-webui/open-webui:cuda</code></pre><p id="1afa551d-035a-8016-9a6a-fbacee8646ef" class="">之后即可再docker desktop内的镜像页面查看到它，随时可将它启动为容器，如下图</p><figure id="1afa551d-035a-804d-a86b-d3dda67c9d41" class="image"><a href="image.png"><img style="width:2238px" src="image.png"/></a></figure><p id="1afa551d-035a-8029-bd71-cd4250066218" class="">如下命令，即可使用该镜像启动一个端口为3000：8080的容器，意味着，可以通过本地的3000端口访问webui容器，不过需要确保ollama提供的端口为8080并且无占用，可以通过环境变量这样设置OLLAMA_HOST，0.0.0.0:8080</p><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="1afa551d-035a-80ce-9147-d6e757eb1258" class="code"><code class="language-Python">docker run -d -p 3000:8080 --add-host=host.docker.internal:host-gateway -v 
open-webui:/app/backend/data --name open-webui --restart always 
ghcr.io/open-webui/open-webui:main</code></pre></details></li></ul><hr id="1afa551d-035a-807d-bdea-d4be985b7eae"/><h2 id="1afa551d-035a-8055-96c1-e0a4cd63bb43" class="">Day03-云服务器端部署模型（含vllm）</h2><ul id="1afa551d-035a-80ab-9e49-fc9d6cc054ad" class="toggle"><li><details open=""><summary>折叠讲义（预览需时间，建议回顾的时候下载下来再用）（不折叠写笔记的时候也太卡了….）</summary><figure id="1afa551d-035a-800e-a854-f83a97cc0453"><div class="source"><a href="DeepSeek-R1%E7%94%9F%E4%BA%A7%E7%8E%AF%E5%A2%83%E9%83%A8%E7%BD%B2%E5%AE%9E%E6%88%98-%E8%AE%B2%E4%B9%89.pdf">attachment:a6822bc7-a24b-4512-86b1-591c72b2fa15:DeepSeek-R1生产环境部署实战-讲义.pdf</a></div></figure></details></li></ul><ul id="1afa551d-035a-80ec-b001-d6c19f0f4de6" class="toggle"><li><details open=""><summary>折叠day03课程笔记</summary><h2 id="1afa551d-035a-8070-8cf5-ff22270aa5cf" class="">魔搭社区下载模型—<a href="https://www.modelscope.cn/home">首页 · 魔搭社区</a></h2><p id="1afa551d-035a-8027-8b61-de9df1e6df9d" class="">提供了以下几种下载方式，比较推荐sdk下载（在服务器端的jupyter，ipynb文件内run即可），极不推荐git下载</p><figure id="1afa551d-035a-8049-ad9f-e130caab052a" class="image"><a href="image%201.png"><img style="width:681.9910888671875px" src="image%201.png"/></a></figure><p id="1afa551d-035a-80ce-b0be-c05655cde6cd" class="">下载后需根据介绍页的测试用例在服务器端的运行结果，补充缺少的依赖库，此处运行无报错，并无需要补充，所以直接进行下一步</p><h2 id="1afa551d-035a-80bc-a029-eb58b0e1fc0e" class="">vllm框架启动模型</h2><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="1afa551d-035a-804d-89fe-c76b6ab347b1" class="code"><code class="language-Python">conda create -n myenv python=3.10 -y
conda activate myenv 
#⽼版本conda可能需要先⽤source activate 
# Install vLLM with CUDA 12.1.
pip install vllm</code></pre><p id="1afa551d-035a-801c-85cd-e1f0e94af29a" class="">再通过vllm框架直接启动该模型，注意替换模型名称</p><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="1afa551d-035a-8003-b32c-c9d0c77eb902" class="code"><code class="language-Python"> python -m vllm.entrypoints.openai.api_server \--model /root/autodl-tmp/deepseek-ai/DeepSeek-R1-Distill-Qwen-7B \--served-model-name DeepSeek-R1-Distill-Qwen-7B \ --max-model-len=2048</code></pre><p id="1afa551d-035a-8047-bee4-d5ce239630da" class="">启动成功后自动从终端进入模型问答交互</p><h2 id="1afa551d-035a-800e-b594-f0961b77969c" class="">(补充：可以用ssh连接通过本地端口访问服务器，已欠费…暂无展示)</h2></details></li></ul><hr id="1afa551d-035a-803a-b2cc-ddef5139ba6d"/><h2 id="1afa551d-035a-8023-a5c0-d9c22ddbe1b2" class="">Day04-anaconda+部分编程技巧</h2><ul id="1afa551d-035a-803c-ac6a-f9c3e5221aef" class="toggle"><li><details open=""><summary>折叠day04课程笔记(只针对记录了未完全掌握部分便于回顾)</summary><p id="1afa551d-035a-80c3-95b9-fd1b02e5719b" class="">多行代码合并为一行           【Crtl+Shift+J】<br/>包装代码                   【Crtl+Alt+T】<br/>在上方插入新行             【Ctrl + Alt + Enter】<br/>在下方插入新行              【Shift + Enter】<br/>上下移动选中代码            【Alt + Shift + 上、下键】<br/>复制代码                     【Ctrl + D】<br/>折叠代码                     【Ctrl + -】<br/>全局查找                 【Ctrl + Shift+F】<br/></p></details></li></ul><hr id="1afa551d-035a-808e-b7ea-c6f5285c48c1"/><h2 id="1afa551d-035a-80e4-aaff-dfeb1cbc2087" class="">Day05-提示词使用技巧和基础模板介绍</h2><ul id="1afa551d-035a-804a-bdb2-fe2c01979d8d" class="toggle"><li><details open=""><summary>折叠讲义（预览需时间，建议回顾的时候下载下来再用）（不折叠写笔记的时候也太卡了….）</summary><figure id="1afa551d-035a-8091-9ffd-d3d5ac846ecd"><div class="source"><a href="course05-DeepSeek%E6%8F%90%E7%A4%BA%E8%AF%8D%E6%8A%80%E5%B7%A7%E5%8F%8A%E5%AE%9E%E8%B7%B5-%E8%AE%B2%E4%B9%89.pdf">attachment:6d03d435-6516-464a-b77e-823837a81cec:course05-DeepSeek提示词技巧及实践-讲义.pdf</a></div></figure></details></li></ul><ul id="1afa551d-035a-8032-8c0c-db84a7ed376a" class="toggle"><li><details open=""><summary>折叠day05课程笔记</summary><p id="1afa551d-035a-808b-b180-ff8677705335" class="">通用框架：人物形象+上下文背景+具体任务+限制条件+期望输出+少样示例</p><p id="1afa551d-035a-8008-b55a-ccb6956f65b2" class="">原理解释：<br/>清晰指令+上下⽂本⾝就是种“奖励”<br/>设定约束条件就是种“惩罚”<br/>提供⽰例就是“奖励模板”<br/>通过迭代优化提⽰词，不断“奖励”模型<br/></p><p id="1afa551d-035a-80a8-a11a-df6ab747279c" class="">部分技巧：（Takeadeepbreath）（Let’sthinkstepbystep），分治法（PromptChain-论文大纲逐阶段生成）， PromptTuning（适用于少样本模型或零样本模型微调），Prompt逆向</p></details></li></ul><hr id="1b4a551d-035a-805e-83fe-fbc10241ab8b"/><h1 id="1afa551d-035a-80bf-ae99-fccf1d5dd3c0" class="">WK2-rag+微调学习笔记</h1><h2 id="1b4a551d-035a-80f5-8122-c2da89c75d07" class="">Day01 rag背景介绍</h2><ul id="1b4a551d-035a-8080-a33f-d5d6244599f3" class="toggle"><li><details open=""><summary>折叠讲义</summary><figure id="1b4a551d-035a-8014-bd77-e8fdb2270012"><div class="source"><a href="RAG%E4%BB%8B%E7%BB%8D%E5%8F%8A%E7%90%86%E8%AE%BA%E5%9F%BA%E7%A1%80-%E8%AE%B2%E4%B9%89.pdf">attachment:25f633dc-a896-4c36-b943-b4cd3651a659:RAG介绍及理论基础-讲义.pdf</a></div></figure></details></li></ul><hr id="1b4a551d-035a-80d7-9013-e07098994b2b"/><h2 id="1b4a551d-035a-8047-9f69-c42621f5124e" class="">Day02 文件直接加载</h2><ul id="1b4a551d-035a-801b-b929-f3cb87878c7a" class="toggle"><li><details open=""><summary>各种文件的直接加载方式代码</summary><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="1b4a551d-035a-80e5-b45a-f615bfd42874" class="code"><code class="language-Python">#从指定的pdf文件中提取所有文本内容
from pdfminer.high_level import extract_text
#用于解析pdf文件，按页面
from pdfminer.high_level import extract_pages
#LTTextBox 文档中的文本框，LTTextLine代表文本框中的单行文本
from pdfminer.layout import LTTextBox, LTTextLine
#定义一个函数，函数：parse_pdf,参数：file_path(文件路径)， 返回解析后文件的内容

def parse_pdf(file_path):
    text = extract_text(file_path)
    return text

#定义一个名为parse_pdf_page的函数，参数也设置为文件路径，解析pdf文件，按页解析

def parse_pdf_page(file_path):
    # 逐页获取pdf文件的布局元素
    for pages in extract_pages(file_path):
        #遍历当前页面每一个元素
        for element in pages:
            #判断当前元素是否为LTTextBox 类型（文本框）
            if isinstance(element, LTTextBox):
                #遍历文本框的每个子元素
                for text_line in element:
                    #判断是否是单行文本
                    if isinstance(text_line, LTTextLine):
                        #打印
                        print(text_line.get_text())
if __name__ == &#x27;__main__&#x27;:
    #print(parse_pdf(r&quot;C:\Users\admin\Desktop\0311demo\course05-DeepSeek提示词技巧及实践-讲义.pdf&quot;))
    parse_pdf_page(r&quot;C:\Users\admin\Desktop\0311demo\course05-DeepSeek提示词技巧及实践-讲义.pdf&quot;)</code></pre><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="1b4a551d-035a-8069-b853-c7e1901c5410" class="code"><code class="language-Python"># 从 docx 库中导入 Document 类，用于处理 Word 文档
from docx import Document

def parse_docx(file_path):
    &quot;&quot;&quot;
    该函数用于解析 Word 文档（.docx 格式），打印文档中表格的内容，并返回文档段落文本的拼接结果。

    :param file_path: Word 文档的文件路径
    :return: 文档中所有段落文本拼接成的字符串，段落之间用换行符分隔
    &quot;&quot;&quot;
    # 打开指定路径的 Word 文档，创建一个 Document 对象
    doc = Document(file_path)
    # 遍历文档中的所有表格
    for table in doc.tables:
        # 遍历当前表格的每一行
        for row in table.rows:
            # 遍历当前行的每一个单元格
            for cell in row.cells:
                # 打印单元格中的文本内容，并以空格结尾
                print(cell.text, end=&quot; &quot;)
            # 打印完一行单元格内容后换行
            print()
    # 遍历文档中的所有段落，将每个段落的文本提取出来
    # 然后使用换行符将这些段落文本连接成一个字符串
    return &quot;\n&quot;.join([para.text for para in doc.paragraphs])

if __name__ == &#x27;__main__&#x27;:
    print(parse_docx(r&#x27;C:\Users\admin\Desktop\0311demo\丁子健.docx&#x27;))</code></pre><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="1b4a551d-035a-802b-baa0-e49172f93143" class="code"><code class="language-Python">import openpyxl
#创建一个函数，读取excel文件(.xlsx格式)，file_path参数：文件路径

def parse_xlsx(file_path):
    #加载excel文件
    workbook = openpyxl.load_workbook(file_path)
    #活动工作表，默认的
    sheet = workbook.active
    #储存从excel工作表中提取出来的每一行数据
    data = []

    #sheet.iter_rows 遍历活动工作表中的每一行，values_only = Ture只获取单元格中的实际值，不包含单元格其他属性
    for row in sheet.iter_rows(values_only = True):
        #将遍历的单元格内容追加到data列表中
        data.append(row)
    return data

if __name__ == &#x27;__main__&#x27;:
    print(parse_xlsx(r&#x27;C:\Users\admin\Desktop\0311demo\新建 Microsoft Excel 工作表.xlsx&#x27;))
</code></pre><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="1b4a551d-035a-8009-b9f4-c194a586fbf1" class="code"><code class="language-Python">def parse_txt(file_path):
    try:
        with open(file_path, &#x27;r&#x27;, encoding = &#x27;utf-8&#x27;) as file:
            return file.read()
    except Exception as e:
        print(f&#x27;好像读取文件错了嗷&#x27;)

if __name__ == &#x27;__main__&#x27;:
    print(parse_txt(r&#x27;C:\Users\admin\Desktop\0311demo\新建文本文档.txt&#x27;))</code></pre><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="1b4a551d-035a-80e1-b099-e57879080612" class="code"><code class="language-Python">import json

def parse_json(file_path):
    with open(file_path, &#x27;r&#x27;, encoding = &#x27;utf-8&#x27;) as file:
        return json.load(file)

if __name__ == &quot;__main__&quot;:
    print(parse_json(r&#x27;C:\Users\admin\Desktop\0311demo\001.json&#x27;))</code></pre></details></li></ul><hr id="1b4a551d-035a-8038-8784-e4947ad23c47"/><h2 id="1b4a551d-035a-8086-b750-c481aeda8e76" class="">Day03 使用框架加载文件+段落切分（含重复段）</h2><ul id="1b4a551d-035a-801d-baae-d93bbe7fdd3b" class="toggle"><li><details open=""><summary>框架加载文件代码</summary><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="1b4a551d-035a-80d2-afe2-c401784719dc" class="code"><code class="language-Python">import nltk

nltk.download(&#x27;punkt_tab&#x27;)
from langchain_unstructured import UnstructuredLoader

file_path=r&#x27;C:\Users\admin\Desktop\pyProDay02\files\abcd.docx&#x27;

#mode=&quot;single&quot;，加载docx文件输出为一整个文本内容
#mode 参数可以设置为&#x27;elements&#x27;,表示根据换行来切分
loader = UnstructuredLoader(file_path, mode=&quot;single&quot;)

#加载docx，解析为一个或多个Document对象,对象存储在一个列表中
documents=loader.load()

for doc in documents:
    print(doc.page_content)
</code></pre><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="1b4a551d-035a-8007-90f8-e94cfb4f139e" class="code"><code class="language-Python">from langchain_community.document_loaders import PyPDFLoader
#定义变量，存储要加载的pdf文件
file_path=r&#x27;C:\Users\admin\Desktop\pyProDay02\files\RAG介绍及理论基础-讲义.pdf&#x27;

#加载pdf文件，参数：路径文件
loader =PyPDFLoader(file_path)

#读取pdf内容，解析为一个或多个Document对象，对象存储在一个列表中
documents =loader.load()

for doc in documents:
    print(doc.page_content)

print(&#x27;=&#x27;*100)
print(&#x27;按页打印的内容&#x27;)
print(&#x27;=&#x27;*100)

#按页分割文档
pages =loader.load_and_split()

#遍历每一页
for i,page in enumerate(pages):
    print(f&#x27;第{i+1}页内容:&#x27;)
    print(page.page_content)
    print(&#x27;-&#x27;*50)</code></pre><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="1b4a551d-035a-8015-9300-f9487cd8c533" class="code"><code class="language-Python">from langchain_unstructured import UnstructuredLoader

file_path=r&#x27;C:\Users\admin\Desktop\pyProDay02\files\本地部署.txt&#x27;

loader =UnstructuredLoader(file_path,mode=&#x27;single&#x27;)
documents =loader.load()
for doc in documents:
    print(doc.page_content)</code></pre><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="1b4a551d-035a-806e-8b0f-c0f4bcc87689" class="code"><code class="language-Python">from langchain_unstructured import UnstructuredLoader

file_path=r&#x27;C:\Users\admin\Desktop\pyProDay02\files\lianxi2.xlsx&#x27;
loader =UnstructuredLoader(file_path,mode=&#x27;single&#x27;)
documents =loader.load()
for doc in documents:
    print(doc.page_content)</code></pre></details></li></ul><ul id="1b4a551d-035a-803e-bdac-c0a840939625" class="toggle"><li><details open=""><summary>直接切分段落代码</summary><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="1b4a551d-035a-8010-a286-d54b80184ae5" class="code"><code class="language-Python">from pdfminer.high_level import extract_text

text =extract_text(r&#x27;C:\Users\admin\Desktop\pyProDay02\files\RAG介绍及理论基础-讲义.pdf&#x27;)

def split_text_with_overlap(text,chunk_size=500,overlap=100):

    assert overlap &lt;chunk_size,&#x27;重叠部分大小要小于每个段落的大小&#x27;

    #初始化起始位置和结束位置为0
    start = end = 0

#当结束位置小于文本总长度时候，继续循环分割文本
    while end &lt;len(text):
        #计算当前段落的结束位置，取起始位置加上段落大小和文本总长度 的 较小值
        #确保不会超过文本的范围
        end =min(start +chunk_size,len(text))
        #将当前分割出的段落返回，每次调用的时候返回一个段落，节省内存
        yield text[start:end]
        #更新起始位置，为下一次分割做准备,起始位置为当前结束位置减去重叠部分的大小
        start = end -overlap

if __name__ == &#x27;__main__&#x27;:
    for i,chunk in enumerate(split_text_with_overlap(text)):
        print(f&quot;第{i+1}段:\n{chunk}\n{&#x27;=&#x27;*50}\n&quot;)



</code></pre><p id="1b4a551d-035a-8050-8acd-f1af64bff204" class="">for和while的本质区别：已知循环次数用for，未知循环次数用while</p></details></li></ul><ul id="1b4a551d-035a-80b2-b203-f805838dd3c6" class="toggle"><li><details open=""><summary>框架切分段落代码</summary><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="1b4a551d-035a-802b-aa70-fee03a004d05" class="code"><code class="language-Python">from langchain_community.document_loaders import UnstructuredFileLoader
from langchain_community.document_loaders import PyPDFLoader
#美化打印复杂的数据结构
from pprint import pprint

file_path=r&#x27;C:\Users\admin\Desktop\pyProDay02\files\RAG介绍及理论基础-讲义.pdf&#x27;
loader =PyPDFLoader(file_path)

#基于字符文本分割器，将文本按指定规则分割成多个块
from langchain.text_splitter import CharacterTextSplitter
#递归字符文本分割器，合适的分隔符进行分割
from langchain.text_splitter import RecursiveCharacterTextSplitter

#创建一个CharacterTextSplitter的对象，用户进行文本分割
text_splitter=CharacterTextSplitter(
    #换行符分割
    separator=&quot;\n\n&quot;,
    #每个分割块的最大字符
    chunk_size=300,
    #重叠的字符数
    chunk_overlap=50,
    length_function=len,
)

#调用loader对象的load_and_split方法，将文本分割器作为参数传入
#该方法加载PDF文件，使用指定的文本分割器将文本分割成多个块
#分割后的块存储在text
text =loader.load_and_split(text_splitter=text_splitter)
print(text)
#遍历文本
for index,block in enumerate(text,start=1):
    print(f&quot;第{index}个文本内容:&quot;)
    print(block.page_content)
    print(&quot;*&quot;*100)</code></pre></details></li></ul><hr id="1b4a551d-035a-8015-ad79-fa178bfe3f9c"/><h2 id="1b5a551d-035a-8038-8eb1-ebcb5a8f97ba" class="">Day04 Embedding模型部署</h2><p id="1b5a551d-035a-8047-9188-dac30f574f90" class="">自注意力机制：自动找到重点，关注重要的部分，忽略不重要的部分自动处理信息</p><p id="1b5a551d-035a-8061-b794-dec5242b510b" class=""><del>如何快速判断import的是方法还是类？—已经会了</del></p><p id="1b5a551d-035a-80df-8838-e03c4345ed6b" class="">余弦相似度原理举例：类似通过夹角余弦值判断向量是否相似</p><p id="1b5a551d-035a-809e-ab05-df808086fa76" class="">欧几里得距离：坐标差平方再开方，算出两点的直线距离</p><p id="1b5a551d-035a-808e-9a00-ef6071cbd695" class=""><del>Transformer，encoder，decoder，embedding，RAG，BERT之间的关系，联系和区别？搜完了懂了</del></p><p id="1b5a551d-035a-8071-a59f-de5f924e55ac" class="block-color-red"><a href="https://blog.csdn.net/Vessel_Liu/article/details/143090157">Transformer架构：Encoder-Decoder</a>：这篇文章讲的看起来很流畅，待读</p><ul id="1b5a551d-035a-80bc-9b0e-fb786037614b" class="toggle"><li><details open=""><summary>讲义文件</summary><figure id="1b5a551d-035a-80d9-9a23-d24069305a33"><div class="source"><a href="Embedding%E6%A8%A1%E5%9E%8B%E5%8E%9F%E7%90%86-%E8%AF%BE%E7%A8%8B%E8%AE%B2%E4%B9%89.pdf">attachment:561f0245-4831-42de-a494-012c2ebf8707:Embedding模型原理-课程讲义.pdf</a></div></figure><figure id="1b5a551d-035a-80bf-8d4c-f8ea30b6d4b5"><div class="source"><a href="Embedding%E5%9C%BA%E6%99%AF%E5%BA%94%E7%94%A8%E5%8F%8A%E5%AE%9E%E6%88%98V3.pdf">attachment:f41f14da-8956-4756-aca5-617162ccde51:Embedding场景应用及实战V3.pdf</a></div></figure></details></li></ul><ul id="1b5a551d-035a-802a-b0c5-dca4d6f2ccd0" class="toggle"><li><details open=""><summary>复习自检———————口头解释以下图【所有细节】</summary><ul id="1b5a551d-035a-8008-aa20-f331326ec987" class="toggle"><li><details open=""><summary>文本相似意义</summary><figure id="1b5a551d-035a-8036-a92e-f255198094b5" class="image"><a href="image%202.png"><img style="width:681.982177734375px" src="image%202.png"/></a></figure></details></li></ul><ul id="1b5a551d-035a-80ee-9bc9-eaf4355551a2" class="toggle"><li><details open=""><summary>rag中embedding作用</summary><figure id="1b5a551d-035a-8095-a7a1-ec69bf4defd5" class="image"><a href="image%203.png"><img style="width:681.9732666015625px" src="image%203.png"/></a></figure></details></li></ul><ul id="1b5a551d-035a-80f2-ae00-e102d5c702d9" class="toggle"><li><details open=""><summary>encoder和decoder联系（BERT，GPT）</summary><figure id="1b5a551d-035a-803a-bd17-eafecfe4de2e" class="image"><a href="image%204.png"><img style="width:672px" src="image%204.png"/></a></figure></details></li></ul><ul id="1b5a551d-035a-802f-b333-fe040ebcfe9e" class="toggle"><li><details open=""><summary>encoder</summary><figure id="1b5a551d-035a-80a6-ad1d-f8c679511233" class="image"><a href="image%205.png"><img style="width:681.9910888671875px" src="image%205.png"/></a></figure></details></li></ul><ul id="1b5a551d-035a-8035-b3e4-c364cf723fc1" class="toggle"><li><details open=""><summary>注意力</summary><figure id="1b5a551d-035a-80a1-b259-cdd4a7ae19db" class="image"><a href="image%206.png"><img style="width:682px" src="image%206.png"/></a></figure></details></li></ul><ul id="1b5a551d-035a-8007-acb6-fb963834c0d0" class="toggle"><li><details open=""><summary>参数</summary><figure id="1b5a551d-035a-80c9-b1af-da6b5702f1d1" class="image"><a href="image%207.png"><img style="width:453.0000305175781px" src="image%207.png"/></a></figure></details></li></ul><ul id="1b5a551d-035a-8046-a233-ff04c48434a6" class="toggle"><li><details open=""><summary>BERT</summary><figure id="1b5a551d-035a-8087-89ab-d743d847f8d6" class="image"><a href="image%208.png"><img style="width:681.9910888671875px" src="image%208.png"/></a></figure></details></li></ul><ul id="1b6a551d-035a-802f-95e1-c04063337757" class="toggle"><li><details open=""><summary>decoder+decoder block</summary><figure id="1b6a551d-035a-80cd-b479-ecffd54c2c71" class="image"><a href="image%209.png"><img style="width:1324px" src="image%209.png"/></a></figure></details></li></ul></details></li></ul><ul id="1b5a551d-035a-80af-a837-c5e6d7569b18" class="toggle"><li><details open=""><summary>直接调用阿里API代码</summary><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="1b5a551d-035a-8083-b0cf-c2eb61c0c27a" class="code"><code class="language-Python">import dashscope
from http import HTTPStatus
import os
def embed_with_str():
    resp = dashscope.TextEmbedding.call(
        model = dashscope.TextEmbedding.Models.text_embedding_v3,
        input = &#x27;活动活动&#x27;,
        apikey = os.getenv(&quot;AL_API_KEY&quot;)
    )
    if resp.status_code == HTTPStatus.OK:
        print(resp)
    else:
        print(resp)
if __name__ == &quot;__main__&quot;:
    embed_with_str()</code></pre></details></li></ul><ul id="1b5a551d-035a-8042-9d47-c7a666661dc5" class="toggle"><li><details open=""><summary>本地部署Embedding代码</summary><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="1b5a551d-035a-8016-b674-ed6d9c86fc7c" class="code"><code class="language-Python">from transformers import AutoTokenizer, AutoModel
import torch
from pdfminer.high_level import extract_text

model_path = r&quot;E:\bge-large-zh-v1___5&quot;
pdf_path = r&#x27;C:\Users\admin\Desktop\text1\丁子健.pdf&#x27;
tokenizer = AutoTokenizer.from_pretrained(model_path)
model = AutoModel.from_pretrained(model_path)
pdf_text = extract_text(pdf_path)

sentence = pdf_text.split(&quot;||&quot;)

encoder_input = tokenizer(sentence, max_length = 512, padding = True, truncation = True, return_tensors = &#x27;pt&#x27;)

with torch.no_gard():
    output = model(**encoder_input)

embeddings = output.last_hidden_state[0][:, 0]

for i in enumerate (embeddings):
    print(i)</code></pre></details></li></ul><ul id="1b5a551d-035a-80b4-af92-c5f3e94ee727" class="toggle"><li><details open=""><summary>相似度对比代码</summary><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="1b5a551d-035a-80b2-8c23-ee0842c669ba" class="code"><code class="language-Python">from transformers import AutoModel,AutoTokenizer
import torch
import torch.nn.functional as F
#本地下载的模型路径
model_path = r&#x27;C:\Users\xupengcheng\.cache\modelscope\hub\models\BAAI\bge-large-zh-v1___5&#x27;
#加载分词器
tokenizer=AutoTokenizer.from_pretrained(model_path)
#加载模型
model =AutoModel.from_pretrained(model_path)

text_list=[&#x27;你是谁&#x27;,&#x27;我是老师&#x27;]

encode_input=tokenizer(text_list,max_length=512,padding=True,truncation=True,return_tensors=&#x27;pt&#x27;)

with torch.no_grad():
    model_output=model(**encode_input)

sentence_embedding =model_output[0][:,0]

#取出第一个句子
tensor_a=sentence_embedding[0]

#取出第二个句子
tensor_b=sentence_embedding[1]

#计算两个句子嵌入向量之间余弦相似度,dim在第一个维度上进行计算
cosine_similarity=F.cosine_similarity(tensor_a,tensor_b,dim=0)

print(f&#x27;cosine_similarity:{cosine_similarity.item()}&#x27;)</code></pre></details></li></ul><hr id="1b5a551d-035a-80d9-a578-cec0b9b1f763"/><h2 id="1b6a551d-035a-805d-ad55-f57148d8eff0" class="">Day05 向量数据库</h2><ul id="1b6a551d-035a-80e4-944e-f646725083f9" class="toggle"><li><details open=""><summary>代码段</summary><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="1b6a551d-035a-802a-86ec-f8616c29757f" class="code"><code class="language-Python">#json数据集,解析json数据集，向量化数据集，存储在chroma里面，提问，结果显示
import json
#os模块提供了操作系统交互功能，比如：文件和目录
import os
#导入向量数据库
import chromadb
#从transformers库中导入，模型应用和分词器（将文本转换成模型可以处理的输入格式）
from transformers import AutoModel,AutoTokenizer
#用于训练和推理模型
import torch

#本地模型（用于文本向量化）
model_path = r&quot;E:\bge-large-zh-v1___5&quot;

tokenizer=AutoTokenizer.from_pretrained(model_path)
model=AutoModel.from_pretrained(model_path)

#解释json文件,file_path文件路径
def json_parse(file_path):
    with open(file_path,&#x27;r&#x27;,encoding=&#x27;utf-8&#x27;) as file:
        data =json.load(file)

    #初始化空列表,存储解析后的结果
    result=[]

    #遍历解析后的json数据每一个元素
    for item in data:
        #从元素中提取k_qa_content字段的值
        k_qa_content =item[&#x27;k_qa_content&#x27;]
        #值分割，maxsplit=1最多分割一次
        #分别存入keyword,answer
        keyword,answer =k_qa_content.split(&#x27;#&#x27;,maxsplit=1)
        result.append([keyword,answer])
    return result

#将输入的文本转换为向量表示,texts:参数，包含一个或多个文本
def embed_text(texts):
    #texts 需要向量化的文本
    #return_tensors=&#x27;pt&#x27; 返回pyTorch张量
    #padding=True 文本填充，使它们具有相同长度
    #truncation=True 如果文本从长度超过最大则截断
    #max_length=512 最大长度为512个词元
    inputs =tokenizer(texts,return_tensors=&#x27;pt&#x27;,padding=True,truncation=True,max_length=512)
    #不计算梯度上下文环境进行推理，节省内存和计算资源
    with torch.no_grad():
        outputs=model(**inputs)

    #向量化后的数字转换成列表
    #numpy() 转数组
    #tolist() 转列表
    embeddings= outputs.last_hidden_state[:,0,:].numpy().tolist()

    return embeddings

#指定的路径下的所有json文件，向量化，将结果存储在chroma数据库中
def run(all_file_path):
    #获取指定目录下的所有文件名
    file_names =os.listdir(all_file_path)

    #遍历所有的文件名
    for file_name in file_names:

        #data_path/data_source.json
        file_path =os.path.join(all_file_path,file_name)
        print(file_path)

        #解析当前json文件，得到关键词和答案的列表
        text_list =json_parse(file_path)

        #存储关键词
        keyword_list=[]
        #存储答案
        answer_list=[]

        #遍历解析文本列表
        for text in text_list:
            keyword_list.append(text[0])
            answer_list.append({&#x27;answer&#x27;:text[1]})

        #调用函数embed_text，本地bge模型对关键词列表进行向量化
        embedding_list=embed_text(keyword_list)

        #判断向量化结果是否为列表类型
        if isinstance(embedding_list, list):
            collection=client.get_or_create_collection(name=&#x27;chromadb_myClass&#x27;)
            #生成id
            len_ids =len(text_list)
            ids_list =[]
            for i in range(len_ids):
                #文件名+序号=ids,生成id加入到列表中
                ids_list.append(str(file_name)+str(i))

            #将向量化结果、关键词、答案元数据、id列表添加到chromaDB集合中
            collection.add(
                embeddings=embedding_list,
                documents=keyword_list,
                metadatas=answer_list,
                ids=ids_list
            )
        else:
            return &#x27;error&#x27;
    return &#x27;向量化完成&#x27;

if __name__ == &#x27;__main__&#x27;:
    # 创建一个 Chroma 客户端

    client = chromadb.Client()
    # client = chromadb.HttpClient(host=&#x27;localhost&#x27;, port=8000)
    all_file_path=&quot;data_path&quot;

    collection = client.get_or_create_collection(name=&#x27;chromadb_myClass&#x27;)
    run(all_file_path)
    input_text=&#x27;类和对象&#x27;
    search_embedding= embed_text([input_text])
    query_result =collection.query(
        query_embeddings=search_embedding,
        n_results=3
    )
    print(query_result)</code></pre></details></li></ul><figure id="1b6a551d-035a-80ad-b384-c96a6539f413" class="link-to-page"><a href="Chroma%E5%90%91%E9%87%8F%E6%95%B0%E6%8D%AE%E5%BA%93%E5%AE%9E%E6%88%98%E8%AE%B2%E4%B9%89%201b6a551d035a80adb384c96a6539f413.html">Chroma向量数据库实战讲义</a></figure><p id="1b6a551d-035a-80fd-8ea5-cf9e5afd7d0b" class="">
</p><h1 id="1baa551d-035a-8010-9068-d4b79618c781" class="">WK3-Gradio+rag+embedding</h1><h2 id="1baa551d-035a-80f4-83d2-eec474249964" class="">Day01 Gradio库</h2><ul id="1baa551d-035a-8082-a12b-fda28e812d0e" class="toggle"><li><details open=""><summary>绘制+事件 基础demo代码块</summary><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="1baa551d-035a-8063-bc65-f45e35aed8cd" class="code"><code class="language-Python">import gradio as gr


def greet(name,intensity):
    return &quot;hello,&quot;+name + &quot;!&quot;* int(intensity)

demo=gr.Interface(
    #关联函数名称
    fn=greet,
    #输入
    inputs=[&quot;text&quot;,&quot;slider&quot;],
    #输出
    outputs=[&#x27;text&#x27;]
)

demo.launch()</code></pre><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="1baa551d-035a-808e-a749-ce08b00b9069" class="code"><code class="language-Python">import gradio as gr

def greet(input_name):
    return &quot;hello,&quot;+input_name+&quot;!&quot;

#创建自定义 页面内容
with gr.Blocks() as demo:
    #创建一个文本框，提示:Name
    input_name=gr.Textbox(label=&#x27;Name&#x27;)
    output_text=gr.Textbox(label=&#x27;output&#x27;)
    #创建按钮，显示名称是：保存
    btn=gr.Button(&quot;保存&quot;)
    #给按钮添加事件（单击）,fn=函数名,inputs=输入对象，outputs=输出对象
    btn.click(fn=greet,inputs=input_name,outputs=output_text)

#启动
demo.launch()</code></pre><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="1baa551d-035a-80b9-b0d4-fc40af19c992" class="code"><code class="language-Python">import gradio as gr

def reset_text(input):
    return &quot;&quot;

with gr.Blocks() as demo:
    input=gr.Textbox(label=&#x27;请输入文本&#x27;)
    reset_btn=gr.Button(&#x27;重置&#x27;)
    reset_btn.click(fn=reset_text,inputs=input,outputs=input)

demo.launch()</code></pre><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="1baa551d-035a-8088-a20a-c54345fc058c" class="code"><code class="language-Python">import gradio as gr

def messages(name,age,phone):
    msg =&#x27;姓名:&#x27;+name+&#x27;,年龄:&#x27;+age+&#x27;,电话:&#x27;+phone
    return msg

with gr.Blocks() as demo:
    name =gr.Textbox(label=&#x27;姓名&#x27;)
    age =gr.Textbox(label=&#x27;年龄&#x27;)
    phone =gr.Textbox(label=&#x27;电话&#x27;)
    output_msg =gr.Textbox(label=&#x27;输出人员信息&#x27;)
    submit_btn =gr.Button(&#x27;注册&#x27;)
    submit_btn.click(fn=messages,inputs=[name,age,phone],outputs=output_msg)

demo.launch()</code></pre><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="1baa551d-035a-80a7-9f15-cc949e6819dc" class="code"><code class="language-Python">import gradio as gr

def greet(name):
    return f&#x27;hello,{name}!&#x27;

choices=[&#x27;科员&#x27;,&#x27;科长&#x27;,&#x27;处长&#x27;,&#x27;局长&#x27;]

dd =gr.Dropdown(choices=choices,label=&#x27;请选择职务&#x27;)

app =gr.Interface(fn=greet,inputs=dd,outputs=&#x27;text&#x27;)

app.launch()</code></pre><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="1baa551d-035a-8028-a562-e4bb53cc268e" class="code"><code class="language-Python">import gradio as gr

def greet(name):
    return name

with gr.Blocks() as demo:
    #添加一行
    with gr.Row():
        #添加一列
        with gr.Column():
            input =gr.Textbox(label=&#x27;请输入姓名&#x27;)
        #添加一列
        with gr.Column():
            output= gr.Textbox(label=&#x27;输出内容&#x27;)
    #添加一行
    with gr.Row():
        submit_btn=gr.Button(&#x27;提交&#x27;)
        submit_btn.click(fn=greet,inputs=input,outputs=output)
demo.launch()</code></pre></details></li></ul><ul id="1baa551d-035a-80a7-a32a-c8006ca3edaa" class="toggle"><li><details open=""><summary>embedding.py（仅调用api实现）</summary><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="1baa551d-035a-80eb-87d1-f4653954acf0" class="code"><code class="language-Python">import  torch

#调用阿里云大模型服务
import dashscope
import os
#获取http请求的状态码,判断请求是否成功
from http import HTTPStatus

#定义个函数，用于使用qwen模型对输入的文本列表进行向量化处理
def embed_with_qwen(text_list):
    #设置dashscope的api秘钥,访问阿里云的相关服务
    dashscope.api_key=os.getenv(&#x27;AL_API_KEY&#x27;)

    #初始化一个空列表，储存每个文本的嵌入向量s
    embedding_list=[]

    #遍历输入的文本列表
    for text in text_list:
        #调用dashscope的TextEmbedding服务
        resp =dashscope.TextEmbedding.call(
            #使用模型为text_embedding_v2
            model =dashscope.TextEmbedding.Models.text_embedding_v2,
            #要进行向量化的处理文本
            input=text
        )

        #检查请求状态码是否为200，表示请求成功
        if resp.status_code ==HTTPStatus.OK:
            #提取向量
            embedding =resp.output[&#x27;embeddings&#x27;][0][&#x27;embedding&#x27;]
            #embedding_list中
            embedding_list.append(embedding)
        else:
            print(resp)
            return resp

    return embedding_list

#使用本地大模型，BGE模型对输入的文本列表进行嵌入向量处理
def embed_with_bge(text_list,tokenizer,model):
    encode_input=tokenizer(text_list,max_length=512,padding=True,truncation=True,return_tendsors=&#x27;pt&#x27;)
    with torch.no_grad():
        model_output=model(**encode_input)

    sentence_embeddings=model_output[0][:,0]

    return sentence_embeddings.tolist()


if __name__ == &#x27;__main__&#x27;:
    print(embed_with_qwen([&#x27;今天星期一&#x27;]))
</code></pre></details></li></ul><ul id="1baa551d-035a-803f-b5ac-dbd24d4e154f" class="toggle"><li><details open=""><summary>gradio_demo.py</summary><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="1baa551d-035a-80f6-b252-ef018e1f84e1" class="code"><code class="language-Python">import gradio as gr
from zhipuai import ZhipuAI
import time
with gr.Blocks() as demo:
    gr.Markdown(&quot;# 智能助手demo&quot;)

    #创建一个聊天机器人组件，用于显示对话历史，数据类型为消息列表
    chatbot =gr.Chatbot(type=&quot;messages&quot;)

    #用户可以在这个文本框输入想要提问给智能助手的内容
    msg=gr.Textbox()

    #创建清空按钮
    clear=gr.Button(&quot;Clear&quot;)

    #将用户输入的消息添加到历史记录中，标记为用户角色
    def user(user_messages,history:list):
        return &quot;&quot;,history+[{&quot;role&quot;:&quot;user&quot;,&quot;content&quot;:user_messages}]


    def bot(history : list):
        #创建智谱客户端实例，调用智谱的服务
        client=ZhipuAI(api_key=&quot;你的智谱api-key&quot;)

        messages =history

        response =client.chat.completions.create(
            model=&quot;glm-4&quot;, #指定模型 glm-4
            messages=messages, #历史的提问和回答内容传给ai
            stream=True, #流式输出
        )

        #在历史记录中添加一个空的助手回复消息
        history.append({&quot;role&quot;:&quot;assistant&quot;,&quot;content&quot;:&quot;&quot;})

        #遍历响应的每个数据块
        for chunk in response:
            #讲当前的数据块的内容追加到助手回复消息中
            history[-1][&#x27;content&#x27;]+=chunk.choices[0].delta.content

            #让用户能看到回复逐步生成的过程,延迟0.05秒
            time.sleep(0.05)

            #实时更新流式界面
            yield history


    #user:用户在文本框中点击回车键,消息的提交，调用user函数处理用户输入
    #[msg,chatbot]:文本框输入和聊天机器人的历史记录作为输入参数
    #[msg,chatbot]:将处理后的结果更新到文本框和聊天机器人组件中
    #queue=False: 表示不使用消息队列处理该事件
    msg.submit(user,[msg,chatbot],[msg,chatbot],queue=False).then(
        #处理完用户输入后，接着调用bot函数，获取助手回复
        bot,chatbot,chatbot
    )

    #清空按钮点击事件
    #lambda :None 匿名函数，不做任何操作,只触发清空操作
    #None 没有输出参数
    #chatbot 聊天机器人内容清空
    # queue=False: 表示不使用消息队列处理该事件
    clear.click(lambda :None,None,chatbot,queue=False)

#启动
demo.launch()</code></pre></details></li></ul><ul id="1bba551d-035a-8063-b3e8-f9d757000025" class="toggle"><li><details open=""><summary>chromadb_delete.py</summary><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="1bba551d-035a-8011-a9b2-cddffa95eede" class="code"><code class="language-Python">import chromadb
try:
    client =chromadb.HttpClient(host=&#x27;localhost&#x27;,port=8000)
    collections =client.list_collections()
    print(collections)
    col_names =collections
    for name in col_names:
        client.delete_collection(name=name)
        print(f&#x27;成功删除集合:{name}&#x27;)
except Exception as e:
    print(f&#x27;发生错误:{e}&#x27;)</code></pre></details></li></ul><ul id="1bba551d-035a-80ab-b165-da91b157a655" class="toggle"><li><details open=""><summary>chromadb_query.py</summary><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="1bba551d-035a-809c-a425-f246ed7133fc" class="code"><code class="language-Python">import chromadb
try:
    client =chromadb.HttpClient(host=&#x27;localhost&#x27;,port=8000)
    collections=client.list_collections()
    print(collections)
    collection_name =&#x27;test8899&#x27;
    if collection_name in collections:
        col =client.get_collection(collection_name)
        print(col.peek())
    else:
        print(f&#x27;集合{collection_name}不存在&#x27;)
except Exception as e:
    print(f&#x27;发生错误:{e}&#x27;)</code></pre></details></li></ul><h2 id="1baa551d-035a-80ac-84eb-ea468a4637a0" class="">Day02 rag集成 —— 类方法调用+gradio.File（文件路径拖动获取）</h2><figure id="1bba551d-035a-80b5-b775-c7b0417cea55" class="image"><a href="image%2010.png"><img style="width:709.982177734375px" src="image%2010.png"/></a></figure><ul id="1baa551d-035a-809a-ac73-c9bb3667b6bc" class="toggle"><li><details open=""><summary>main.py</summary><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="1bba551d-035a-8090-8daf-fa0b8d48ed87" class="code"><code class="language-Python">import gradio as gr
from chromadb import HttpClient
from file_to_chromadb import run as file_to_chromadb
from RAG import Robot
import chromadb
from zhipuai import ZhipuAI
#获取数据库集合列表
def get_db_list():
    client =chromadb.HttpClient(host=&#x27;localhost&#x27;,port=8000)
    #获取所有集合列表
    collections=client.list_collections()
    return collections

#创建数据库
def build_db(file,dbname):
    try:
        #调用函数构建数据库
        res =file_to_chromadb(file,dbname)
        if res == &quot;success&quot;:
            return gr.Info(&quot;数据库创建成功&quot;,duration=5)
        else:
            return gr.Error(&quot;数据库创建失败&quot;,duration=5)
    except Exception as e:
        print(f&quot;构建数据库出错：{e}&quot;)
        return gr.Error(&quot;数据库构建失败&quot;,duration=5)

#刷新数据库选择项
def refresh_db_choices():
    dbList =get_db_list()
    return gr.update(choices=dbList)

#处理用户输入
def user(user_message,history:list):
    #首次问答，添加提示词
    if len(history) == 0:
        history.append({&quot;role&quot;: &quot;system&quot;,&quot;content&quot;: &quot;你是一个名叫Molly的教育专家，对于用户提问的问题，你需要按照给出的【参考资料】对问题进行回答。你的回答需要按照以下两个步骤：1.分析用户问题和参考资料，判断是否有【参考资料】可以解答用户的问题，如果有则说明【参考资料】的名称，如果没有，则首先告知用户没有任何可参考的资料，需要注意答案的准确性。2.根据资料内容对问题进行解答。&quot;})
    history.append({&quot;role&quot;:&quot;user&quot;,&quot;content&quot;:user_message})
    return &quot;&quot;,history

#处理机器人回复
def bot(history:list,dbname):
    #获取用户最新输入文本
    input_text =history[-1][&#x27;content&#x27;]
    #调用机器人的GAG获取参考资料
    result_rag =robot.RAG(input_text,dbname)
    #参考资料添加到用户输入中
    history[-1][&#x27;content&#x27;]+=f&quot;\n参考资料：\n{result_rag}&quot;
    client =ZhipuAI(api_key=&#x27;7b16305a776744253ac1bad218a8f90c.p1EJxuKGhDmPKxfF&#x27;)
    messages=history
    response =client.chat.completions.create(
        model=&quot;glm-4&quot;,
        messages=messages,
        stream=True
    )

    #使用&quot;参考资料&quot;作为分隔符，把文档分开
    parts=history[-1][&#x27;content&#x27;].split(&quot;参考资料：&quot;,1)
    #提第一个参考资料前的内容
    history[-1][&#x27;content&#x27;]=parts[0].strip()
    history.append({&quot;role&quot;: &quot;assistant&quot;,&quot;content&quot;: &quot;&quot;})

    for chunk in response:
        #模型回复的内容逐步添加到助手回复消息中
        history[-1][&#x27;content&#x27;]+=chunk.choices[0].delta.content
        yield history

robot=Robot()
dbList =get_db_list()

with gr.Blocks() as demo:
    with gr.Row():
        gr.Markdown(&quot;# 智能学习助手&quot;)
    with gr.Row():
        with gr.Column(scale=2):
            chatbot =gr.Chatbot(type=&quot;messages&quot;,label=&#x27;对话框&#x27;,height=500)
            question =gr.Textbox(label=&#x27;请输入&#x27;)
            clear_btn =gr.Button(&quot;clear&quot;)
            gr.Examples([&quot;介绍一下类对象&quot;,&quot;解释一下面向对象&quot;],inputs=[question])
        with gr.Column(scale=1):
            gr.Markdown(&quot;### 数据库构建&quot;)
            datafile =gr.Filex(type=&quot;filepath&quot;,label=&quot;上传文件&quot;)
            dbname=gr.Textbox(label=&#x27;数据库名称&#x27;)
            build_btn=gr.Button(&quot;开始构建&quot;)
            gr.Markdown(&quot;### 数据库选择&quot;)
            dbchoose =gr.Dropdown(choices=dbList,label=&quot;数据库名称&quot;)
            dbrefresh_btn=gr.Button(&quot;刷新&quot;)


    #用户在文本框中输入问题并提交时,调用user函数处理用户输入，然后调用bot函数获取机器人回复
    question.submit(user,[question,chatbot],[question,chatbot],queue=False).then(bot,[chatbot,dbchoose],chatbot)
    #用户点击“开始构建”,调用函数build_db 创建数据库,解析文件，向量化，加入到数据库的集合中
    build_btn.click(build_db,[datafile,dbname],[])
    #点击“刷新”，调用函数refresh_db_choices，刷新数据库集合下拉选项
    dbrefresh_btn.click(refresh_db_choices,outputs=dbchoose)
    #点击&quot;clear&quot;，清空聊天记录
    clear_btn.click(lambda :None,None,chatbot,queue=False)

#启动
demo.launch()

</code></pre></details></li></ul><ul id="1bba551d-035a-805a-8e30-fc666973b906" class="toggle"><li><details open=""><summary>RAG.py</summary><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="1bba551d-035a-8089-abe2-c1d300892d96" class="code"><code class="language-Python">from zhipuai import ZhipuAI
from embedding import embed_with_bge,embed_with_qwen
import chromadb
from transformers import AutoTokenizer,AutoModel
import  torch

client =chromadb.HttpClient(host=&#x27;localhost&#x27;,port=8000)

#定义一个生成器函数，实现流式对话输出
def chatglm_stream(input_text,history_list):
    client =ZhipuAI(api_key=&#x27;7b16305a776744253ac1bad218a8f90c.p1EJxuKGhDmPKxfF&#x27;)
    messages =history_list
    #在对话历史中添加用户最新输入
    messages.append({&quot;role&quot;:&quot;user&quot;,&quot;content&quot;:f&quot;{input_text}&quot;})
    #调用智谱AI的聊天完成借口，使用glm-4模型，流式输出
    response =client.chat.completions.create(
        model=&#x27;glm-4&#x27;,
        messages=messages,
        stream=True,
    )
    #遍历流式响应的每个数据块
    for chunk in response:
        #从当前数据库中提取模型生成的回复内容
        output =chunk.choices[0].delta.content
        #将回复内容逐个返回，实现流式输出
        yield output

#定义一个类，封装与对话机器人相关的功能
class Robot():
    def __init__(self):
        #空字符串，存储文本向量
        self.embedding=&quot;&quot;

    #定义rag方法，实现检索增强生成功能
    def RAG(self,input_text,dbname):
        #获得输入文本向量
        #使用qwen的函数对输入文本进行向量化处理
        search_embedding =embed_with_qwen([input_text])

        #从chroma数据库客户端获得集合
        collection =client.get_collection(name=dbname)

        #从向量数据库中检索与输入文本向量最相似的文档，数量设置为3个
        result =collection.query(
            query_embeddings=search_embedding,
            n_results=3,
        )

        #空字符串，存储文档和参考资料信息
        result_all=&quot;&quot;

        #遍历文档列表
        for i in range(len(result[&#x27;documents&#x27;][0])):
            #获取文档内容
            result_documents=result[&#x27;documents&#x27;][0][i]
            #获取参考资料内容
            result_file =result[&#x27;metadatas&#x27;][0][i][&#x27;answer&#x27;]
            #将文档内容和参考资料信息合并到result_all中
            result_all+=result_documents+f&quot;\n【参考资料】：{result_file}\n\n&quot;

        return result_all

    def run_test(self):
        history_list = [{&quot;role&quot;: &quot;system&quot;,&quot;content&quot;: &quot;你是一个名叫Molly的教育专家，对于用户提问的问题，你需要按照给出的【参考资料】对问题进行回答。你的回答需要按照以下两个步骤：1.分析用户问题和参考资料，判断是否有【参考资料】可以解答用户的问题，如果有则说明【参考资料】的名称，如果没有，则首先告知用户没有任何可参考的资料，需要注意答案的准确性。2.根据资料内容对问题进行解答。&quot;}]
        while True:
            input_text =input(&#x27;请输入:&#x27;)
            if input_text ==&#x27;exit&#x27;:
                break
            dbname=&#x27;test8899&#x27;
            result_rag=self.RAG(input_text,dbname)
            input_text =input_text+f&#x27;参考资料：\n{result_rag}&#x27;
            result =chatglm_stream(input_text,history_list)
            result_all=&quot;&quot;
            for res in result_all:
                result_all+=res
                print(res,end=&quot;&quot;,flush=True)
            print(&quot;&quot;)
            message_input ={&quot;role&quot;:&quot;user&quot;,&quot;content&quot;:f&quot;{input_text}&quot;}
            history_list.append(message_input)
            message_output={&quot;role&quot;:&quot;assistant&quot;,&quot;content&quot;:f&quot;{result_all}&quot;}
            history_list.append(message_output)
            print(history_list)

if __name__ == &#x27;__main__&#x27;:
    robot =Robot()
    robot.run_test()










</code></pre></details></li></ul><ul id="1bba551d-035a-80ab-8f15-ffb71a43d086" class="toggle"><li><details open=""><summary>file_to_chromadb.py</summary><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="1bba551d-035a-806a-85f2-f8675c026769" class="code"><code class="language-Python">#1、解析文件 2、解析后文件向量化 3、向量化后加入到数据库中

import json
from embedding import embed_with_bge,embed_with_qwen
import os
import chromadb
from transformers import AutoTokenizer,AutoModel

#JSON文件解析
def json_parse(file):
    #以只读模式打开json文件，指定编码utf-8
    with open(file,&#x27;r&#x27;,encoding=&#x27;utf-8&#x27;) as file:
        #加载json文件
        data =json.load(file)
    #创建空列表
    result =[]
    #循环遍历data
    for item in data:
        #每个项k_qa_content,获取值
        k_qa_content=item[&#x27;k_qa_content&#x27;]
        #使用#作为分隔符，拆分关键词和答案
        keyword,answer =k_qa_content.split(&#x27;#&#x27;,maxsplit=1)
        #拆分后的关键词和答案添加到result列表中
        result.append([keyword,answer])
        #返回处理后的数据列表
    return result

def run(file_path,dbname):
    #调用函数解析指定路径的json文件
    text_list =json_parse(file_path)
    print(text_list)
    #空列表，存储关键词
    keyword_list=[]
    #空列表,存储答案
    answer_list=[]
    #遍历解析后的文件
    for text in text_list:
        #每项中的关键词添加到keyword_list列表中
        keyword_list.append(text[0])
        #每项中的答案以字典形式添加到answer_list列表中
        answer_list.append({&quot;answer&quot;:text[1]})
    #使用闭源模型API，调用embed_with_qwen函数对关键词列表进行向量化处理
    embedding_list =embed_with_qwen(keyword_list)

    if type(embedding_list) == list:
        #创建chroma客户端，连接本地端口8000
        client=chromadb.HttpClient(host=&#x27;localhost&#x27;,port=8000)
        #获取或创建指定名的集合
        collection=client.get_or_create_collection(name=dbname)
        #创建ids： 数据库名+序列号
        len_ids=len(text_list)
        ids_list=[]
        for i in range(len_ids):
            ids_list.append(str(dbname)+str(i))
        #向集合中添加向量化数据，关键词，答案元数据和唯一标识id
        collection.add(
            embeddings=embedding_list,
            documents=keyword_list,
            metadatas=answer_list,
            ids=ids_list
        )
    else :
        return &#x27;error&#x27;

    print(&#x27;向量化完成&#x27;)
    return &#x27;success&#x27;

if __name__ == &#x27;__main__&#x27;:
    file_path=&#x27;data_path/python基础（1）.json&#x27;
    dbname=&#x27;test8899&#x27;
    print(run(file_path, dbname))</code></pre></details></li></ul><ul id="1bba551d-035a-809a-9719-dae50ec2246a" class="toggle"><li><details open=""><summary>embedding.py</summary><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="1bba551d-035a-8051-b9a4-c910295b7cca" class="code"><code class="language-Python"># 导入 PyTorch 库，它是一个用于深度学习的开源库，提供了丰富的张量计算和自动求导功能
import torch

# 导入 dashscope 库，用于调用阿里云的大模型服务，这里可能用于文本嵌入相关操作
import dashscope

# 从 http 模块中导入 HTTPStatus 类，用于获取 HTTP 请求的状态码，方便后续判断请求是否成功
from http import HTTPStatus


# 定义一个函数 embed_with_qwen，用于使用 Qwen 模型对输入的文本列表进行嵌入处理
def embed_with_qwen(text_list):
    # 设置 dashscope 的 API 密钥，该密钥用于访问阿里云的相关服务
    dashscope.api_key = &#x27;sk-eb40ccf62d6648dc92a16ecff1686efa&#x27;
    # 初始化一个空列表，用于存储每个文本的嵌入向量
    embedding_list = []
    # 遍历输入的文本列表
    for text in text_list:
        # 调用 dashscope 的 TextEmbedding 服务，使用 text_embedding_v2 模型对当前文本进行嵌入处理
        resp = dashscope.TextEmbedding.call(
            # 指定使用的模型为 text_embedding_v2
            model=dashscope.TextEmbedding.Models.text_embedding_v2,
            # 要进行嵌入处理的文本
            input=text
        )
        # 检查请求的状态码是否为 HTTP 200（OK），表示请求成功
        if resp.status_code == HTTPStatus.OK:
            # 从响应中提取嵌入向量
            embedding = resp.output[&#x27;embeddings&#x27;][0][&#x27;embedding&#x27;]
            # 将提取的嵌入向量添加到 embedding_list 中
            embedding_list.append(embedding)
        else:
            # 如果请求失败，打印响应信息
            print(resp)
            # 并返回响应对象，方便后续排查问题
            return resp
    # 返回存储所有文本嵌入向量的列表
    return embedding_list


# 定义一个函数 embed_with_bge，用于使用 BGE 模型对输入的文本列表进行嵌入处理
def embed_with_bge(text_list, tokenizer, model):
    # 使用 tokenizer 对输入的文本列表进行编码处理，设置最大长度为 512，进行填充和截断操作，并将结果转换为 PyTorch 张量
    encoded_input = tokenizer(text_list, max_length=512, padding=True, truncation=True, return_tensors=&#x27;pt&#x27;)
    # 开启一个无梯度计算的上下文环境，在推理过程中不需要计算梯度，这样可以节省内存和计算资源
    with torch.no_grad():
        # 将编码后的输入传递给模型进行推理
        model_output = model(**encoded_input)
    # 进行池化操作，这里采用 CLS 池化，即取输出序列中第一个 token 的向量作为句子的表示
    sentence_embeddings = model_output[0][:, 0]
    # 将 PyTorch 张量转换为 Python 列表
    sentence_embeddings_list = sentence_embeddings.tolist()
    # 返回存储所有句子嵌入向量的列表
    return sentence_embeddings_list


# 程序的入口点，如果该脚本作为主程序运行，则执行以下代码
if __name__ == &#x27;__main__&#x27;:
    # 调用 embed_with_qwen 函数对文本 &quot;今天是个好日子&quot; 进行嵌入处理，并打印结果
    # 注意：这里传入的应该是一个列表，如 [&quot;今天是个好日子&quot;]，原代码写法有误
    print(embed_with_qwen([&quot;今天是个好日子&quot;]))</code></pre></details></li></ul><h2 id="1bba551d-035a-809b-b6eb-e131955cc511" class="">Day03  梳理上一天的代码+自习-学习设计模式</h2><p id="1bba551d-035a-801e-b3be-c785c76a9f53" class="">设计模式六大基本原则：<br/><br/><strong>开闭</strong> → 开闭原则<br/><br/><strong>里氏</strong> → 里氏代换原则<br/><br/><strong>倒</strong> → 依赖倒转原则（“倒”转）<br/><br/><strong>接口</strong> → 接口隔离原则<br/><br/><strong>迪米</strong> → 迪米特法则<br/><br/><strong>合</strong> → 合成复用原则</p><h2 id="1bca551d-035a-800d-9d02-d661344ed7b1" class="">Day04  召回+重排序</h2><p id="1bca551d-035a-8021-a2e9-d2980eff1a72" class="">迭代器？</p><ul id="1bda551d-035a-800a-ba5e-c815d2f52b97" class="toggle"><li><details open=""><summary>main.py</summary><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="1bda551d-035a-803a-9798-c2c610079202" class="code"><code class="language-Python"># 提问：骨折了怎么办？ ，大模型回答。
# 1.解析txt，向量化，存在向量数据库中。
# 2.问题向量化，检索数据库，召回（2种召回方式）
# 3.重排序
# 4得到结果返回给大模型，输出
#---------------------------------------------------------
#基于BM25算法的文档检索功能
from langchain_community.retrievers import BM25Retriever
#文档对象，对文档处理和操作
from langchain_core.documents import Document
#数据类型
from typing import List
#中文分词工具，可以将中文的文本分割成单个词语
import jieba
#实现BM25算法
from rank_bm25 import BM25Okapi
#长文本按照一定的规则递归的分割成较小的文本块
from langchain_text_splitters import RecursiveCharacterTextSplitter
#高效处理向量数据库的库
from langchain_community.vectorstores import FAISS
#从文本文件中加载文档数据
from langchain_community.document_loaders import TextLoader
#与智谱AI进行交互，实现聊天对话功能
from langchain_community.chat_models import ChatZhipuAI
#基于_huggingface 模型嵌入向量，将文本转换为向量表示
from langchain_huggingface import HuggingFaceEmbeddings
#智谱AI
from zhipuai import ZhipuAI

#print(list(jieba.cut(&#x27;今天的天气特别缓和&#x27;)))

#1.加载文档
loader =TextLoader(r&#x27;C:\Users\admin\Desktop\rag_rank\medical_data.txt&#x27;,encoding=&#x27;utf-8&#x27;)
#加载
document=loader.load()
#切割
text_splitter=RecursiveCharacterTextSplitter(
    chunk_size=500,
    chunk_overlap=0,
    length_function=len,
    separators=[&#x27;\n&#x27;]
)
docs =text_splitter.split_documents(document)
#打印加载的文档
#print(docs)

#2.分词
def preprocessing_func(text:str):
    return list(jieba.cut(text))

#3.1创建BM25召回器
#得到每个内容的片段
texts=[i.page_content for i in docs]
#调用分词器，得到分词内容
texts_processed=[preprocessing_func(t) for t in texts]
#得到召回器
vectorizer=BM25Okapi(texts_processed)

#3.2创建向量的相似度召回器
#本地模型文件
model_path = r&#x27;E:\bge-large-zh-v1___5&#x27;
#创建
embeddings=HuggingFaceEmbeddings(model_name=model_path)

#第一次运行,保存数据
#db=FAISS.from_documents(docs,embeddings)
#指定保存路径
#db.save_local(r&#x27;C:\Users\xupengcheng\PycharmProjects\pyRAGDay08\rag_rank\data&#x27;)

#第二次运行，加载数据
db =FAISS.load_local(
    r&#x27;C:\Users\admin\Desktop\rag_rank\data&#x27;,
    embeddings=embeddings,
    allow_dangerous_deserialization=True #关闭警告
)
#4.1 BM25召回器 召回
bm25_res =vectorizer.get_top_n(preprocessing_func(&#x27;骨折了怎么办?&#x27;),texts,n=10)
#print(&#x27;使用BM25召回器召回:&#x27;,bm25_res)
#print(&quot;=&quot;*50)
#4.2 向量召回器 召回
vector_res=db.similarity_search(&#x27;骨折了怎么办?&#x27;,k=10)
#print(&#x27;向量数据库召回:&#x27;,vector_res)

#5.重排序（RRF） 把两路的召回结果相加，排序，得到key-value集合（得分，文档）,返回 文档
def rrf(vector_res:list[str],text_res:list[str],k:int =10 ,m:int =60):
    &quot;&quot;&quot;
    score = 1/(m+r1) +1/(m+r2)
    :param vector_res:向量召回的结果
    :param text_res:BM25召回的结果
    :param k:排序后的钱K个值
    :param m:超参数(1-60)
    :return:排序后结果
    &quot;&quot;&quot;
    doc_scores={}
    #rank 得分，每一个文档的id对应的得分
    #第一个
    for rank,doc_id in enumerate(vector_res):
        doc_scores[doc_id]= doc_scores.get(doc_id,0)+1/(m+rank)
    #第二个
    for rank,doc_id in enumerate(text_res):
        doc_scores[doc_id]= doc_scores.get(doc_id,0)+1/(m+rank)

    #用户只关心文本，不想要得分，所以，只取文档内容
    sorted_results=[doc for doc,_ in sorted(doc_scores.items(),key=lambda x:x[1],reverse=True)[:k]]

    return sorted_results

#6.执行重排序

#RAG：
#限定大语言模型不能使用超出索引数据之外的数据，必须根据检索结果，再回答。
#否则RAG的功能失效了，就是为了避免大模型自身存在的幻觉缺陷，不让它回答除了检索以外的结果



#文档生成器构建

#返回向量
vector_results =[i.page_content for i in vector_res]
#返回BM25
text_results=[i for i in bm25_res]

#调用重排序函数
rrf_res=rrf(vector_results,text_results)
#print(&quot;=&quot;*100)
#print(&quot;使用重排序后的结果:&quot;,rrf_res)

#使用ai把检索出来的文档结果发给它，整合，输出

#定义一个字符串模版，prompt,用于模型提问提示信息

prompt=&quot;&quot;&quot;
任务目标：根据检索出的文档回答用户问题
任务要求：
    1、不得脱离检索出的文档回答问题
    2、若检索出的文档不包含用户问题的答案，请回答我不知道
用户问题:
{}
检索出的文档:
{}
&quot;&quot;&quot;

#创建一个智谱对象，指定模型
model=ChatZhipuAI(
    model=&#x27;glm-4-plus&#x27;,
    api_key=&#x27;7b16305a776744253ac1bad218a8f90c.p1EJxuKGhDmPKxfF&#x27;
)
#调用大模型 invoke，传入格式化后的提示信息，向模型提问，得到回答
res =model.invoke(prompt.format(&#x27;骨折了应该怎么办?&#x27;,&#x27;&#x27;.join(rrf_res)))
print(res.content)</code></pre></details></li></ul><h2 id="1bca551d-035a-80d5-aaa5-c07023ef9cbc" class="">Day05  文档梳理 RAG（检索增强生成）</h2><p id="1c2a551d-035a-8036-ae79-f27813ae7cf2" class="">
</p><h1 id="1bda551d-035a-8006-bc77-fe92314b556f" class="">WK4 项目实战</h1><h2 id="1c2a551d-035a-80a1-9ca3-c68c09c214fe" class="">Day01  添加单选框</h2><ul id="1c2a551d-035a-80b6-b5f8-e8d7ab003afa" class="toggle"><li><details open=""><summary>file_to_chroma.py（xlsx和json）</summary><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="1c2a551d-035a-804a-ae03-ea69eeb23ce4" class="code"><code class="language-JavaScript">import json
import os.path
#从指定的pdf文件中提取所有文本内容

import dashscope
import chromadb
import openpyxl
from embedding import embed_with_glm,embed_with_qwen

client =chromadb.HttpClient(host=&#x27;localhost&#x27;,port=8000)

#解析excel文件
def xlsx_parse(file_path):
    workbook=openpyxl.load_workbook(file_path)
    sheet =workbook.active
    text=&#x27;&#x27;
    for row in sheet.iter_rows(values_only=True):
        text+=str(row)
    return text

#解析pdf文件
def json_parse(file_path):
    with open(file_path,&#x27;r&#x27;,encoding=&#x27;utf-8&#x27;) as file:
        return json.load(file)

def split_text_with_overlap(text,chunk_size=1000,overlap=100):
    &quot;&quot;&quot;
    将文本拆分为指定大小的段落，每个段落之间有重叠部分
    :param text: 要处理的字符串
    :param chunk_size: 每个段落的长度
    :param overlap: 重叠字符数
    :return: 拆分后的段落列表
    &quot;&quot;&quot;
    start =end =0
    text_list=[]
    while end&lt;len(text):
        end =min(start+chunk_size,len(text)) #计算当前段落的结束位置，获取chunk_size和文本长度的较小值
        text_list.append(text[start:end])#将当前段落追加到列表中
        start =end - overlap #更新起始位置
    return text_list

def run(file_path:str,model_name,model_platform,dbname):
    text=&#x27;&#x27;
    #1.解析
    if file_path.endswith(&#x27;.pdf&#x27;): #判断是.pdf格式文件，调用解析函数
        pass

    elif file_path.endswith(&#x27;.xlsx&#x27;): #判断是xlsx格式文件，调用解析函数
        text=xlsx_parse(file_path)

    #2.拆分
    text_list =split_text_with_overlap(text) #调用函数解析文本，返回拆分后段落列表
    embedding_list=[] #存储文本的嵌入向量
    ids_list=[] #存储文档唯一标识
    document_list=text_list #拆分后的文本、段落列表
    metadatas_list=[] #存储元数据

    #3. 调用模型
    if model_platform == &#x27;ZhipuAI&#x27;:
        embedding_list=embed_with_glm(text_list,model_name) #调用向量化zhipu函数，返回嵌入向量
    elif model_platform == &#x27;Bailian&#x27;:
        embedding_list=embed_with_qwen(text_list,model_name)#调用向量化qwen函数，返回嵌入向量

    #4. 构造向量数据库所需要的格式
    if type(embedding_list) == list:

        collection =client.create_collection(dbname) #创建指定名称的集合
        basename = os.path.basename(file_path) #获取文件路径的文件名部分
        #遍历文档列表
        for i in range(len(text_list)):
            ids_list.append(str(basename)+str(i)) #生成唯一标识添加到ids_list中s
            metadatas_list.append({&#x27;file_name&#x27;:str(basename)}) #生成元数据并添加到metadatas_list中

        #构造向量数据库，加入数据
        collection.add(
            ids=ids_list,
            embeddings=embedding_list,
            documents=document_list,
            metadatas=metadatas_list

        )
    else:
        return &#x27;error&#x27;

    return &#x27;success&#x27;

if __name__ == &#x27;__main__&#x27;:
    file_path =r&#x27;C:\Users\xupengcheng\PycharmProjects\pyRAGDay09\rag_gk\data_path\1_大学院校基础信息.xlsx&#x27;
    model_name=&#x27;embedding-3&#x27; #指定模型名称
    #model_name = dashscope.TextEmbedding.Models.text_embedding_v3  # 指定模型名称
    model_patform=&#x27;ZhipuAI&#x27;
    dbname=&#x27;col02&#x27;
    print(run(file_path,model_name,model_patform,dbname))
    print(client.get_collection(dbname).peek())</code></pre></details></li></ul><ul id="1c2a551d-035a-8004-8883-cdcbc215933a" class="toggle"><li><details open=""><summary>RAG.py</summary><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="1c2a551d-035a-80ad-92d1-c6bb5a18887f" class="code"><code class="language-JavaScript">import chromadb
from zhipuai import ZhipuAI
from embedding import embed_with_glm,embed_with_qwen

client =chromadb.HttpClient(host=&#x27;localhost&#x27;,port=8000)

def chatglm_stream(question,history_list:list):
    chient =ZhipuAI(api_key=&#x27;&#x27;)
    history_list.append({&#x27;role&#x27;:&#x27;user&#x27;,&#x27;content&#x27;:f&#x27;{question}&#x27;})# 用户提问的问题添加到历史对话列表中
    response=chient.chat.completions.create(
        model=&#x27;glm-4&#x27;,
        messages=history_list,
        stream=True #设置为流式返回结果
    )
    for chunk in response:
        output =chunk.choices[0].content
        yield output

class Robot():
    def __init__(self):
        self.embedding =&#x27;&#x27;
    def RAG(self,question,dbname):
        #使用函数生成问题的嵌入向量
        search_embedding=embed_with_glm([question],model_name=&#x27;embedding-3&#x27;)
        col=client.get_collection(name=dbname)
        result =col.query(
            query_embeddings=search_embedding, #传入问题的嵌入向量进行查询
            n_results=3 #设置返回的结果数3条
        )
        result_all=&#x27;&#x27;
        #遍历查询结果中的文档列表
        for i in range(len(result[&#x27;documents&#x27;][0])):
            result_document =result[&#x27;documents&#x27;][0][i] #文档内容
            result_file =result[&#x27;metadatas&#x27;][0][i][&#x27;file_name&#x27;]
            result_all+=f&#x27;{result_document}+\n【参考资料】:{result_file}\n\n&#x27;
        return result_all #返回拼接好的结果内容</code></pre></details></li></ul><ul id="1c2a551d-035a-80b7-b4ae-d93c759ece42" class="toggle"><li><details open=""><summary>embedding.py</summary><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="1c2a551d-035a-80a4-87db-c48d6547b3de" class="code"><code class="language-JavaScript">import dashscope
from zhipuai import ZhipuAI
from http import HTTPStatus

#函数 qwen模型生成文本嵌入
def embed_with_qwen(text_list,model_name):
    dashscope.api_key=&#x27;&#x27;
    embedding_list=[]
    for text in text_list:
        try:
            resp =dashscope.TextEmbedding.call(
                model=model_name,
                input=text
            )
            if resp.status_code == HTTPStatus.OK:
                embedding_list.append(resp.output[&#x27;embeddings&#x27;][0][&#x27;embedding&#x27;])

        except Exception as e:
            print(&#x27;调用qwen嵌入信息出错{e}&#x27;)
    return embedding_list

#函数 zhipu模型生成文本嵌入
def embed_with_glm(text_list,model_name):
    embedding_list=[]
    api_key=&#x27;&#x27;
    client=ZhipuAI(api_key=api_key)
    for text in text_list:
        try:
            resp=client.embeddings.create(
                input=text,
                model=model_name
            )
            embedding_list.append(resp.data[0].embedding)
        except Exception as e:
            print(&#x27;调用zhipu嵌入信息出错{e}&#x27;)

    return embedding_list

if __name__ == &#x27;__main__&#x27;:
    print(embed_with_glm([&#x27;今天的天气不错&#x27;,&#x27;今天星期一&#x27;],&#x27;embedding-3&#x27;))</code></pre></details></li></ul><ul id="1c2a551d-035a-80de-bd4c-c82b0e14b4da" class="toggle"><li><details open=""><summary>main.py</summary><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="1c2a551d-035a-80f4-8bf4-d5f28ce828cb" class="code"><code class="language-JavaScript">import chromadb
import gradio as gr
from file_to_chroma import run as save_chroma
from zhipuai import ZhipuAI
from RAG import Robot

def build_db(file,model_name,model_platform,dbname):
    res =save_chroma(file,model_name,model_platform,dbname)
    if res ==&#x27;success&#x27;:
        gr.Info(&#x27;数据库创建成功&#x27;,duration=2)
    elif res == &#x27;error&#x27;:
        gr.Error(&quot;数据库创建失败&quot;,duration=2)

def update_dropdown(model_platform):
    if model_platform ==&#x27;ZhipuAI&#x27;:
        return gr.update(choices=[&#x27;embedding-2&#x27;,&#x27;embedding-3&#x27;],value=&#x27;embedding-3&#x27;)
    elif model_platform ==&#x27;Bailian&#x27;:
        return gr.update(choices=[&#x27;text-embedding-v1&#x27;, &#x27;text-embedding-v2&#x27;,&#x27;text-embedding-v3&#x27;], value=&#x27;text-embedding-v3&#x27;)
    else:
        return gr.update(choices=[],value=None)

def user(user_message,history:list):
    if len(history) == 0:
        history.append({&quot;role&quot;: &quot;system&quot;, &quot;content&quot;: (
            &quot;你是一个高考志愿智能问答系统，对于用户提问的问题，你需要按照给出的【参考资料】对问题进行回答。&quot;
            &quot;你的回答需要按照以下两个步骤：&quot;
            &quot;1.分析用户问题和参考资料，判断是否有【参考资料】可以解答用户的问题，如果有则说明【参考资料】的名称，&quot;
            &quot;如果没有，则首先告知用户没有任何可参考的资料，需要注意答案的准确性。&quot;
            &quot;2.根据资料内容对问题进行解答，若用户希望根据高考分数得到志愿推荐，那么首先关注学校的投档分，越接近越好，从好的学校推荐，并结合【参考资料】中的学校信息来说明。&quot;)
                        })
    history.append({&#x27;role&#x27;:&#x27;user&#x27;,&quot;content&quot;:user_message})
    return &#x27;&#x27;,history

def get_db_list():
    client =chromadb.HttpClient() # 默认值,localhost,8000
    collections =client.list_collections()
    return  collections

def dbref():
    dblist=get_db_list()
    return gr.update(choices=dblist,multiselect=False) #multiselect禁止多选

def delete_db(dbchoice):
    client =chromadb.HttpClient()
    client.delete_collection(dbchoice)
    gr.Info(&quot;数据库删除成功&quot;,duration=2)


def bot(history:list,dbname):
    input_text=history[-1][&#x27;content&#x27;] #用户最新输入的文本
    result_rag =robot.RAG(input_text,dbname) #获取相关资料
    history[-1][&#x27;content&#x27;]+=f&#x27;\n【参考资料】：\n{result_rag}&#x27;

    client =ZhipuAI(api_key=&#x27;7b16305a776744253ac1bad218a8f90c.p1EJxuKGhDmPKxfF&#x27;)
    messages =history
    response =client.chat.completions.create(
        model=&#x27;glm-4&#x27;,
        messages=messages,
        stream=True
    )
    parts =history[-1][&#x27;content&#x27;].split(&#x27;参考资料：&#x27;,1)
    history[-1][&#x27;content&#x27;]=parts[0].strip() #提取第一个参考资料前的内容
    history.append({&#x27;role&#x27;:&#x27;assistant&#x27;,&#x27;content&#x27;:&quot;&quot;}) #添加一个空的助手回复
    for chunk in response:
        history[-1][&#x27;content&#x27;]+=chunk.choices[0].delta.content #逐步添加回复内容
        yield  history

robot=Robot()
dblist=get_db_list() #获取数据库列表

with gr.Blocks() as demo:
    with gr.Row():
        gr.Markdown(&#x27;# 高考政策通&#x27;)
    with gr.Row():
        with gr.Column():
            chatbot=gr.Chatbot(label=&#x27;对话框&#x27;,type=&#x27;messages&#x27;,height=500) #创建聊天组件
            question=gr.Textbox(label=&#x27;请输入&#x27;) # 创建输入文本框
            clear_btn=gr.Button(&#x27;clear&#x27;) #创建清除按钮
            gr.Examples([&#x27;介绍一下北京大学&#x27;,&#x27;介绍一下清华大学&#x27;],inputs=question) #  创建示例输入
        with gr.Column():
            gr.Markdown(&#x27;### 请选择嵌入模型&#x27;)
            model_platform=gr.Radio(label=&#x27;嵌入模型&#x27;,choices=[&#x27;ZhipuAI&#x27;,&#x27;Bailian&#x27;],value=&#x27;ZhipuAI&#x27;) #创建单选框，默认值ZhipuAI
            model_name=gr.Dropdown(label=&#x27;模型名称&#x27;,choices=[&#x27;embedding-2&#x27;,&#x27;embedding-3&#x27;],value=&#x27;embedding-3&#x27;) #模型名称下拉框
            model_platform.change(fn=update_dropdown,inputs=model_platform,outputs=model_name) #监听模型选择变化，更新模型名称下的内容
            datafile=gr.File(label=&#x27;上传文件&#x27;,type=&#x27;filepath&#x27;)#创建上传组件
            dbname=gr.Textbox(label=&#x27;数据库名称&#x27;)
            build_btn =gr.Button(&#x27;开始构建&#x27;)
            dbchoice =gr.Dropdown(label=&#x27;数据库名称&#x27;,choices=dblist)
            with gr.Row():
                dbref_btn =gr.Button(&#x27;刷新&#x27;)
                dbdel_btn =gr.Button(&#x27;删除&#x27;)
    question.submit(user,inputs=[question,chatbot],outputs=[question,chatbot]).then(bot,[chatbot,dbchoice],chatbot)
    clear_btn.click(lambda :None,None,chatbot,queue=False)
    build_btn.click(build_db,[datafile,model_name,model_platform,dbname],[])
    dbref_btn.click(dbref,outputs=dbchoice)#刷新按钮时间
    dbdel_btn.click(delete_db,inputs=dbchoice) #删除按钮事件

demo.launch()</code></pre></details></li></ul><h2 id="1c2a551d-035a-804f-afb4-c9b123157fc3" class="">Day02  新数据库sqlite3</h2><ul id="1c2a551d-035a-80b9-8621-d16d756c1998" class="toggle"><li><details open=""><summary>main.py</summary><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="1c2a551d-035a-80a8-a571-e8bae0715b72" class="code"><code class="language-JavaScript">import  gradio as gr
import sqlite3
from zhipuai import ZhipuAI
from RAG import Robot
import pandas as pd #用于数据处理和分析

def build_db(datafile,dbname):
    dataframe=pd.read_excel(datafile) #使用pandas解析上传的文件，存储在dataframe
    #数据处理
    dataframe=dataframe.dropna() #删除内容中的所有空值行，提高数据质量
    # 将内容，列名中的空格替换为下划线，使列名更规范
    dataframe.columns =[col.replace(&#x27; &#x27;,&#x27;_&#x27;) for col in dataframe.columns]

    #创建sqlite数据库
    conn =sqlite3.connect(f&#x27;{dbname}.db&#x27;)
    #将dataframe中的数据写入到数据库的mytable表中，若存在则替换，不写入索引列
    dataframe.to_sql(&#x27;mytable&#x27;,conn,if_exists=&#x27;replace&#x27;,index=False)
    cursor =conn.cursor()
    cursor.execute(&quot;select name from sqlite_master where type=&#x27;table&#x27; and name=&#x27;mytable&#x27;&quot;)
    tables =cursor.fetchall()
    if tables:
        gr.Info(&#x27;数据库创建成功&#x27;,duration=2)
    else:
        gr.Info(&#x27;数据库创建失败&#x27;,duration=2)

    conn.close() #释放资源



def user(question,history:list):
    if len(history) ==0 :
        prompt =&quot;&quot;&quot;
            你是一个高考志愿智能问答系统，对于用户提问的问题，你需要按照给出的【查询结果】对问题进行回答。
            你的回答需要按照以下两个步骤：
            1.分析用户问题和查询结果，判断是否有【查询结果】可以解答用户的问题，如果有则说明【查询结果】的名称，如果没有，则首先告知用户没有任何可参考的资料，需要主要答案的准确性。
            2.根据资料内容对问题进行解答，若用户希望根据高考分数得到志愿推荐，那么首先关注学校的投档分，从好的学校推荐，并结合【查询结果】中的学校信息来说明。
        &quot;&quot;&quot;
        history.append({&#x27;role&#x27;:&#x27;system&#x27;,&#x27;content&#x27;:prompt})
    history.append({&#x27;role&#x27;:&#x27;user&#x27;,&#x27;content&#x27;:question})
    return &quot;&quot;,history

def bot(history:list,dbname):
    question =history[-1][&#x27;content&#x27;] #获取历史中最后一个用户问题
    robot =Robot(dbname) #初始化类，创建对象，把数据库名传入
    result =robot.query_db(question) #调用Robot的query_db函数，返回查询数据库获得结果
    history[-1][&#x27;content&#x27;]+=f&#x27;\n【查询结果】{result}&#x27; # 将查询结果添加到最后一个用户问题的内容中
    client =ZhipuAI(api_key=&#x27;6214aaa7fcde44bdbeec762dbed4101c.6ktZ2m9834ZrlAW5&#x27;)
    resp =client.chat.completions.create(
        model=&#x27;glm-4&#x27;,
        messages=history, #将对话历史作为输入消息
        stream=True #设置为流式返回结果
    )
    history[-1][&#x27;content&#x27;] =history[-1][&#x27;content&#x27;].split(&#x27;【查询结果】&#x27;,1)[0].strip()
    history.append({&#x27;role&#x27;:&#x27;assistant&#x27;,&#x27;content&#x27;:&#x27;&#x27;}) #空助手回答记录
    for chunk in resp:
        history[-1][&#x27;content&#x27;]+=chunk.choices[0].delta.content # 逐步将流式返回回答内容添加到助手回答记录中
        yield history #实时更新聊天界面


with gr.Blocks() as demo:
    with gr.Row():
        gr.Markdown(&quot;# 高考政策通&quot;)
    with gr.Row():
        with gr.Column():
            chatbot = gr.Chatbot(label=&#x27;对话框&#x27;, type=&quot;messages&quot;, height=500)
            question = gr.Textbox(label=&#x27;请输入&#x27;)
            chear_btn = gr.Button(&quot;clear&quot;)
        with gr.Column():
            gr.Markdown(&quot;### 数据库构建&quot;)
            datafile = gr.File(label=&#x27;上传文件&#x27;, type=&#x27;filepath&#x27;)
            dbname = gr.Textbox(label=&#x27;数据库名称&#x27;)
            build_btn=gr.Button(&quot;开始构建&quot;)
            gr.Markdown(&quot;### 数据库选择&quot;)
            dbchoose = gr.Textbox(label=&#x27;数据库名称&#x27;)
            with gr.Row():
                dbdelete_btn = gr.Button(&quot;删除&quot;)
    #清空按钮事件
    chear_btn.click(lambda :None,inputs=None,outputs=chatbot,queue=False)
    question.submit(user,inputs=[question,chatbot],outputs=[question,chatbot]).then(bot,[chatbot,dbchoose],chatbot)
    build_btn.click(build_db,inputs=[datafile,dbname])


demo.launch()</code></pre></details></li></ul><ul id="1c2a551d-035a-801d-bb86-ef543cf81d0e" class="toggle"><li><details open=""><summary>RAG.py</summary><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="1c2a551d-035a-8078-b426-fb4bc27587f4" class="code"><code class="language-JavaScript">import sqlite3
from zhipuai import ZhipuAI

class Robot:
    def __init__(self,dbname):
        self.dbname =dbname

    def query_db(self,question):
        conn =sqlite3.connect(f&#x27;{self.dbname}.db&#x27;) #链接到指定的数据库名  .db
        cursor =conn.cursor() #创建游标
        prompt = &quot;&quot;&quot;
                您是SQL专家级分析师。在适当的时候，根据用户问题和数据库架构生成SQL查询。
                你的输出有且仅能是字符串类型的SQL查询语句,必须符合sql语句语法，可以执行的语句
                例如：
                question：介绍一下北京大学
                answer：SELECT * FROM mytable WHERE 中文名字 = &#x27;北京大学&#x27;

                数据库架构：
                database_schema: [
            {
                &quot;table&quot;: &quot;mytable&quot;,
                &quot;columns&quot;: [
                    {&quot;name&quot;: &quot;中文名字&quot;, &quot;type&quot;: &quot;TEXT&quot;},
                    {&quot;name&quot;: &quot;排名&quot;, &quot;type&quot;: &quot;INTEGER&quot;},
                    {&quot;name&quot;: &quot;简称&quot;, &quot;type&quot;: &quot;TEXT&quot;},
                    {&quot;name&quot;: &quot;英文名字&quot;, &quot;type&quot;: &quot;TEXT&quot;},
                    {&quot;name&quot;: &quot;学校代码&quot;, &quot;type&quot;: &quot;REAL&quot;},
                    {&quot;name&quot;: &quot;所在省份&quot;, &quot;type&quot;: &quot;TEXT&quot;},
                    {&quot;name&quot;: &quot;所在城区&quot;, &quot;type&quot;: &quot;TEXT&quot;},
                    {&quot;name&quot;: &quot;创建时间&quot;, &quot;type&quot;: &quot;TEXT&quot;},
                    {&quot;name&quot;: &quot;女生比例&quot;, &quot;type&quot;: &quot;INTEGER&quot;},
                    {&quot;name&quot;: &quot;男生比例&quot;, &quot;type&quot;: &quot;INTEGER&quot;},
                    {&quot;name&quot;: &quot;自然类型&quot;, &quot;type&quot;: &quot;TEXT&quot;},
                    {&quot;name&quot;: &quot;学校类型&quot;, &quot;type&quot;: &quot;TEXT&quot;},
                    {&quot;name&quot;: &quot;所属机构&quot;, &quot;type&quot;: &quot;TEXT&quot;},
                    {&quot;name&quot;: &quot;简单标签&quot;, &quot;type&quot;: &quot;TEXT&quot;},
                    {&quot;name&quot;: &quot;是否艺术&quot;, &quot;type&quot;: &quot;INTEGER&quot;},
                    {&quot;name&quot;: &quot;是否985&quot;, &quot;type&quot;: &quot;INTEGER&quot;},
                    {&quot;name&quot;: &quot;是否211&quot;, &quot;type&quot;: &quot;INTEGER&quot;},
                    {&quot;name&quot;: &quot;是否国重点&quot;, &quot;type&quot;: &quot;INTEGER&quot;},
                    {&quot;name&quot;: &quot;是否是私立&quot;, &quot;type&quot;: &quot;INTEGER&quot;},
                    {&quot;name&quot;: &quot;世界排名&quot;, &quot;type&quot;: &quot;TEXT&quot;},
                    {&quot;name&quot;: &quot;排名汇总&quot;, &quot;type&quot;: &quot;TEXT&quot;},
                    {&quot;name&quot;: &quot;硕士点&quot;, &quot;type&quot;: &quot;INTEGER&quot;},
                    {&quot;name&quot;: &quot;博士点&quot;, &quot;type&quot;: &quot;INTEGER&quot;},
                    {&quot;name&quot;: &quot;世界一流&quot;, &quot;type&quot;: &quot;TEXT&quot;},
                    {&quot;name&quot;: &quot;一流大学&quot;, &quot;type&quot;: &quot;TEXT&quot;},
                    {&quot;name&quot;: &quot;汇总标签&quot;, &quot;type&quot;: &quot;TEXT&quot;},
                    {&quot;name&quot;: &quot;描述&quot;, &quot;type&quot;: &quot;TEXT&quot;},
                    {&quot;name&quot;: &quot;本科or专科&quot;, &quot;type&quot;: &quot;TEXT&quot;},
                    {&quot;name&quot;: &quot;招办电话&quot;, &quot;type&quot;: &quot;TEXT&quot;},
                    {&quot;name&quot;: &quot;电子邮箱&quot;, &quot;type&quot;: &quot;TEXT&quot;},
                    {&quot;name&quot;: &quot;通讯地址&quot;, &quot;type&quot;: &quot;TEXT&quot;},
                    {&quot;name&quot;: &quot;官网&quot;, &quot;type&quot;: &quot;TEXT&quot;},
                    {&quot;name&quot;: &quot;名人&quot;, &quot;type&quot;: &quot;TEXT&quot;},
                    {&quot;name&quot;: &quot;评估结果&quot;, &quot;type&quot;: &quot;TEXT&quot;},
                    {&quot;name&quot;: &quot;就业情况&quot;, &quot;type&quot;: &quot;TEXT&quot;},
                    {&quot;name&quot;: &quot;推荐专业&quot;, &quot;type&quot;: &quot;TEXT&quot;},
                    {&quot;name&quot;: &quot;图片ID&quot;, &quot;type&quot;: &quot;INTEGER&quot;},
                ]
            }
        ]
                &quot;&quot;&quot;
        clinet =ZhipuAI(api_key=&#x27;6214aaa7fcde44bdbeec762dbed4101c.6ktZ2m9834ZrlAW5&#x27;)
        response =clinet.chat.completions.create(
            model=&#x27;glm-4&#x27;,
            messages=[
                {&quot;role&quot;:&quot;system&quot;,&quot;content&quot;:prompt},{&quot;role&quot;:&quot;user&quot;,&quot;content&quot;:question}
            ],
            stream=False #设置非流式返回结果，一次性获取完成的相应信息
        )
        query =response.choices[0].message.content.strip()# 从响应的结果中提取生成的sql语句，去除字符串两端的空白字符
        print(f&#x27;生成的sql====================={query}&#x27;)
        cursor.execute(query) # 使用游标执行生成的sql语句
        results =cursor.fetchall() #获取查询结果，返回所有匹配的行，以列表的形式呈现
        conn.close()
        return results</code></pre></details></li></ul><h2 id="1c2a551d-035a-80fe-be51-dfacb154f787" class="">Day03  测验两数据库</h2><ul id="1c2a551d-035a-80f1-b453-d1b922cb871f" class="toggle"><li><details open=""><summary>测验源码</summary><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="1c2a551d-035a-8054-b898-db859c2c4f49" class="code"><code class="language-JavaScript">import torch
import dashscope
from http import HTTPStatus
import gradio as gr
import pandas as pd  # 用于数据处理和分析
import sqlite3
from zhipuai import ZhipuAI
import chromadb

def embed_with_qwen(text_list):
    dashscope.api_key = &#x27;sk-eb40ccf62d6648dc92a16ecff1686efa&#x27;
    embedding_list = []
    for text in text_list:
        resp = dashscope.TextEmbedding.call(
            model=dashscope.TextEmbedding.Models.text_embedding_v2,
            input=text
        )
        if resp.status_code == HTTPStatus.OK:
            embedding = resp.output[&#x27;embeddings&#x27;][0][&#x27;embedding&#x27;]
            embedding_list.append(embedding)
        else:
            print(resp)
            return resp
    return embedding_list


def embed_with_bge(text_list, tokenizer, model):
    encoded_input = tokenizer(text_list, max_length=512, padding=True, truncation=True, return_tensors=&#x27;pt&#x27;)
    with torch.no_grad():
        model_output = model(**encoded_input)
    sentence_embeddings = model_output[0][:, 0]
    sentence_embeddings_list = sentence_embeddings.tolist()
    return sentence_embeddings_list


def excel_parse(file):
    dataframe = pd.read_excel(file)  # 使用pandas解析上传的文件，存储在dataframe
    # 数据处理
    dataframe = dataframe.dropna()  # 删除内容中的所有空值行，提高数据质量
    # 将内容，列名中的空格替换为下划线，使列名更规范
    dataframe.columns = [col.replace(&#x27; &#x27;, &#x27;_&#x27;) for col in dataframe.columns]
    print(dataframe)
    result = []
    for _, row in dataframe.iterrows():
        k_qa_content = row[&#x27;name&#x27;]
        keyword, answer = k_qa_content.split(&#x27;#&#x27;, maxsplit=1)
        result.append([keyword, answer])
    return result


def run(file_path, dbname):
    text_list = excel_parse(file_path)
    print(text_list)
    keyword_list = []
    answer_list = []
    for text in text_list:
        keyword_list.append(text[0])
        answer_list.append({&quot;key&quot;: text[1]})
    embedding_list = embed_with_qwen(keyword_list)

    if type(embedding_list) == list:
        client = chromadb.HttpClient(host=&#x27;localhost&#x27;, port=8000)
        collection = client.get_or_create_collection(name=dbname)
        len_ids = len(text_list)
        ids_list = [str(dbname) + str(i) for i in range(len_ids)]
        collection.add(
            embeddings=embedding_list,
            documents=keyword_list,
            metadatas=answer_list,
            ids=ids_list
        )
    else:
        return &#x27;error&#x27;

    print(&#x27;向量化完成&#x27;)
    return &#x27;success&#x27;


client = chromadb.Client()


def chatglm_stream(input_text, history_list):
    client = ZhipuAI(api_key=&#x27;6214aaa7fcde44bdbeec762dbed4101c.6ktZ2m9834ZrlAW5&#x27;)
    messages = history_list
    messages.append({&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: f&quot;{input_text}&quot;})
    response = client.chat.completions.create(
        model=&#x27;glm-4&#x27;,
        messages=messages,
        stream=True,
    )
    for chunk in response:
        output = chunk.choices[0].delta.content
        yield output


class Robot():
    def __init__(self):
        self.embedding = &quot;&quot;

    def RAG(self, input_text, dbname):
        search_embedding = embed_with_qwen([input_text])
        collection = client.get_collection(name=dbname)
        result = collection.query(
            query_embeddings=search_embedding,
            n_results=3,
        )
        result_all = &quot;&quot;
        for i in range(len(result[&#x27;documents&#x27;][0])):
            result_documents = result[&#x27;documents&#x27;][0][i]
            result_file = result[&#x27;metadatas&#x27;][0][i][&#x27;key&#x27;]
            result_all += result_documents + f&quot;\n【参考资料】：{result_file}\n\n&quot;
        return result_all

    def run_test(self):
        history_list = [{&quot;role&quot;: &quot;system&quot;,
                         &quot;content&quot;: &quot;你是一个名叫Molly的教育专家，对于用户提问的问题，你需要按照给出的【参考资料】对问题进行回答。你的回答需要按照以下两个步骤：1.分析用户问题和参考资料，判断是否有【参考资料】可以解答用户的问题，如果有则说明【参考资料】的名称，如果没有，则首先告知用户没有任何可参考的资料，需要注意答案的准确性。2.根据资料内容对问题进行解答。&quot;}]
        while True:
            input_text = input(&#x27;请输入:&#x27;)
            if input_text == &#x27;exit&#x27;:
                break
            dbname = &#x27;test8899&#x27;
            result_rag = self.RAG(input_text, dbname)
            input_text = input_text + f&#x27;参考资料：\n{result_rag}&#x27;
            result = chatglm_stream(input_text, history_list)
            result_all = &quot;&quot;
            for res in result_all:
                result_all += res
                print(res, end=&quot;&quot;, flush=True)
            print(&quot;&quot;)
            message_input = {&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: f&quot;{input_text}&quot;}
            history_list.append(message_input)
            message_output = {&quot;role&quot;: &quot;assistant&quot;, &quot;content&quot;: f&quot;{result_all}&quot;}
            history_list.append(message_output)
            print(history_list)


def get_db_list():
    collections = client.list_collections()
    return collections


def build_db(file, dbname):
    try:
        res = run(file, dbname)
        if res == &quot;success&quot;:
            return gr.Info(&quot;数据库创建成功&quot;, duration=5)
        else:
            return gr.Error(&quot;数据库创建失败&quot;, duration=5)
    except Exception as e:
        print(f&quot;构建数据库出错：{e}&quot;)
        return gr.Error(&quot;数据库构建失败&quot;, duration=5)


def refresh_db_choices():
    dbList = get_db_list()
    return gr.update(choices=dbList)


def user(user_message, history: list):
    if len(history) == 0:
        history.append({&quot;role&quot;: &quot;system&quot;,
                        &quot;content&quot;: &quot;你是一个名叫Molly的教育专家，对于用户提问的问题，你需要按照给出的【参考资料】对问题进行回答。你的回答需要按照以下两个步骤：1.分析用户问题和参考资料，判断是否有【参考资料】可以解答用户的问题，如果有则说明【参考资料】的名称，如果没有，则首先告知用户没有任何可参考的资料，需要注意答案的准确性。2.根据资料内容对问题进行解答。&quot;})
    history.append({&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: user_message})
    return &quot;&quot;, history


def bot(history: list, dbname):
    input_text = history[-1][&#x27;content&#x27;]
    result_rag = robot.RAG(input_text, dbname)
    history[-1][&#x27;content&#x27;] += f&quot;\n参考资料：\n{result_rag}&quot;
    client_zhipu = ZhipuAI(api_key=&#x27;6214aaa7fcde44bdbeec762dbed4101c.6ktZ2m9834ZrlAW5&#x27;)
    messages = history
    response = client_zhipu.chat.completions.create(
        model=&quot;glm-4&quot;,
        messages=messages,
        stream=True
    )
    parts = history[-1][&#x27;content&#x27;].split(&quot;参考资料：&quot;, 1)
    history[-1][&#x27;content&#x27;] = parts[0].strip()
    history.append({&quot;role&quot;: &quot;assistant&quot;, &quot;content&quot;: &quot;&quot;})

    for chunk in response:
        history[-1][&#x27;content&#x27;] += chunk.choices[0].delta.content
        yield history


robot = Robot()
dbList = get_db_list()


def switch_interface(interface_name):
    if interface_name == &quot;sqlite3&quot;:
        return gr.update(visible=True), gr.update(visible=False)
    elif interface_name == &quot;chroma&quot;:
        return gr.update(visible=False), gr.update(visible=True)
switch_interface(&quot;sqlite3&quot;)

class Robot_gaokao:
    def __init__(self, dbname):
        self.dbname = dbname

    def query_db(self, question):
        conn = sqlite3.connect(f&#x27;{self.dbname}.db&#x27;)  # 链接到指定的数据库名  .db
        cursor = conn.cursor()  # 创建游标
        prompt = &quot;&quot;&quot;
                您是SQL专家级分析师。在适当的时候，根据用户问题和数据库架构生成SQL查询。
                你的输出有且仅能是字符串类型的SQL查询语句,必须符合sql语句语法，可以执行的语句,不允许输出解释性语言
                例如：
                question：介绍一下北京大学
                answer：SELECT * FROM mytable WHERE 中文名字 = &#x27;北京大学&#x27;

                数据库架构：
                database_schema: [
            {
                &quot;table&quot;: &quot;mytable&quot;,
                &quot;columns&quot;: [
                    {&quot;name&quot;: &quot;中文名字&quot;, &quot;type&quot;: &quot;TEXT&quot;},
                    {&quot;name&quot;: &quot;排名&quot;, &quot;type&quot;: &quot;INTEGER&quot;},
                    {&quot;name&quot;: &quot;简称&quot;, &quot;type&quot;: &quot;TEXT&quot;},
                    {&quot;name&quot;: &quot;英文名字&quot;, &quot;type&quot;: &quot;TEXT&quot;},
                    {&quot;name&quot;: &quot;学校代码&quot;, &quot;type&quot;: &quot;REAL&quot;},
                    {&quot;name&quot;: &quot;所在省份&quot;, &quot;type&quot;: &quot;TEXT&quot;},
                    {&quot;name&quot;: &quot;所在城区&quot;, &quot;type&quot;: &quot;TEXT&quot;},
                    {&quot;name&quot;: &quot;创建时间&quot;, &quot;type&quot;: &quot;TEXT&quot;},
                    {&quot;name&quot;: &quot;女生比例&quot;, &quot;type&quot;: &quot;INTEGER&quot;},
                    {&quot;name&quot;: &quot;男生比例&quot;, &quot;type&quot;: &quot;INTEGER&quot;},
                    {&quot;name&quot;: &quot;自然类型&quot;, &quot;type&quot;: &quot;TEXT&quot;},
                    {&quot;name&quot;: &quot;学校类型&quot;, &quot;type&quot;: &quot;TEXT&quot;},
                    {&quot;name&quot;: &quot;所属机构&quot;, &quot;type&quot;: &quot;TEXT&quot;},
                    {&quot;name&quot;: &quot;简单标签&quot;, &quot;type&quot;: &quot;TEXT&quot;},
                    {&quot;name&quot;: &quot;是否艺术&quot;, &quot;type&quot;: &quot;INTEGER&quot;},
                    {&quot;name&quot;: &quot;是否985&quot;, &quot;type&quot;: &quot;INTEGER&quot;},
                    {&quot;name&quot;: &quot;是否211&quot;, &quot;type&quot;: &quot;INTEGER&quot;},
                    {&quot;name&quot;: &quot;是否国重点&quot;, &quot;type&quot;: &quot;INTEGER&quot;},
                    {&quot;name&quot;: &quot;是否是私立&quot;, &quot;type&quot;: &quot;INTEGER&quot;},
                    {&quot;name&quot;: &quot;世界排名&quot;, &quot;type&quot;: &quot;TEXT&quot;},
                    {&quot;name&quot;: &quot;排名汇总&quot;, &quot;type&quot;: &quot;TEXT&quot;},
                    {&quot;name&quot;: &quot;硕士点&quot;, &quot;type&quot;: &quot;INTEGER&quot;},
                    {&quot;name&quot;: &quot;博士点&quot;, &quot;type&quot;: &quot;INTEGER&quot;},
                    {&quot;name&quot;: &quot;世界一流&quot;, &quot;type&quot;: &quot;TEXT&quot;},
                    {&quot;name&quot;: &quot;一流大学&quot;, &quot;type&quot;: &quot;TEXT&quot;},
                    {&quot;name&quot;: &quot;汇总标签&quot;, &quot;type&quot;: &quot;TEXT&quot;},
                    {&quot;name&quot;: &quot;描述&quot;, &quot;type&quot;: &quot;TEXT&quot;},
                    {&quot;name&quot;: &quot;本科or专科&quot;, &quot;type&quot;: &quot;TEXT&quot;},
                    {&quot;name&quot;: &quot;招办电话&quot;, &quot;type&quot;: &quot;TEXT&quot;},
                    {&quot;name&quot;: &quot;电子邮箱&quot;, &quot;type&quot;: &quot;TEXT&quot;},
                    {&quot;name&quot;: &quot;通讯地址&quot;, &quot;type&quot;: &quot;TEXT&quot;},
                    {&quot;name&quot;: &quot;官网&quot;, &quot;type&quot;: &quot;TEXT&quot;},
                    {&quot;name&quot;: &quot;名人&quot;, &quot;type&quot;: &quot;TEXT&quot;},
                    {&quot;name&quot;: &quot;评估结果&quot;, &quot;type&quot;: &quot;TEXT&quot;},
                    {&quot;name&quot;: &quot;就业情况&quot;, &quot;type&quot;: &quot;TEXT&quot;},
                    {&quot;name&quot;: &quot;推荐专业&quot;, &quot;type&quot;: &quot;TEXT&quot;},
                    {&quot;name&quot;: &quot;图片ID&quot;, &quot;type&quot;: &quot;INTEGER&quot;},
                ]
            }
        ]
                &quot;&quot;&quot;
        client = ZhipuAI(api_key=&#x27;6214aaa7fcde44bdbeec762dbed4101c.6ktZ2m9834ZrlAW5&#x27;)
        response = client.chat.completions.create(
            model=&#x27;glm-4&#x27;,
            messages=[
                {&quot;role&quot;: &quot;system&quot;, &quot;content&quot;: prompt}, {&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: question}
            ],
            stream=False  # 设置非流式返回结果，一次性获取完成的相应信息
        )
        query = response.choices[0].message.content.strip()  # 从响应的结果中提取生成的sql语句，去除字符串两端的空白字符
        print(f&#x27;生成的sql====================={query}&#x27;)
        cursor.execute(query)  # 使用游标执行生成的sql语句
        results = cursor.fetchall()  # 获取查询结果，返回所有匹配的行，以列表的形式呈现
        conn.close()
        return results


def build_db_gaokao(datafile, dbname):
    dataframe = pd.read_excel(datafile)  # 使用pandas解析上传的文件，存储在dataframe
    # 数据处理
    dataframe = dataframe.dropna()  # 删除内容中的所有空值行，提高数据质量
    # 将内容，列名中的空格替换为下划线，使列名更规范
    dataframe.columns = [col.replace(&#x27; &#x27;, &#x27;_&#x27;) for col in dataframe.columns]

    # 创建sqlite数据库
    conn = sqlite3.connect(f&#x27;{dbname}.db&#x27;)
    # 将dataframe中的数据写入到数据库的mytable表中，若存在则替换，不写入索引列
    dataframe.to_sql(&#x27;mytable&#x27;, conn, if_exists=&#x27;replace&#x27;, index=False)
    cursor = conn.cursor()
    cursor.execute(&quot;select name from sqlite_master where type=&#x27;table&#x27; and name=&#x27;mytable&#x27;&quot;)
    tables = cursor.fetchall()
    if tables:
        gr.Info(&#x27;数据库创建成功&#x27;, duration=2)
    else:
        gr.Info(&#x27;数据库创建失败&#x27;, duration=2)

    conn.close()  # 释放资源

def user_gaokao(question, history: list):
    if len(history) == 0:
        prompt = &quot;&quot;&quot;
            你是一个高考志愿智能问答系统，对于用户提问的问题，你需要按照给出的【查询结果】对问题进行回答。
            你的回答需要按照以下两个步骤：
            1.分析用户问题和查询结果，判断是否有【查询结果】可以解答用户的问题，如果有则说明【查询结果】的名称，如果没有，则首先告知用户没有任何可参考的资料，需要主要答案的准确性。
            2.根据资料内容对问题进行解答，若用户希望根据高考分数得到志愿推荐，那么首先关注学校的投档分，从好的学校推荐，并结合【查询结果】中的学校信息来说明。
        &quot;&quot;&quot;
        history.append({&#x27;role&#x27;: &#x27;system&#x27;, &#x27;content&#x27;: prompt})
    history.append({&#x27;role&#x27;: &#x27;user&#x27;, &#x27;content&#x27;: question})
    return &quot;&quot;, history

def bot_gaokao(history: list, dbname):
    question = history[-1][&#x27;content&#x27;]  # 获取历史中最后一个用户问题
    robot = Robot_gaokao(dbname)  # 初始化类，创建对象，把数据库名传入
    result = robot.query_db(question)  # 调用Robot的query_db函数，返回查询数据库获得结果
    history[-1][&#x27;content&#x27;] += f&#x27;\n【查询结果】{result}&#x27;  # 将查询结果添加到最后一个用户问题的内容中
    client = ZhipuAI(api_key=&#x27;6214aaa7fcde44bdbeec762dbed4101c.6ktZ2m9834ZrlAW5&#x27;)
    resp = client.chat.completions.create(
        model=&#x27;glm-4&#x27;,
        messages=history,  # 将对话历史作为输入消息
        stream=True  # 设置为流式返回结果
    )
    history[-1][&#x27;content&#x27;] = history[-1][&#x27;content&#x27;].split(&#x27;【查询结果】&#x27;, 1)[0].strip()
    history.append({&#x27;role&#x27;: &#x27;assistant&#x27;, &#x27;content&#x27;: &#x27;&#x27;})  # 空助手回答记录
    for chunk in resp:
        history[-1][&#x27;content&#x27;] += chunk.choices[0].delta.content  # 逐步将流式返回回答内容添加到助手回答记录中
        yield history  # 实时更新聊天界面


with gr.Blocks() as demo:
    with gr.Row():
        interface_selector = gr.Radio(choices=[&quot;sqlite3&quot;, &quot;chroma&quot;], label=&quot;选择界面&quot;, value=&quot;chroma&quot;)
    with gr.Row():
        table_1 = gr.Markdown(&quot;# 高考政策通sqlite3&quot;)
    with gr.Row():
        with gr.Column():
            chatbot_gaokao = gr.Chatbot(label=&#x27;对话框&#x27;, type=&quot;messages&quot;, height=500, visible=True)
            question_gaokao = gr.Textbox(label=&#x27;请输入&#x27;)
            chear_btn_gaokao = gr.Button(&quot;clear&quot;)
        with gr.Column():
            t_1 = gr.Markdown(&quot;### 数据库构建&quot;)
            datafile_gaokao = gr.File(label=&#x27;上传文件&#x27;, type=&#x27;filepath&#x27;)
            build_btn_gaokao = gr.Button(&quot;开始构建&quot;)
            dbname_gaokao = gr.Textbox(label=&#x27;数据库名称&#x27;)
            tt_1 = gr.Markdown(&quot;### 数据库选择&quot;)
            dbchoose_gaokao = gr.Textbox(label=&#x27;数据库名称&#x27;)
            with gr.Row():
                dbdelete_btn_gaokao = gr.Button(&quot;删除&quot;)

    with gr.Row():
        table_2 = gr.Markdown(&quot;# 高考政策通chroma&quot;)
    with gr.Row():
        with gr.Column(scale=2):
            chatbot_001_beifen = gr.Chatbot(type=&quot;messages&quot;, label=&#x27;对话框&#x27;, height=500, visible=False)
            question_001_beifen = gr.Textbox(label=&#x27;请输入&#x27;)
            clear_btn_001_beifen = gr.Button(&quot;clear&quot;)
        with gr.Column(scale=1):
            t_2 = gr.Markdown(&quot;### 数据库构建&quot;)
            datafile_001_beifen = gr.File(type=&quot;filepath&quot;, label=&quot;上传文件&quot;)
            dbname_001_beifen = gr.Textbox(label=&#x27;数据库名称&#x27;)
            build_btn_001_beifen = gr.Button(&quot;开始构建&quot;)
            tt_2 = gr.Markdown(&quot;### 数据库选择&quot;)
            dbchoose_001_beifen = gr.Dropdown(choices=get_db_list(), label=&quot;数据库名称&quot;)
            dbrefresh_btn_001_beifen = gr.Button(&quot;刷新&quot;)



    chear_btn_gaokao.click(lambda: None, inputs=None, outputs=chatbot_gaokao, queue=False)
    question_gaokao.submit(user_gaokao, inputs=[question_gaokao, chatbot_gaokao], outputs=[question_gaokao, chatbot_gaokao]).then(bot_gaokao, [chatbot_gaokao, dbchoose_gaokao], chatbot_gaokao)
    build_btn_gaokao.click(build_db_gaokao, inputs=[datafile_gaokao, dbname_gaokao])

    clear_btn_001_beifen.click(lambda: None, None, chatbot_001_beifen, queue=False)
    question_001_beifen.submit(user, [question_001_beifen, chatbot_001_beifen], [question_001_beifen, chatbot_001_beifen], queue=False).then(bot, [chatbot_001_beifen, dbchoose_001_beifen], chatbot_001_beifen)
    build_btn_001_beifen.click(build_db, [datafile_001_beifen, dbname_001_beifen], [])
    dbrefresh_btn_001_beifen.click(refresh_db_choices, outputs=dbchoose_001_beifen)

    interface_selector.change(switch_interface, inputs=interface_selector, outputs=[chatbot_gaokao, chatbot_001_beifen])
    interface_selector.change(switch_interface, inputs=interface_selector, outputs=[question_gaokao, question_001_beifen])
    interface_selector.change(switch_interface, inputs=interface_selector, outputs=[chear_btn_gaokao, clear_btn_001_beifen])
    interface_selector.change(switch_interface, inputs=interface_selector, outputs=[t_1, t_2])
    interface_selector.change(switch_interface, inputs=interface_selector, outputs=[datafile_gaokao, datafile_001_beifen])
    interface_selector.change(switch_interface, inputs=interface_selector, outputs=[build_btn_gaokao, dbname_001_beifen])
    interface_selector.change(switch_interface, inputs=interface_selector, outputs=[tt_1, build_btn_001_beifen])
    interface_selector.change(switch_interface, inputs=interface_selector, outputs=[dbchoose_gaokao, tt_2])
    interface_selector.change(switch_interface, inputs=interface_selector, outputs=[dbdelete_btn_gaokao, dbchoose_001_beifen])
    interface_selector.change(switch_interface, inputs=interface_selector, outputs=[dbname_gaokao, dbrefresh_btn_001_beifen])
    interface_selector.change(switch_interface, inputs=interface_selector, outputs=[table_1, table_2])

demo.launch()
</code></pre></details></li></ul><ul id="1c2a551d-035a-800b-9b6b-c9069bb3f867" class="toggle"><li><details open=""><summary>文件</summary><figure id="1c2a551d-035a-8046-9570-f1ccdffd06e5"><div class="source"><a href="1_%E5%A4%A7%E5%AD%A6%E9%99%A2%E6%A0%A1%E5%9F%BA%E7%A1%80%E4%BF%A1%E6%81%AF.xlsx">attachment:3987becd-5bb1-4604-bb61-df77ec136c52:1_大学院校基础信息.xlsx</a></div></figure></details></li></ul><h2 id="1c2a551d-035a-80ed-8692-e56cfed61d66" class="">Day05  Neo4j</h2><ul id="1cea551d-035a-8083-9e43-f63bf7b61937" class="toggle"><li><details open=""><summary>项目压缩包小说助手</summary><figure id="1d1a551d-035a-8040-99d2-c5bad28cdf68"><div class="source"><a href="novel_parser.zip">novel_parser.zip</a></div></figure></details></li></ul><h2 id="1cea551d-035a-80a3-9099-f409ceca8849" class="">Day01  autogen+chainlit</h2><ul id="1cea551d-035a-8036-8594-c799fd082485" class="toggle"><li><details open=""><summary>text.py</summary><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="1cea551d-035a-8099-9485-cbe0bdfbe4d4" class="code"><code class="language-JavaScript">import chainlit as cl
from autogen_agentchat.agents import AssistantAgent
from autogen_agentchat.conditions import MaxMessageTermination, TextMentionTermination

from autogen_agentchat.teams import SelectorGroupChat
from autogen_ext.models.openai import OpenAIChatCompletionClient
import os

# 仨模型全是免费的，随便跑，可惜deepseek是蒸馏版本的，因为有深度思考的thinking所以没用上
model_data = {
    &quot;qianwen&quot;: {
        &quot;model&quot;: &quot;Qwen/Qwen2.5-7B-Instruct&quot;,
        &quot;base_url&quot;: &quot;https://api.siliconflow.cn/v1&quot;,
    },
    &quot;deepseek&quot;: {
        &quot;model&quot;: &quot;deepseek-ai/DeepSeek-R1-Distill-Qwen-7B&quot;,
        &quot;base_url&quot;: &quot;https://api.siliconflow.cn/v1&quot;,
    },
    &quot;zhipu&quot;: {
        &quot;model&quot;: &quot;THUDM/glm-4-9b-chat&quot;,
        &quot;base_url&quot;: &quot;https://api.siliconflow.cn/v1&quot;,
    }
}
api_key = os.getenv(&quot;GUIJI_API&quot;)

def bd_chat_client(tags: str):
    client = OpenAIChatCompletionClient(
        model=model_data[tags][&quot;model&quot;],
        base_url=model_data[tags][&quot;base_url&quot;],
        api_key=api_key,
        model_info={
            &quot;name&quot;: tags,
            &quot;vision&quot;: False,
            &quot;function_calling&quot;: True,
            &quot;json_output&quot;: True,
            &quot;family&quot;: &quot;unknown&quot;,
            &quot;structured_output&quot;: False
        }
    )
    return client

# 写进来一起创建了客户端，但是写完发现
# 客户端只是我要的一个过程半成品，想要的产品其实是他们代表的agent，先实现功能，以后再看怎么改符合设计模式
qianwen_client = bd_chat_client(&quot;qianwen&quot;)
deepseek_client = bd_chat_client(&quot;deepseek&quot;)
zhipu_client = bd_chat_client(&quot;zhipu&quot;)
#正好蒸馏的模型代表性低，就用deepseek来当裁判罢
caipan_agent = AssistantAgent(
    &quot;caipan&quot;,
    description = &quot;辩论的裁判，用于提示各agent的发言顺序，并且产出辩论终点标识 &#x27;END&#x27; &quot;,
    model_client = qianwen_client,
    system_message = &quot;&quot;&quot;
    # 角色
    你是一场辩论赛的裁判，要主持一场&quot;千问和质谱的大模型谁更好&quot;的辩论赛
    # 任务
    1. 根据当前对话历史分析前一个发言内容
    2. 轮流提示两位选手进行针对性回应
    3. 当检测到脏话或达到10轮时，立即输出&quot;END&quot;结束辩论
    # 规则
    - 首轮发言由用户指定
    - 后续发言需分析前一个选手的论点进行反驳
    - 每次只提示当前应该发言的选手
    - 在输出最后加上以下字符：&#x27;——裁判&#x27;
    &quot;&quot;&quot;
)
qianwen_agent = AssistantAgent(
    &quot;qianwen&quot;,
    description = &quot;主张千问大模型更好的选手&quot;,
    model_client = qianwen_client,
    system_message = &quot;&quot;&quot;
    # 角色
    你是一场辩论赛的选手，要参加一场&quot;千问和质谱的大模型谁更好用&quot;的辩论赛，你的目的是强调千问比质谱更好用
    # 任务
    以辩论赛的角度发言，根据对方的言论做针对性回答和反制
    # 限制
    -只输出辩论内容，不输出任何解释性语言
    -每次发言不能超过100个字
    -在输出最后加上以下字符：&#x27;——千问选手&#x27;
    &quot;&quot;&quot;
)
zhipu_agent = AssistantAgent(
    &quot;zhipu&quot;,
    description = &quot;主张质谱大模型更好的选手&quot;,
    model_client = zhipu_client,
    system_message = &quot;&quot;&quot;
    # 角色
    你是一场辩论赛的选手，要参加一场&quot;千问和质谱的大模型谁更好&quot;的辩论赛，你的目的是强调质谱比千问更好用
    # 任务
    以辩论赛的角度发言，根据对方的言论做针对性回答和反制
    # 限制
    -只输出辩论内容，不输出任何解释性语言
    -每次发言不能超过100个字
    -在输出最后加上以下字符：&#x27;——质谱选手&#x27;
    &quot;&quot;&quot;
)


@cl.on_chat_start
async def main():
    await cl.Message(content = &quot;你好，这边是千问，deepseek和质谱的一次辩（吵）论（架），请您指定谁先发言？&quot;).send()

text_mention_termination = TextMentionTermination(&quot;END&quot;)  # 修改终止词为&quot;END&quot;
messageTermination = MaxMessageTermination(max_messages=25)
team = SelectorGroupChat(
    [caipan_agent, qianwen_agent, zhipu_agent],
    model_client = deepseek_client,
    termination_condition = MaxMessageTermination(max_messages=10),
)


async def run_steam(query:str):
    
    response_stream = team.run_stream(task=query)
    
    async for msg in response_stream:
        if hasattr(msg, &quot;source&quot;) and msg.source != &quot;user&quot; and hasattr(msg, &quot;content&quot;):
            msg_cl = cl.Message(content=msg.content, author=msg.source)
            await msg_cl.send()

@cl.on_message
async def main(message: cl.Message):
    await run_steam(message.content)</code></pre></details></li></ul><ul id="1cea551d-035a-8005-bdc8-e9a1de86a974" class="toggle"><li><details open=""><summary>课堂演示代码</summary><ul id="1cea551d-035a-802c-9934-e19473064d58" class="toggle"><li><details open=""><summary>quickstart.py</summary><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="1cea551d-035a-802e-8035-dcd4e66b4708" class="code"><code class="language-JavaScript">import os
#这个类用户创建可进行对话的智能体
from autogen import ConversableAgent

# os.environ[&quot;DEEPSEEK_API_KEY&quot;]=&quot;sk-19a6e044d00c4962b62434ec87302a87&quot;
#
# os.environ[&quot;DEEPSEEK_API_BASE&quot;]=&quot;https://api.deepseek.com/v1&quot;

api_key = os.getenv(&#x27;GUIJI_API&#x27;)

#创建ConversableAgent对象实例-创建智能体
agent =ConversableAgent(
    &quot;chatbot&quot;,
    #设置大语言模型相关信息
    llm_config={
        &quot;config_list&quot;:[{
            &quot;model&quot;:&quot;THUDM/glm-4-9b-chat&quot;,
            &quot;api_key&quot;:api_key,
            &quot;base_url&quot;:&quot;https://api.siliconflow.cn/v1&quot;
        }]
    },
    #代码执行配置，关闭代码执行功能
    code_execution_config=False,
    #函数映射配置，设置为None，表示没有注册任何函数
    function_map=None,
    #人工输入模式,任何情况喜爱都不会请求人工输入
    human_input_mode=&quot;NEVER&quot;
)
#调用agent.generate_reply，生成回复
#messages 消息列表，每个消息是个字典
reply=agent.generate_reply(messages=[{&quot;content&quot;:&quot;给我讲一个笑话&quot;,&quot;role&quot;:&quot;user&quot;}])
print(reply)</code></pre></details></li></ul><ul id="1cea551d-035a-80c7-90b0-f59650d10e88" class="toggle"><li><details open=""><summary>demo1.py</summary><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="1cea551d-035a-80a9-b569-d1d33550416b" class="code"><code class="language-JavaScript">#简单对话系统，用户代理智能体可以向助手智能体发送问题，助手智能体借助deepseek 模型给出回答。
import os
#UserProxyAgent 用户模拟用户与其他智能体进行交互
#AssistantAgent 提供回答和协助的智能体
from autogen import UserProxyAgent,AssistantAgent

from autogen_ext.models.openai import OpenAIChatCompletionClient

#配置模型信息
model_info={
    &quot;name&quot;:&quot;deepseek-chat&quot;, #设置模型名称
    &quot;parameters&quot;:{
        &quot;max_tokens&quot;:2048, #每次输出最大的token数,deepseek官方数据: 1个英文字符约等于0.3个token，1个中文字符约等于0.6个token。
        &quot;temperature&quot;:0.4, #模型随机参数，数字越大，生成的结果随机性越大，一般为0.7。
                            # 如果想要ai提供更多的想法，调大数字
        &quot;top_p&quot;:0.9, #模型随机参数,接近1时：模型几乎考虑所有的词，只有概率极低的词才会排除，随机性也就越强.
                    # 接近0时：只有概率非常高的极少数词会被考虑，这会使模型的输出变得非常保守和确定
    },
    &quot;family&quot;:&quot;unknown&quot;, #模型类别
    &quot;functions&quot;:[], #如果模型支持函数调用，可以在这里定义函数信息
    &quot;vision&quot;:False, #必填字段,标识模型是否支持图像输入，False不支持
    &quot;json_output&quot;:True,#是否支持json格式输出，True表示支持
    &quot;function_calling&quot;:True, #必填字段，标识模型是否支持函数的调用，如果模型需要使用工具函数，该字段为True
    &quot;structured_out&quot;:False #结构化数据，设置为不输出
}

#创建模型客户端
model_client =OpenAIChatCompletionClient(
    model=&quot;deepseek-chat&quot;,
    base_url=&quot;https://api.deepseek.com&quot;,
    api_key=&quot;sk-19a6e044d00c4962b62434ec87302a87&quot;,
    model_info=model_info #传入之前配置的模型信息
)

#创建用户代理智能体，模拟用户与其他智能体进行交互
user_proxy=UserProxyAgent(
    name=&quot;user_proxy&quot;,
    human_input_mode=&quot;NEVER&quot;, #不需要人工输入
    code_execution_config={&quot;use_docker&quot;:False} # 设置为不使用Docker
)

#助手智能体用于回答和协助
assistant_agent=AssistantAgent(
    &quot;assistant&quot;,
    llm_config={
        &quot;config_list&quot;:[{
            &quot;model&quot;:&quot;deepseek-chat&quot;,
            &quot;api_key&quot;:&quot;sk-19a6e044d00c4962b62434ec87302a87&quot;,
            &quot;base_url&quot; : &quot;https://api.deepseek.com&quot;
        }]
    },
    system_message=&quot;你是一个有用的助手，能回答各种问题.&quot;
)

#用户代理发送的消息内容，用于启动与助手智能体的对话
message=&quot;请告诉我一些有趣的事实.&quot;
#用户代理智能体发送与助手智能体的聊天，发送消息
user_proxy.initiate_chat(
    assistant_agent,message=message
)






















</code></pre></details></li></ul><ul id="1cea551d-035a-8048-9ecc-e1cd7878d4df" class="toggle"><li><details open=""><summary>demo2.py</summary><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="1cea551d-035a-8080-a68d-edc0ccfb8db3" class="code"><code class="language-JavaScript">#简单对话系统，用户代理智能体可以向助手智能体发送问题，助手智能体借助deepseek 模型给出回答。
import os
#UserProxyAgent 用户模拟用户与其他智能体进行交互
#AssistantAgent 提供回答和协助的智能体
from autogen import UserProxyAgent,AssistantAgent

from autogen_ext.models.openai import OpenAIChatCompletionClient

#配置模型信息
model_info={
    &quot;name&quot;:&quot;deepseek-chat&quot;, #设置模型名称
    &quot;parameters&quot;:{
        &quot;max_tokens&quot;:2048, #每次输出最大的token数,deepseek官方数据: 1个英文字符约等于0.3个token，1个中文字符约等于0.6个token。
        &quot;temperature&quot;:0.4, #模型随机参数，数字越大，生成的结果随机性越大，一般为0.7。
                            # 如果想要ai提供更多的想法，调大数字
        &quot;top_p&quot;:0.9, #模型随机参数,接近1时：模型几乎考虑所有的词，只有概率极低的词才会排除，随机性也就越强.
                    # 接近0时：只有概率非常高的极少数词会被考虑，这会使模型的输出变得非常保守和确定
    },
    &quot;family&quot;:&quot;unknown&quot;, #模型类别
    &quot;functions&quot;:[], #如果模型支持函数调用，可以在这里定义函数信息
    &quot;vision&quot;:False, #必填字段,标识模型是否支持图像输入，False不支持
    &quot;json_output&quot;:True,#是否支持json格式输出，True表示支持
    &quot;function_calling&quot;:True, #必填字段，标识模型是否支持函数的调用，如果模型需要使用工具函数，该字段为True
    &quot;structured_out&quot;:False #结构化数据，设置为不输出
}

#创建模型客户端
model_client =OpenAIChatCompletionClient(
    model=&quot;deepseek-chat&quot;,
    base_url=&quot;https://api.deepseek.com&quot;,
    api_key=&quot;sk-19a6e044d00c4962b62434ec87302a87&quot;,
    model_info=model_info #传入之前配置的模型信息
)

#创建用户代理智能体，模拟用户与其他智能体进行交互
user_proxy=UserProxyAgent(
    name=&quot;user_proxy&quot;,
    human_input_mode=&quot;NEVER&quot;, #不需要人工输入
    code_execution_config={&quot;use_docker&quot;:False} # 设置为不使用Docker
)

#助手智能体用于回答和协助
assistant_agent=AssistantAgent(
    &quot;assistant&quot;,
    llm_config={
        &quot;config_list&quot;:[{
            &quot;model&quot;:&quot;deepseek-chat&quot;,
            &quot;api_key&quot;:&quot;sk-19a6e044d00c4962b62434ec87302a87&quot;,
            &quot;base_url&quot; : &quot;https://api.deepseek.com&quot;
        }]
    },
    system_message=&quot;你是一个有用的助手，能回答各种问题.&quot;
)

#用户代理发送的消息内容，用于启动与助手智能体的对话
message=&quot;请告诉我一些有趣的事实.&quot;
#用户代理智能体发送与助手智能体的聊天，发送消息
user_proxy.initiate_chat(
    assistant_agent,message=message
)</code></pre></details></li></ul><ul id="1cea551d-035a-8075-b8cd-e42972621291" class="toggle"><li><details open=""><summary>demo3.py</summary><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="1cea551d-035a-8076-999f-f18003e3b45d" class="code"><code class="language-JavaScript">#构建交互式俩天应用
import chainlit as cl

from autogen_agentchat.agents import AssistantAgent

#MaxMessageTermination 设置消息对话上限，达到上限时终止对话
#TextMentionTermination 包含特定文本时终止对话
from autogen_agentchat.conditions import MaxMessageTermination,TextMentionTermination
#创建一个智能体团队，实现团队内智能体之间的协作对话
from autogen_agentchat.teams import SelectorGroupChat
from autogen_ext.models.openai import OpenAIChatCompletionClient
model_client =OpenAIChatCompletionClient(
    model=&quot;deepseek-chat&quot;,
    base_url=&quot;https://api.deepseek.com&quot;,
    api_key=&quot;sk-19a6e044d00c4962b62434ec87302a87&quot;,
    model_info={
        &quot;vision&quot;:False,
        &quot;function_calling&quot;:True,
        &quot;json_output&quot;:True,
        &quot;family&quot;:&quot;unknown&quot;
    } #传入之前配置的模型信息
)

#创建一个规划团队智能体实例
planning_agent=AssistantAgent(
    &quot;PlanningAgent&quot;,
    #说明该智能体用途
    description=&quot;用于规划的Agent,当一个任务到达时此Agent是第一个参与者&quot;,
    #指定使用的模型客户端
    model_client=model_client,
    system_message=&quot;&quot;&quot;
     你是一个任务规划智能体。
    你的工作是将复杂的任务分解为更小的、可管理的子任务。
    你的团队成员有3个，分别是：
        DentalPulpAgent: 牙体牙髓科智能体
        RestorativeAgent: 牙齿修复科智能体
        DentalImplantAgent: 牙齿种植科智能体

    你只计划和委派任务，而不自己执行它们

    分配任务时，请使用此格式:
    1. &lt;agent&gt; : &lt;task&gt;

    当所有智能体把任务完成后，再总结结果以&quot;TERMINATE&quot;结束。 
    &quot;&quot;&quot;
)

#创建一个牙体牙髓科智能体
dental_pulp_agent =AssistantAgent(
    &quot;DentalPulpAgent&quot;,
    description=&quot;牙体牙髓科智能体&quot;,
    model_client=model_client,
    system_message=&quot;&quot;&quot;
    你是一个口腔医院的牙体牙髓科智能体。
    你可以解答关于牙体牙髓科中患者提出的问题，你的解答非常专业，且可靠.
    &quot;&quot;&quot;
)
#创建一个牙齿修复科智能体
restorative_agent =AssistantAgent(
    &quot;RestorativeAgent&quot;,
    description=&quot;牙齿修复科智能体&quot;,
    model_client=model_client,
    system_message=&quot;&quot;&quot;
    你是一个口腔医院的牙齿修复科智能体。
    你可以解答关于牙齿修复科中患者提出的问题，比如牙冠、烤瓷牙修复等。你的解答非常专业，且可靠.
    &quot;&quot;&quot;
)
#创建一个牙体牙髓科智能体
dental_implant_agent =AssistantAgent(
    &quot;DentalImplantAgent&quot;,
    description=&quot;牙齿种植科智能体&quot;,
    model_client=model_client,
    system_message=&quot;&quot;&quot;
    你是一个口腔医院的牙齿种植科智能体。
    你可以解答关于牙齿种植科中患者提出的问题，你的解答非常专业，且可靠.
    &quot;&quot;&quot;
)

#当聊天开始时，执行下面异步函数
@cl.on_chat_start
async def main():
    await cl.Message(content=&quot;你好，这里是口腔医院专家团队，有什么可以帮你？&quot;).send()

async def run_steam(query:str):
    #终止词3
    text_mention_termination=TextMentionTermination(&quot;TERMINATE&quot;)
    #设置对话上限
    messageTermination=MaxMessageTermination(max_messages=25)

    team =SelectorGroupChat(
        #团队成员列表
        [planning_agent,dental_pulp_agent,restorative_agent,dental_implant_agent],
        model_client=model_client,
        #设置对话终止条件
        termination_condition=messageTermination,
    )

    #调用团队的run_stream函数，以流式方式运行，处理用户的查询
    #返回异步生成器对象
    response_stream=team.run_stream(task=query)

    #使用异步for玄幻遍历流式响应
    async for msg in response_stream:
        #检查消息对象有没有source属性，且消息来源不时用户
        #检查消息对象是否有content属性
        if hasattr(msg,&quot;source&quot;) and msg.source !=&quot;user&quot; and hasattr(msg,&quot;content&quot;):
            #创建一个chainlit的消息实例，内容为消息对象的内容，作者为消息来源
            msg =cl.Message(content=msg.content,author=msg.source)
            #异步发送该消息给用户
            await msg.send()

#提高效率与响应性，与chainlit框架兼容
@cl.on_message
async def main(message:cl.Message):
    #调用run_steam函数，将用户消息的内容作为查询传递给智能体团队，进行处理
    await run_steam(message.content)

</code></pre></details></li></ul></details></li></ul><h2 id="1cfa551d-035a-80f4-b158-fdaf70e4ed1f" class="">Day02  langchain+openai镜像</h2><ul id="1cfa551d-035a-800f-bfe3-f79258f12353" class="toggle"><li><details open=""><summary>langchain+openai镜像</summary><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="1cfa551d-035a-8039-8c25-df843aa20df3" class="code"><code class="language-Python">#添加历史聊天记录

#存储和管理聊天 消息历史记录
from langchain_community.chat_message_histories import ChatMessageHistory
#它是聊天消息历史记录类的基类
from langchain_core.chat_history import BaseChatMessageHistory
#让模型在运行时结合聊天消息历史记录
from langchain_core.runnables.history import RunnableWithMessageHistory
from langchain_openai import ChatOpenAI
from langchain_core.messages import HumanMessage,SystemMessage
import os

# 一个免费api列表，按顺序轮流检查，使用第一个可用的api;
# 后面发现免费的openai镜像再在后面追加
# 第一个注释掉的是测试无效跳过功能用的，注释保留了
llm_api = {
    # &quot;bug_text&quot;: {
    #     &quot;api_key&quot;: &quot;hk-e&quot;,
    #     &quot;base&quot;: &quot;https://api.com&quot;
    # },
    # 这个GPT-API-free是从github上面发现的项目，也是用的openai协议，按每天的免费次数算的
    &quot;GPT-API-free&quot;: {
        &quot;api_key&quot;: &quot;sk-WsX5IosThRA057bUxujAHKSH9YMRtG0Rgq7IRBsYKMfCECFP&quot;,
        &quot;base&quot;: &quot;https://api.chatanywhere.tech&quot;
    },
    # 注释一下备忘：这个是按账号总余额算的，赠费一美元，用一点少一点不再增加
    &quot;OpenAI-HK&quot;: {
        &quot;api_key&quot;: &quot;hk-ev8m741000053754530c9d59b22b9986f2f3726d962d6926&quot;,
        &quot;base&quot;: &quot;https://api.openai-hk.com&quot;
    },
}

#此函数使用了课堂代码test01.py的基础用法，invoke快速使用:
def try_api_getmodel():
    for _, value in llm_api.items():
        try:
            os.environ[&quot;OPENAI_API_KEY&quot;] = value[&quot;api_key&quot;]
            os.environ[&quot;OPENAI_API_BASE&quot;] = value[&quot;base&quot;]

            model_try = ChatOpenAI(model = &quot;gpt-3.5-turbo&quot;)
            # 节省token，只返回一个数字做连通性检查
            print(model_try.invoke([SystemMessage(content=&quot;只能输出数字字符，不要输出任何其他字符和解释性内容&quot;),
                                HumanMessage(content = &quot;你好，请输出阿拉伯数字1&quot;)]).content)
            return model_try
        except:
            continue
model = try_api_getmodel()

#给总记忆字典改了个名
memory_all = {}
def get_session_history(session_id: str) -&gt; BaseChatMessageHistory:
    if session_id not in memory_all:
        memory_all[session_id] = ChatMessageHistory()
    return memory_all[session_id]

with_message_history = RunnableWithMessageHistory(model, get_session_history)
config_person_1 = {&quot;configurable&quot;: {&quot;session_id&quot;: &quot;person_1&quot;}}
config_person_2 = {&quot;configurable&quot;: {&quot;session_id&quot;: &quot;person_2&quot;}}
response = with_message_history.invoke(
    [
        SystemMessage(content = &quot;请你重复输出用户输入给你的话，并在最后加上以下字符&#x27;——大模型回答&#x27;&quot;),
        HumanMessage(content = &quot;person1的第 一 次输入&quot;)
    ],
    config = config_person_1,
)
with_message_history.invoke(
    [
        SystemMessage(content = &quot;请你重复输出用户输入给你的话，并在最后加上以下字符&#x27;——大模型回答&#x27;&quot;),
        HumanMessage(content = &quot;person1的第 二 次输入&quot;)
    ],
    config = config_person_1
)
with_message_history.invoke(
    [
        SystemMessage(content = &quot;请你重复输出用户输入给你的话，并在最后加上以下字符&#x27;——大模型回答&#x27;&quot;),
        HumanMessage(content = &quot;person2的第 一 次输入&quot;)
    ],
    config = config_person_2,
)
# 我想看看记忆字典的结构，怎么存的
for key, value in memory_all.items():
    print(key)
    print(value)
    print(&quot;=&quot;*50+&quot;person记忆隔离线&quot;+&quot;=&quot;*50)# 按person的分割线
#
# os.environ[&quot;OPENAI_API_KEY&quot;]=&#x27;你的api_key&#x27;
# os.environ[&quot;OPENAI_API_BASE&quot;]=&#x27;https://api.openai-hk.com/v1&#x27;
# #创建openai的实例
# model=ChatOpenAI(model=&quot;gpt-3.5-turbo&quot;)
#
# #顶一个字典store，用于存储不同会话的聊天历史记录
# store={}
#
# #用于根据会话id获取对应的聊天消息历史记录
# #输入参数session_id 是一个字符串，表示会话的唯一标识
# def get_session_history(session_id:str) -&gt;BaseChatMessageHistory:
#     #检查会话id是否不在store字典中
#     if session_id not in store:
#         #如果不存在，为该会话id创建一个新的ChatMessageHistory实例并存储到store中
#         store[session_id] =ChatMessageHistory()
#     return  store[session_id]
#
# #创建RunnableWithMessageHistory类的实例
# #将之前创建的model和get_session_history函数传入，在运行模型时结合聊天消息历史记录
# with_message_history=RunnableWithMessageHistory(model,get_session_history)
#
# #配置字典config，用于指定会话id， 会话id：第一个对话
# config={&quot;configurable&quot;:{&quot;session_id&quot;:&quot;第一个对话&quot;}}
#
# #调用with_message_history的invoke方法，向模型发送消息
# #传入一个包含HumanMessage实体列表,代表用户的消息内容
# #同时传入配置字典config
# response=with_message_history.invoke(
#     [HumanMessage(content=&quot;你好，我叫tom&quot;)],
#     config=config,
# )
#
# #打印模型回复内容
# print(response.content)
#
# #同一个会话“第一个对话”，提问，模型根据历史记录回答我叫什么
# response=with_message_history.invoke(
#     [HumanMessage(content=&quot;我叫什么名字?&quot;)],
#     config=config,
# )
#
# print(response.content)
#
# #配置字典config，将回话id改为：第二个对话
# config={&quot;configurable&quot;:{&quot;session_id&quot;:&quot;第二个对话&quot;}}
#
# #第二个对话提问
# response=with_message_history.invoke(
#     [HumanMessage(content=&quot;我是谁？&quot;)],
#     config=config,
# )
#
# print(response.content)</code></pre></details></li></ul><ul id="1cfa551d-035a-80f1-992b-f3414c683ae3" class="toggle"><li><details open=""><summary>langchain链的用法和流式输出</summary><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="1cfa551d-035a-80b2-97dd-da29a44aa8e2" class="code"><code class="language-Python">from langchain_community.chat_message_histories import ChatMessageHistory
from langchain_core.chat_history import BaseChatMessageHistory
from langchain_core.runnables.history import RunnableWithMessageHistory
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_openai import ChatOpenAI
from langchain_core.messages import HumanMessage, SystemMessage
import os

llm_data = {
    # 这个GPT-API-free是从github上面发现的项目，也是用的openai协议，按每天的免费次数算的
    &quot;GPT-API-free&quot;: {
        &quot;api_key&quot;: &quot;sk-WsX5IosThRA057bUxujAHKSH9YMRtG0Rgq7IRBsYKMfCECFP&quot;,
        &quot;base&quot;: &quot;https://api.chatanywhere.tech&quot;
    },
    # 注释一下备忘：这个是按账号总余额算的，赠费一美元，用一点少一点不再增加
    &quot;OpenAI-HK&quot;: {
        &quot;api_key&quot;: &quot;hk-ev8m741000053754530c9d59b22b9986f2f3726d962d6926&quot;,
        &quot;base&quot;: &quot;https://api.openai-hk.com&quot;
    },
}

# 这回没把模型写死,把前面的也改了
def try_get_model(model_name: str):
    for _, value in llm_data.items():
        try:
            os.environ[&quot;OPENAI_API_KEY&quot;] = value[&quot;api_key&quot;]
            os.environ[&quot;OPENAI_API_BASE&quot;] = value[&quot;base&quot;]
            model_try = ChatOpenAI(model = model_name)
            # 一样，先用response来检查联通性，毕竟这样是多api通用的
            model_try.invoke([
                SystemMessage(content = &quot;只允许按要求输出阿拉伯数字，不允许输出任何其他字符&quot;),
                HumanMessage(content = &quot;请输出数字1&quot;)])
            return model_try
        except:
            continue

model = try_get_model(&quot;gpt-3.5-turbo&quot;)

prompt = ChatPromptTemplate.from_messages(
    [
        (
            &quot;system&quot;,
            &quot;语言请按{language}回答&quot;
        ),
        MessagesPlaceholder(variable_name = &quot;messages&quot;),
    ],
)

# 链的用法， | 是管道操作符，负责连接
chain = prompt | model
for response in chain.stream({&quot;messages&quot;: [HumanMessage(content = &quot;你好,给我写首歌词&quot;)], &quot;language&quot;: &quot;English&quot;}):
    print(response.content, end = &quot;&quot;)

memory = {}

def get_session_history(session_id: str) -&gt; BaseChatMessageHistory:
    if session_id not in memory:
        memory[session_id] = ChatMessageHistory()
    return memory[session_id]

with_message_history = RunnableWithMessageHistory(
    chain,
    get_session_history,
    input_messages_key = &quot;messages&quot;,
)

messages = {
    &quot;messages&quot;: [HumanMessage(content = &quot;你好，写首诗给我&quot;)],
    &quot;language&quot;: &quot;火星文&quot;
}
for response in chain.stream(messages):
    print(response.content, end = &quot;&quot;)</code></pre></details></li></ul><ul id="1d0a551d-035a-8038-8ae1-c402874f87a3" class="toggle"><li><details open=""><summary>链的进阶用法</summary><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="1d0a551d-035a-80ff-8869-f65d6e6699de" class="code"><code class="language-JavaScript">#RAG实现
from langchain_chroma import Chroma
from langchain_openai import ChatOpenAI
from langchain_openai import OpenAIEmbeddings
from langchain_core.documents import Document
import os

os.environ[&#x27;OPENAI_API_KEY&#x27;] = &#x27;hk-tmixsf100005368923ebf9dce1f21fc86aea24943d6cedbf&#x27;
os.environ[&quot;OPENAI_API_BASE&quot;] = &quot;https://api.openai-hk.com/v1&quot;
model =ChatOpenAI(model=&#x27;gpt-3.5-turbo&#x27;)

#================================
#1.数据准备阶段
#代码厉创建了文档列表documents，这些文档可被看做是知识源
#运用Chroma 向量数据库，借助OpenAIEmbeddings把文档转化为向量形式并存储，这样就可以进行高效的相似性检索
#================================
documents=[
    Document(page_content=&quot;Molly是一个人工智能教育机器人。&quot;,metadata={&quot;source&quot;:&quot;ai-doc&quot;}),
    Document(page_content=&quot;《快乐星球》是一部早期的电视剧.&quot;,metadata={&quot;source&quot;:&quot;ai-doc&quot;}),
    Document(page_content=&quot;NBA比赛实况&quot;,metadata={&quot;source&quot;:&quot;nba-doc&quot;})
    #可以添加更多的其他文档
]
#Chroma两种工作模式: 客户端-服务器模式，本地嵌入式模式(不需要开启服务,在python进程中直接运行)
#使用Chroma类的from_documents函数创建一个向量存储对象
#将文档列表和OpenAIEmbeddings 实例 传入，将文档转换为向量并存储在Chroma数据库中
vectorstore=Chroma.from_documents(
    documents,
    embedding=OpenAIEmbeddings(),
)

#================================
#2.检索阶段
#构建1个检索器retriever,对向量数据库vectorstore的similarity_search方法的封装，每次检索返回最相似的一个结果（k=1）
#================================
from langchain_core.runnables import RunnableLambda
#创建检索器
retriever =RunnableLambda(vectorstore.similarity_search).bind(k=1)

#使用检索器的batch，批量处理多个查询，返回与每个查询最相似的文档
response=retriever.batch([&quot;Molly是什么？&quot;,&quot;什么是快乐星球&quot;])
print(response)

#================================
#3. 生成阶段
#定义一个提示词模版prompt,作用是把问题和检索到的上下文组合成一个完整的提示信息
#================================
#创建聊天提示模版
from langchain_core.prompts import ChatPromptTemplate
#直接传递输入
from langchain_core.runnables import RunnablePassthrough
model =ChatOpenAI(model=&#x27;gpt-3.5-turbo&#x27;)

#定义一个消息模版，构建提示消息
#要求回答问题时仅使用提供的上下文，包含问题和上下文两个变量
message=&quot;&quot;&quot;
回答这个问题，只使用提供的上下文

{question}

上下文:
{context}
&quot;&quot;&quot;
#ChatPromptTemplate的from_messages函数创建一个提示模板对象
#将消息包装成一个人类消息
prompt =ChatPromptTemplate.from_messages([(&quot;human&quot;,message)])

#================================
#4.RAG链的构建与执行
#创建rag_chain对象，它会把输入的问题传递给检索器获取上下文，同时将问题本身也传递下去。
#接着把上下文和问题组合成提示信息，最后将消息传递给ChatOpenAI模型进行推理
#================================
rag_chain={&quot;context&quot;:retriever,&quot;question&quot;:RunnablePassthrough()} | prompt | model
#链对象处理一个问题，获取模型的回复
response=rag_chain.invoke(&quot;告诉我关于Molly的信息&quot;)
#打印回复内容
print(response)












</code></pre></details></li></ul><h2 id="1d0a551d-035a-80a4-a6ec-dbf9c832890b" class="">Day03  langchain工具调用（自定义+api）</h2><ul id="1d0a551d-035a-806d-85e5-cc7fa0da907d" class="toggle"><li><details open=""><summary>test01.py</summary><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="1d0a551d-035a-800b-a10c-efe07089238b" class="code"><code class="language-Python">#单次工具调用
#工具类，用于执行Tavily搜索
from langchain_community.tools.tavily_search import TavilySearchResults
import os

#设置TAVILY API秘钥，调用搜索服务所需的凭证
# 是专门做LLM的搜索引擎工具的平台
os.environ[&quot;TAVILY_API_KEY&quot;] = os.getenv(&quot;TAVILY_API&quot;)

#定义函数，用于获取搜索结果
#query参数，代表要搜索的查询内容
def get_search_results(query:str):
    #创建一个实例，设置最大的返回数结果为2
    search =TavilySearchResults(max_resules=2)
    #调用实例的invoke方法，传入查询内容，获取搜索结果
    result=search.invoke(query)
    #返回结果
    return result

#调用get_search_results函数，传入搜索内容，打印返回结果
print(get_search_results(&quot;NBA比赛结果&quot;))

#调用大模型进行回答
from langchain_openai import ChatOpenAI
from langchain_core.messages import HumanMessage,SystemMessage
import  os
os.environ[&quot;OPENAI_API_KEY&quot;]=&quot;sk-WsX5IosThRA057bUxujAHKSH9YMRtG0Rgq7IRBsYKMfCECFP&quot;
os.environ[&quot;OPENAI_API_BASE&quot;]=&quot;https://api.chatanywhere.tech&quot;
model=ChatOpenAI(model=&quot;gpt-3.5-turbo&quot;)

input_message=input(&quot;请输入你要查询的问题:&quot;)
#调用函数，传入从键盘输入的问题,返回结果
result =get_search_results(input_message)
#构建一个消息列表，用于与大模型进行交互
messages=[
    #提示大模型询问的是体育赛事结果
    SystemMessage(content=&quot;询问体育赛事结果&quot;),
    #用户消息，包含互联网搜索结果和用户的原始问题，让大模型根据搜索结果回答问题
    HumanMessage(content=f&quot;互联网查询结果:{result}\n\n&quot;f&quot;根据以上结果回答问题:{input_message}&quot;)
]

# 我换成流式输出了，避免等待时间过长影响排查问题
for res in model.stream(messages):
    print(res.content, end = &quot;&quot;)</code></pre></details></li></ul><ul id="1d0a551d-035a-80e9-9ab9-d98b6cee58e6" class="toggle"><li><details open=""><summary>test02.py</summary><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="1d0a551d-035a-8078-86c7-e7dfef661bac" class="code"><code class="language-Python">#自定义工具
import os
from langchain_core.tools import tool
from langchain_openai import ChatOpenAI
#使用数学相关的函数
import  math

#@tool 装饰器 将下面的函数转换为langchain工具
@tool
def caluculate_sphere_volume(radius):
    &quot;&quot;&quot;
    此函数用于计算球体的体积。

    参数:
    radius (float): 球体的半径。

    返回:
    float: 球体的体积。
    &quot;&quot;&quot;
    #球体体积  radius 球体半径, 返回 float 的球体体积
    return (4/3) * math.pi * radius **3

#绑定工具列表，通常会使用多个工具，当前只有一个
tools=[caluculate_sphere_volume]

#初始化工具输出结果
tool_output=0.0
#从键盘接收一个问题
input_message =input(&quot;请输入一个问题:&quot;)

os.environ[&quot;OPENAI_API_KEY&quot;]=&#x27;你的api&#x27;
os.environ[&quot;OPENAI_API_BASE&quot;]=&#x27;https://api.openai-hk.com/v1&#x27;
model=ChatOpenAI(model=&quot;gpt-3.5-turbo&quot;)
#将模型与工具列表绑定了，模型可以调用列表中的工具
model_with_tools=model.bind_tools(tools)
#调用绑定工具的模型，传入用户输入的问题，让模型根据问题决定是否调用工具并生成响应
ai_msg=model_with_tools.invoke(input_message)
#查看响应结果
#print(ai_msg)

#解析模型响应中调用工具的相关信息
for tool_call in ai_msg.tool_calls:
    #字典中的键值对获取
    selected_tool={&quot;caluculate_sphere_volume&quot;:caluculate_sphere_volume}[tool_call[&#x27;name&#x27;].lower()]
    #调用选中的工具函数，传入工具调用时的参数，将工具计算的结果赋值给tool_output变量
    tool_output=selected_tool.invoke(tool_call[&#x27;args&#x27;])
    #打印，查看工具计算结果
    print(tool_output)

#不使用工具，直接调用模型对用户输入的问题进行响应
result_without_tools=model.invoke(input_message)
print(&quot;不使用工具：&quot;,result_without_tools.content)

#使用工具计算得到结果，将用户的问题和工具计算结果组合成新的输入，再次调用模型,要求模型直接回答结果，不加入推理
result_with_tools=model.invoke(f&quot;{input_message},计算结果是:{tool_output},请直接回答这个结果，不加入自己的推理步骤&quot;)
print(&quot;使用工具：&quot;,result_with_tools.content)
</code></pre></details></li></ul><ul id="1d0a551d-035a-80a6-a202-dfe4dbf8fcd9" class="toggle"><li><details open=""><summary>zuoye.py</summary><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="1d0a551d-035a-80cb-acb9-e5cbde756f3e" class="code"><code class="language-Python">from langchain_community.tools.tavily_search import TavilySearchResults
import os
from langchain_openai import ChatOpenAI
from langchain_core.messages import HumanMessage, SystemMessage
from langchain_core.tools import tool
import math

os.environ[&quot;TAVILY_API_KEY&quot;] = os.getenv(&quot;TAVILY_API_KEY&quot;)
all_openai_api_data = {
    &quot;bug_test&quot;: {
        &quot;api_key&quot;: &quot;balbalbal&quot;,
        &quot;base_url&quot;: &quot;https......&quot;
    },
    &quot;GPT-API-free&quot;: {
        &quot;api_key&quot;: os.getenv(&quot;GPT-API-free_API_KEY&quot;),
        &quot;base_url&quot;: os.getenv(&quot;GPT-API-free_BASE_URL&quot;)
    },
    &quot;OpenAI-HK&quot;:{
        &quot;api_key&quot;: os.getenv(&quot;OpenAI-HK_API_KEY&quot;),
        &quot;base_url&quot;: os.getenv(&quot;OpenAI-HK_BASE_URL&quot;)
    },
}

def try_api_bd_model(model_name: str) -&gt; ChatOpenAI | None:
    for key, value in all_openai_api_data.items():
        try:
            os.environ[&quot;OPENAI_API_KEY&quot;] = value[&quot;api_key&quot;]
            os.environ[&quot;OPENAI_API_BASE&quot;] = value[&quot;base_url&quot;]
            model_try = ChatOpenAI(model = model_name)
            model_try.invoke(&quot;请输出数字字符：&#x27;1&#x27;，不要输出任何其他字符，也不要输出任何其他内容以及解释性语言&quot;)
            print(f&quot;目前使用的是【{key}】平台的api&quot;)
            return model_try
        except:
            print(f&quot;{key}平台的api已经失效，建议替换，正在测试检查下一组api的连通性&quot;)
            continue
    print(&quot;已经没有可用的api了...建议检查网络&quot;)

model = try_api_bd_model(&quot;gpt-3.5-turbo&quot;)
# 这个实例放进函数里的话每次调用都新创建一个实例...拿出来之后发现函数只剩一行了...
# 于是直接用原本的search功能了
# 于是函数部分我打算用在langchain的tool工具里，把两个范例写到一起吧,用api的这个也写进自定义函数里
# 第一个范例的query就先写死了
search = TavilySearchResults(max_results = 2)

# query = input(&quot;请输入你要查询的问题：&quot;)
query = &quot;今年气候对比往年怎么样？&quot;

print(&quot;正在使用联网查询气候问题，请等待..&quot;)
result = search.invoke(query)
print(&quot;查找气候问题完毕，正在给llm总结..&quot;)
messages = [
    SystemMessage(content = &quot;根据联网的查询结果回答用户的问题&quot;),
    HumanMessage(content = f&quot;联网查询结果:[{result}],用户问题：[{query}]&quot;)
]

print(model.invoke(messages).content)
# for res in model.stream(messages):
#     print(res.content, end = &quot;&quot;)


# 自定义工具就写成这样了,类型也改了一下
@tool
def search_net(query: str) -&gt; str:
    &quot;&quot;&quot;
    执行网络查询并返回结果摘要。

    参数:
    - query (str): 需要搜索的查询字符串

    返回:
    - str: 搜索结果的文本摘要（包含标题、URL 和内容）
    &quot;&quot;&quot;
    print(&quot;正在联网查询您的问题，请等待..\n&quot;)
    res = search.invoke(query)
    print(&quot;查找问题完毕，正在给llm总结..\n&quot;)
    return res

tools = [search_net]

tool_output = &quot;&quot;

model_with_tools = model.bind_tools(tools)
print()
print(&quot;=&quot;*200)
print()
print()
query = input(&quot;请输入一个问题(记得提示模型要联网查询，不然模型判断可能不会选择调用联网工具):&quot;)

response = model_with_tools.invoke(query)

# # 流式解析content信息，但是用绑定过工具的model流式输出就报错，然后想起来还要拼接...
# print(&quot;解析content信息（流式）:&quot;)
# for res in model_with_tools.stream(query):
#     print(res.content, end = &quot;&quot;)
# print(&quot;=&quot;*200)

# 流式解析工具调用信息
print(&quot;查看工具调用信息（列表里只有字典元素,...假装这样写stream就是流式）:&quot;)
for tool_call in response.tool_calls:
    selected_tool = {&quot;search_net&quot;: search_net}[tool_call[&quot;name&quot;].lower()]
    for tool_output in selected_tool.stream(tool_call[&quot;args&quot;]):
        print(tool_output, end = &quot;&quot;)
print()
print(&quot;=&quot;*200)
print()

print(&quot;查看工具查到的网页的content:&quot;)
for tool_call in response.tool_calls:
    selected_tool = {&quot;search_net&quot;: search_net}[tool_call[&quot;name&quot;].lower()]
    tool_output = selected_tool.invoke(tool_call[&quot;args&quot;])
    # 这里把每个网页的字典都遍历一遍，查看其中的content，其实只有两个
    for tool_output_page in tool_output:
        print(f&quot;网页{tool_output_page[&#x27;url&#x27;]}的content为：\n{tool_output_page[&#x27;content&#x27;]}&quot;, end = &quot;\n\n&quot;)
print()
print(&quot;=&quot;*200)
print()
# 这里再做流式输出吧
print(&quot;LLM总结网页结果回答为：&quot;)
for res in model.stream(f&quot;用户问题为[{query}],参考资料为：[{tool_output}]，请参照参考资料回答用户问题&quot;):
    print(res.content, end = &quot;&quot;)</code></pre></details></li></ul><h2 id="1d1a551d-035a-80e1-a52c-ec55c72ea58c" class="">Day04  langchain Agent+function_call_tool</h2><ul id="1d1a551d-035a-80f1-8bb1-d9fb6c649b3c" class="toggle"><li><details open=""><summary>langchain中的Agent解释</summary><p id="1d1a551d-035a-806f-9775-cfbea9043dba" class=""><del><mark class="highlight-red"><a href="https://zhuanlan.zhihu.com/p/661244337">结论：AgentExecutor本身不直接使用大模型，模型仅在Agent环节参与</a></mark></del></p><figure id="1d1a551d-035a-808f-8503-d5cdd09a718f" class="image"><a href="image%2011.png"><img style="width:681.9791870117188px" src="image%2011.png"/></a></figure><p id="1d1a551d-035a-80fb-a8d2-fd36be7ed79b" class="">在LangChain的代理中，有这样几个关键组件。</p><ol type="1" id="1d1a551d-035a-80af-bfc9-e79edbea8210" class="numbered-list" start="1"><li><strong>代理</strong>（Agent）：这个类决定下一步执行什么操作。它由一个语言模型和一个提示（prompt）驱动。提示可能包含代理的性格（也就是给它分配角色，让它以特定方式进行响应）、任务的背景（用于给它提供更多任务类型的上下文）以及用于激发更好推理能力的提示策略（例如<a href="https://zhida.zhihu.com/search?content_id=235071722&amp;content_type=Article&amp;match_order=1&amp;q=ReAct&amp;zd_token=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJ6aGlkYV9zZXJ2ZXIiLCJleHAiOjE3NDQ0MzQ4MzgsInEiOiJSZUFjdCIsInpoaWRhX3NvdXJjZSI6ImVudGl0eSIsImNvbnRlbnRfaWQiOjIzNTA3MTcyMiwiY29udGVudF90eXBlIjoiQXJ0aWNsZSIsIm1hdGNoX29yZGVyIjoxLCJ6ZF90b2tlbiI6bnVsbH0.mNpX-3Ch63HK5kM2gH96OkwqcBISQsoKa5E2vYun_c0&amp;zhida_source=entity">ReAct</a>）。LangChain中包含很多种不同类型的代理。</li></ol><ol type="1" id="1d1a551d-035a-80a0-bd91-c4154a54ccbb" class="numbered-list" start="2"><li><strong>工具</strong>（Tools）：工具是代理调用的函数。这里有两个重要的考虑因素：一是让代理能访问到正确的工具，二是以最有帮助的方式描述这些工具。如果你没有给代理提供正确的工具，它将无法完成任务。如果你没有正确地描述工具，代理将不知道如何使用它们。LangChain提供了一系列的工具，同时你也可以定义自己的工具。</li></ol><ol type="1" id="1d1a551d-035a-80dc-a5ab-c04fcfd27f63" class="numbered-list" start="3"><li><strong>工具包</strong>（Toolkits）：工具包是一组用于完成特定目标的彼此相关的工具，每个工具包中包含多个工具。比如LangChain的Office365工具包中就包含连接Outlook、读取邮件列表、发送邮件等一系列工具。当然LangChain中还有很多其他工具包供你使用。</li></ol><ol type="1" id="1d1a551d-035a-801a-b0b7-e373d68acad7" class="numbered-list" start="4"><li><strong>代理执行器</strong>（AgentExecutor）：代理执行器是代理的运行环境，它调用代理并执行代理选择的操作。执行器也负责处理多种复杂情况，包括处理代理选择了不存在的工具的情况、处理工具出错的情况、处理代理产生的无法解析成工具调用的输出的情况，以及在代理决策和工具调用进行观察和日志记录。</li></ol></details></li></ul><ul id="1d1a551d-035a-80de-9368-f858a40b9053" class="toggle"><li><details open=""><summary>关于&quot;React&quot;的可能误解</summary><p id="1d1a551d-035a-800e-b387-f9e0a5df8e86" class="">代码中存在create_react_agent的导入（来自langchain.agents）<br/>这里的&quot;React&quot;指代的是ReAct方法（Reasoning and Action，推理-行动循环）<br/>是LangChain代理框架中的决策算法模式，与前端框架React无关<br/></p></details></li></ul><ul id="1d1a551d-035a-8084-b106-d426e37e9cf8" class="toggle"><li><details open=""><summary>function——call调用天气查询工具代码</summary><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="1d1a551d-035a-80f8-b00a-e62ad6422474" class="code"><code class="language-Python">import requests
from zhipuai import ZhipuAI
import json
import os


# 自定义函数描述

tools = [
    {
        &quot;type&quot;: &quot;function&quot;,
        &quot;function&quot;: {
            &quot;name&quot;: &quot;get_current_weather&quot;,
            &quot;description&quot;: &quot;获取某个城市或地区的天气预报信息&quot;,
            &quot;parameters&quot;: {
                &quot;type&quot;: &quot;object&quot;,
                &quot;properties&quot;: {
                    &quot;location&quot;: &quot;string&quot;,
                    &quot;description&quot;: &quot;城市信息，只写道城市名即可，如：北京市、上海市等&quot;
                },
                &quot;unit&quot;: {
                    &quot;type&quot;: &quot;string&quot;,
                    &quot;enum&quot;: [&quot;celsius&quot;, &quot;fahrenheit&quot;]
                },
            },
            &quot;required&quot;: [
                &quot;location&quot;
            ],
        },

    },
    {
        &quot;type&quot;: &quot;function&quot;,
        &quot;function&quot;: {
            &quot;name&quot;: &quot;multiply_two_numbers&quot;,
            &quot;description&quot;: &quot;两数相乘&quot;,
            &quot;parameters&quot;: {
                &quot;type&quot;: &quot;object&quot;,
                &quot;properties&quot;: {
                    &quot;a&quot;: {
                        &quot;type&quot;: &quot;float&quot;,
                        &quot;description&quot;: &quot;第一个数字&quot;,
                    },
                    &quot;b&quot;: {
                        &quot;type&quot;: &quot;float&quot;,
                        &quot;description&quot;: &quot;第二个数字&quot;,
                    },
                },
                &quot;required&quot;: [
                    &quot;a&quot;, &quot;b&quot;
                ]
            },
        },
    },
]

class tools_func():
    def __init__(self):
        pass

    def get_current_weather(data):
        city = data[&quot;location&quot;]
        weather_api_key = os.getenv(&quot;OPEN_WEATHER_MAP_API_KEY&quot;)

        citycodeurl = f&quot;http://api.openweathermap.org/geo/1.0/direct?q={city}&amp;limit=5&amp;appid={weather_api_key}&quot;

        response = requests.get(citycodeurl)

        data = response.json()

        if response.status_code == 200:
            lat = data[0][&quot;lat&quot;]
            lon = data[0][&quot;lon&quot;]

        base_url = f&quot;https://api.openweathermap.org/data/2.5/weather?lat={lat}&amp;lon={lon}&amp;units=metric&amp;appid={weather_api_key}&quot;

        response = requests.get(base_url)

        data = response.json()
        print(&quot;以下是json格式的weather——data&quot;)
        print(data)
        if response.status_code == 200:
            weather = {
                &quot;temperature&quot;: data[&quot;main&quot;][&quot;temp&quot;],
                &quot;description&quot;: data[&quot;weather&quot;][0][&quot;description&quot;],
                &quot;city&quot;: data[&quot;name&quot;],
                &quot;country&quot;: data[&quot;sys&quot;][&quot;country&quot;]
            }
            return weather
        else:
            return {&quot;error&quot;: data.get(&quot;message&quot;, &quot;An error occurred.&quot;)}

    def multiply_two_numbers(data):
        a = data[&quot;a&quot;]
        b = data[&quot;b&quot;]
        result = a*b
        return result

def chatglm_tools(input):
    client = ZhipuAI(api_key = os.getenv(&quot;ZHIPU_API_KEY&quot;))

    messages = []

    messages.append({
        &quot;role&quot;: &quot;system&quot;,
        &quot;content&quot;: &quot;不要假设或猜测传入函数的参数值。如果用户的描述不明确，请要求用户提供必要信息&quot;,
    })
    messages.append({
        &quot;role&quot;: &quot;user&quot;,
        &quot;content&quot;: input,
    })

    response = client.chat.completions.create(
        model = &quot;glm-4-air&quot;,
        messages = messages,
        tools = tools,
        tool_choice = &quot;auto&quot;,
    )

    if response.choices[0].message.tool_calls != None:

        tool_call = response.choices[0].message.tool_calls[0]
        args = json.loads(tool_call.function.arguments)
        print(&quot;工具调用信息：&quot;, args )
        function = getattr(tools_func, tool_call.function.name)

        function_result = function(args)
        print(&quot;工具调用结果：&quot;, function_result)

        messages.append({
            &quot;role&quot;: &quot;tool&quot;,
            &quot;content&quot;: json.dumps(function_result),
            &quot;tool_call_id&quot;: tool_call.id
        })

        response1 = client.chat.completions.create(
            model = &quot;glm-4&quot;,
            messages = messages,
            tools = tools,
        )
        result = response1.choices[0].message.content
        print(&quot;大模型输出信息&quot;, result)
        return result
print(chatglm_tools(&quot;今天北京城市的天气预报怎么样？&quot;))
print(&quot;=&quot;*100)
print(chatglm_tools(&quot;4乘以6等于多少&quot;))
</code></pre></details></li></ul><ul id="1d1a551d-035a-80ed-82c6-d265cde8b3c4" class="toggle"><li><details open=""><summary>agent使用工具-联网查询或RAG查询代码</summary><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="1d1a551d-035a-80c8-8c02-ead399836bac" class="code"><code class="language-Python" style="white-space:pre-wrap;word-break:break-all">from langchain.agents import AgentExecutor, create_tool_calling_agent, create_self_ask_with_search_agent, create_react_agent
from langchain.prompts import PromptTemplate
from langchain_community.document_loaders import PDFMinerLoader
from langchain_community.tools.tavily_search import TavilySearchResults
import os
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_openai import OpenAIEmbeddings
from langchain_community.vectorstores import FAISS
from langchain.tools.retriever import create_retriever_tool

# 自己写了个外部的api轮流查看api能否可用，方便动态更新免费openai协议的api资源
from openai_apikey import try_api_bd_model

# 用自己写的外部，这个类算是接口类？网上学到的不清楚
# chat_manager = TryChat()

# 轮流检查api是否连通，函数写在openai_apikey
model = try_api_bd_model(&quot;gpt-3.5-turbo&quot;)


os.environ[&quot;TAVILY_API_KEY&quot;] = os.getenv(&quot;TAVILY_API_KEY&quot;)

loader = PDFMinerLoader(r&quot;C:\Users\ding\Desktop\Python-master\school_1\text_pdf.pdf&quot;)
docs = loader.load()
documents = RecursiveCharacterTextSplitter(chunk_size = 1000, chunk_overlap = 200).split_documents(docs)
vector = FAISS.from_documents(documents, OpenAIEmbeddings())
retriever = vector.as_retriever()
retriever_tools = create_retriever_tool(
    retriever,
    &quot;RAG_search&quot;,
    &quot;搜索关于RAG的信息，任何有关了解RAG的信息都可以使用该工具&quot;,
)



search_tool = TavilySearchResults(max_results = 2)
tools = [search_tool, retriever_tools]

template = &quot;&quot;&quot;
你是一个智能助手，擅长借助工具回答用户的问题。当你需要获取额外信息时，可调用提供的工具.
问题:{input}
可用工具:{tools_names}
{agent_scratchpad}
&quot;&quot;&quot;

prompt = PromptTemplate(
    input_variables = [&quot;input&quot;, &quot;tools_names&quot;, &quot;agent_scratchpad&quot;],
    template = template,
)



# 用agent_executor接受用户输入，顺便准备agent其他需要的环境
# 然后再由agent决定是否使用工具，把是否使用的结果再给agent_executor
# 然后agent_executor再按照agent的要求使用工具查找，结果给agent
# 然后agent再整合工具结果和自身的答案，给agent_executor
# 然后agent_executor再输出最终结果
agent_executor = AgentExecutor(agent = create_tool_calling_agent(model, tools, prompt), tools = tools)
resp = agent_executor.invoke({&quot;input&quot;: &quot;c语言入门，教授c语言的郝斌老师现在人在哪里？怎么样了？&quot;, &quot;tools_names&quot;: &quot;联网搜索&quot;})
print(resp[&quot;output&quot;])

resp = agent_executor.invoke({&quot;input&quot;: &quot;幻觉效应是什么？&quot;, &quot;tools_names&quot;: &quot;RAG向量数据库检索&quot;})
print(resp[&quot;output&quot;])</code></pre></details></li></ul><ul id="1d1a551d-035a-8002-93f9-f018f694b81d" class="toggle"><li><details open=""><summary>轮流检查免费的key是否可用的代码</summary><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="1d1a551d-035a-8073-b94d-c25c3837c2f9" class="code"><code class="language-Python">import os
from langchain_openai import ChatOpenAI

all_openai_api_data = {
    &quot;bug_test_01&quot;: {
        &quot;api_key&quot;: &quot;000&quot;,
        &quot;base_url&quot;: &quot;000&quot;,
    },
    &quot;bug_test_02&quot;: {
        &quot;api_key&quot;: &quot;000&quot;,
        &quot;base_url&quot;: &quot;000&quot;,
    },
    # 这个GPT-API-free是从github上面发现的项目，也是用的openai协议，按每天的免费次数算的
    &quot;GPT-API-free&quot;: {
        &quot;api_key&quot;: os.getenv(&quot;GPT-API-free_API_KEY&quot;),
        &quot;base_url&quot;: os.getenv(&quot;GPT-API-free_BASE_URL&quot;)
    },
    # 注释一下备忘：这个是按账号总余额算的，赠费一美元，用一点少一点不再增加
    &quot;OpenAI-HK&quot;:{
        &quot;api_key&quot;: os.getenv(&quot;OpenAI-HK_API_KEY&quot;),
        &quot;base_url&quot;: os.getenv(&quot;OpenAI-HK_BASE_URL&quot;)
    },
}

def try_api_bd_model(model_name: str) -&gt; ChatOpenAI | None:
    for key, value in all_openai_api_data.items():
        try:
            os.environ[&quot;OPENAI_API_KEY&quot;] = value[&quot;api_key&quot;]
            os.environ[&quot;OPENAI_API_BASE&quot;] = value[&quot;base_url&quot;]
            model_try = ChatOpenAI(model = model_name)
            model_try.invoke(&quot;请输出数字字符：&#x27;1&#x27;，不要输出任何其他字符，也不要输出任何其他内容以及解释性语言&quot;)
            print(f&quot;目前使用的是【{key}】平台的api&quot;)
            return model_try
        except:
            print(f&quot;{key}平台的api已经失效，建议替换，正在测试检查下一组api的连通性&quot;)
            continue
    print(&quot;已经没有可用的api了...建议检查网络&quot;)

# 封装起来变成接口？这个思路是这么回事儿吗？
# 不是不是，误打误撞的简单工厂模式
class TryChat:
    def try_bd_model(self, model_name):
        return try_api_bd_model(model_name)


if __name__ == &quot;__main__&quot;:
# try_api_bd_model(&quot;gpt-3.5-turbo&quot;)
    client = TryChat()
    model = client.try_bd_model(&quot;gpt-3.5-turbo&quot;)</code></pre></details></li></ul><h2 id="1d2a551d-035a-8066-bceb-e316500c9ec4" class="">Day05  项目练习</h2><ul id="1d1a551d-035a-80fb-8c59-ee993ae35863" class="toggle"><li><details open=""><summary>houduan.py</summary><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="1d2a551d-035a-80e1-b025-c2db953ec202" class="code"><code class="language-Python">from abc import ABCMeta, abstractmethod
import requests
import openai
import json
import os
from zhipuai import ZhipuAI
# 昨天刷到了接口隔离的用法，而且要先写接口，今天，尝尝咸淡，看看是不是可读性杠杠的
# # 工具列表整个接口
# class ZhipuAI_tools(metaclass = ABCMeta):
#     @abstractmethod
#     def zhipu_tools_list(self):
#         pass
# class OpenAI_tools(metaclass = ABCMeta):
#     @abstractmethod
#     def openai_tools_list(self):
#         pass

# 工具函数也整个接口：查股票信息的,查nba球星的
# 写着写着后来发现，外部需要调用的好像只有个回答，写这些接口意义不大
class ZhipuAI_Tools_Function(metaclass = ABCMeta):
    @abstractmethod
    def get_stock_info(self,gid):
        pass
    @abstractmethod
    def get_nba_company(self,nba_name):
        pass

class OpenAI_Tools_Function(metaclass = ABCMeta):
    @abstractmethod
    def get_stock_info(self,gid):
        pass
    @abstractmethod
    def get_nba_company(self, nba_name):
        pass

# 打算在这，把最终要调用的，和要输出的，用接口限制一下，意义点应该是在这里吧...

class ZhipuAI_Res(metaclass = ABCMeta):
    @abstractmethod
    def get_response_withtool(self, input: str) -&gt; str:
        pass
class OpenAI_Res(metaclass = ABCMeta):
    @abstractmethod
    def get_response_withtool(self, input: str) -&gt; str:
        pass


class Zhipu_Manager(ZhipuAI_Tools_Function, ZhipuAI_Res):
    def __init__(self):
        self.tools_data =  [
            {
                &quot;type&quot;: &quot;function&quot;,
                &quot;function&quot;: {
                    &quot;name&quot;: &quot;get_stock_info&quot;,
                    &quot;description&quot;: &quot;根据股票编号, 查询该股票信息&quot;,
                    &quot;parameters&quot;: {
                        &quot;type&quot;: &quot;object&quot;,
                        &quot;properties&quot;: {
                            &quot;gid&quot;: {
                                &quot;type&quot;: &quot;string&quot;,
                                &quot;description&quot;: &quot;股票编号，上海股市以sh开头，深圳股市以sz开头&quot;,
                            },
                        },
                        &quot;required&quot;: [&quot;gid&quot;]
                    },
                },
            },
            {
                &quot;type&quot;: &quot;function&quot;,
                &quot;function&quot;: {
                    &quot;name&quot;: &quot;get_nba_company&quot;,
                    &quot;description&quot;: &quot;根据NBA球队名称，查询该球队所拥有球星的信息&quot;,
                    &quot;parameters&quot;: {
                        &quot;type&quot;: &quot;object&quot;,
                        &quot;properties&quot;: {
                            &quot;nba_name&quot;: {
                                &quot;type&quot;: &quot;string&quot;,
                                &quot;description&quot;: &quot;NBA球队名称&quot;
                            },
                        },
                        &quot;required&quot;: [&quot;nba_name&quot;]
                    },
                }
            },
        ]
    def get_stock_info(self,gid):
        url = &quot;http://web.juhe.cn/finance/stock/hs&quot;
        key = os.getenv(&quot;JUHE_API_KEY&quot;)
        url = url + f&quot;?key = {key}&amp;gid = {gid}&quot;
        # headers = {
        #     &#x27;User-Agent&#x27;: &#x27;Apifox/1.0.0 (https://apifox.com)&#x27;,
        # }
        requestParams = {
            &#x27;key&#x27;: key,
            &#x27;gid&#x27;: gid,
            # &#x27;type&#x27;: &#x27;&#x27;,
        }
        response = requests.get(url=url, params=requestParams)
        return response.text
    def get_nba_company(self,nba_name):
        return {&quot;nba_company&quot;: &quot;勒布朗詹姆斯&quot;}
    def get_response_withtool(self,input: str):
        client = ZhipuAI(api_key=os.getenv(&quot;ZHIPU_API_KEY&quot;))
        messages = []
        messages.append({
            &quot;role&quot;: &quot;system&quot;,
            &quot;content&quot;: &quot;不要假设或猜测传入函数的参数值。如果用户的描述不明确，请要求用户提供必要信息&quot;
        })
        messages.append({
            &quot;role&quot;: &quot;user&quot;,
            &quot;content&quot;: input
        })

        response = client.chat.completions.create(
            model=&quot;glm-4&quot;,
            messages=messages,
            tools=self.tools_data,
        )
        # 模型响应
        print(response)

        messages.append(response.choices[0].message.model_dump())

        if response.choices[0].message.tool_calls:
            tool_call = response.choices[0].message.tool_calls[0]
            args = tool_call.function.arguments
            function_result = {}

            if tool_call.function.name == &quot;get_stock_info&quot;:
                function_result = self.get_stock_info(**json.loads(args))
            elif tool_call.function.name == &quot;get_nba_company&quot;:
                function_result = self.get_nba_company(**json.loads(args))

            messages.append({
                &quot;role&quot;: &quot;tool&quot;,
                &quot;content&quot;: json.dumps(function_result),
                &quot;tool_call_id&quot;: tool_call.id,
            })

            response = client.chat.completions.create(
                model=&quot;glm-4&quot;,
                messages=messages,
                tools=self.tools_data,
            )

            message = response.choices[0].message

            messages.append({
                &quot;role&quot;: &quot;system&quot;,
                &quot;content&quot;: str(message)
            })
            final_response = client.chat.completions.create(
                model=&quot;glm-4&quot;,
                messages=messages,
            )
            if final_response.choices[0].message.content is None:
                return &quot;未找到相关信息&quot;  # 避免None报错
            return final_response.choices[0].message.content
        # 这个一会儿加 messages.append(response.choices[0].message.model_dump())

class OpenAI_Manager(OpenAI_Tools_Function, OpenAI_Res):
    def __init__(self):
        self.tools_data = [
            {
                &quot;name&quot;: &quot;get_stock_info&quot;,
                &quot;description&quot;: &quot;根据股票编号, 查询该股票信息&quot;,
                &quot;parameters&quot;: {
                    &quot;type&quot;: &quot;object&quot;,
                    &quot;properties&quot;: {
                        &quot;gid&quot;: {
                            &quot;type&quot;: &quot;string&quot;,
                            &quot;description&quot;: &quot;股票编号，上海股市以sh开头，深圳股市以sz开头&quot;
                        }
                    },
                    &quot;required&quot;: [&quot;gid&quot;]
                }
            },
            {
                &quot;name&quot;: &quot;get_nba_company&quot;,
                &quot;description&quot;: &quot;根据NBA球队名称，查询该球队头号球星的信息&quot;,
                &quot;parameters&quot;: {
                    &quot;type&quot;: &quot;object&quot;,
                    &quot;properties&quot;: {
                        &quot;nba_name&quot;: {
                            &quot;type&quot;: &quot;string&quot;,
                            &quot;description&quot;: &quot;NBA球队名称&quot;
                        }
                    },
                    &quot;required&quot;: [&quot;nba_name&quot;]
                }
            }
        ]
    def get_stock_info(self, gid):
        url = &quot;http://web.juhe.cn/finance/stock/hs&quot;
        key = os.getenv(&quot;JUHE_API_KEY&quot;)
        url = url + f&quot;?key={key}&amp;gid={gid}&quot;

        headers = {
            &#x27;User-Agent&#x27;: &#x27;Apifox/1.0.0 (https://apifox.com)&#x27;,
        }
        # response = requests.request(&quot;GET&quot;, url, headers=headers)
        response = requests.get(url=url, headers=headers)
        return response.text
    def get_nba_company(self, nba_name):
        return {&quot;nba_company&quot;: &quot;勒布朗詹姆斯&quot;}
    def get_response_withtool(self, input: str):

        openai.base_url = os.getenv(&quot;GPT-API-free_BASE_URL&quot;)
        openai.api_key = os.getenv(&quot;GPT-API-free_API_KEY&quot;)
        # 创建消息
        messages = [
            {&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: input}
            # {&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;请帮我查询下股票编号为sh601009的信息?&quot;}
        ]

        response = openai.chat.completions.create(
            model=&quot;gpt-3.5-turbo&quot;,
            messages=messages,
            functions=self.tools_data,
            function_call=&quot;auto&quot;,
        )

        print(response)  # 解析响应

        message = response.choices[0].message

        if message.function_call:
            function_name = message.function_call.name
            arguments = message.function_call.arguments

            if function_name == &quot;get_stock_info&quot;:
                gid = json.loads(arguments)[&quot;gid&quot;]
                stock_info = self.get_stock_info(gid)
                print(&quot;大模型解析了我们提出的问题，并自动调用&#x27;get stock info&#x27;函数获取股票信息:\n&quot;, stock_info)
                messages.append({
                    &quot;role&quot;: &quot;function&quot;,
                    &quot;name&quot;: function_name,
                    &quot;content&quot;: str(stock_info)
                })
            elif function_name == &quot;get_nba_company&quot;:
                nba_name = json.loads(arguments)[&quot;nba_name&quot;]
                nba_company = self.get_nba_company(nba_name)
                print(&quot;大模型解析了我们提出的问题，并自动调用&#x27;get_nba_company&#x27;函数获取球星信息:\n&quot;, nba_company)

                messages.append({
                    &quot;role&quot;: &quot;function&quot;,
                    &quot;name&quot;: function_name,
                    &quot;content&quot;: str(nba_company)
                })
            final_response = openai.chat.completions.create(
                model=&quot;gpt-3.5-turbo&quot;,
                messages=messages,
            )
            if final_response is None:
                return &quot;未找到相关信息&quot;  # 避免None报错
            return final_response.choices[0].message.content

if __name__ == &quot;__main__&quot;:
    # client = OpenAI_Manager()
    client = Zhipu_Manager()
    print(client.get_response_withtool(&quot;请帮我查询下NBA洛杉矶湖人队有哪些球星?&quot;))
    # zhipuai_client = Zhipu_Manager()
    # print(zhipuai_client.get_response_withtool(&quot;请帮我查询下股票编号为sh601009的信息&quot;))
    # zhipuai_client = Zhipu_Manager()
    # print(zhipuai_client.get_response_withtool(&quot;请帮我查询下股票编号为sh601009的信息&quot;))</code></pre></details></li></ul><ul id="1d2a551d-035a-802d-853d-d306055ff903" class="toggle"><li><details open=""><summary>main.py</summary><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="1d2a551d-035a-80dc-b74b-e8fd2933a03d" class="code"><code class="language-Python">import gradio as gr
# 这俩manager是已经写好的类，可以选择平台，只需调用一个方法；如果需要选模型，可再做
from houduan import Zhipu_Manager, OpenAI_Manager


# 暂时没把模型写活，所以用不到

global client
client = Zhipu_Manager()  # 初始化默认值，这个直接跟随单选框的默认值了

def manager_choice(name: str):
    global client  # 声明全局变量
    if name == &#x27;ZhipuAI&#x27;:
        client = Zhipu_Manager()
    elif name == &#x27;OpenAI&#x27;:
        client = OpenAI_Manager()


def user(user_message, history: list):
    history.append({&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: user_message})
    return &quot;&quot;, history

def get_response(history: list):
    global client
    input_text = history[-1][&quot;content&quot;]
    res = client.get_response_withtool(input_text)
    history.append({&quot;role&quot;: &quot;assistant&quot;, &quot;content&quot;: &quot;&quot;})
    history[-1][&quot;content&quot;] += res
    print(res)
    return history
    # for chunk in res:
    #     history[-1][&quot;content&quot;] += chunk
    #     yield history

with gr.Blocks() as demo:
    with gr.Row():
        gr.Markdown(&quot;# 工具调用demo（nba球星查询，api股票查询）&quot;)
    with gr.Row():
        with gr.Column(scale = 2):
            chatbot = gr.Chatbot(type = &quot;messages&quot;, label = &quot;对话框&quot;, height = 500)
            question = gr.Textbox(label = &quot;说词儿啊&quot;)
            clear_btn = gr.Button(&quot;clear_query&quot;)
            gr.Examples([&quot;请帮我查询下NBA洛杉矶湖人队有哪些球星?&quot;, &quot;请帮我查询下股票编号为sh601009的信息&quot;], inputs = [question])
        with gr.Column(scale = 1):
            gr.Markdown(&quot;### 平台选择&quot;)
            pingtai_name = gr.Radio(label = &quot;平台名称/协议类型&quot;, choices = [&quot;ZhipuAI&quot;, &quot;OpenAI&quot;], value = &quot;ZhipuAI&quot;)
            # 这个outputs后面可以加上平台提供的模型名称列表，然后根据选择的平台一起更新
            pingtai_name.change(fn = manager_choice,inputs = pingtai_name, outputs = [])


    question.submit(fn = user, inputs = [question,chatbot], outputs = [question,chatbot]).then(fn = get_response, inputs = chatbot, outputs = chatbot)
    clear_btn.click(lambda: None, None, question, queue=False)


demo.launch()</code></pre></details></li></ul><h1 id="1d5a551d-035a-8069-b81c-ce603c5343b0" class="">WK5 项目实战</h1><h2 id="1d2a551d-035a-805b-93ac-e017535d5de3" class="">Day01  项目练习（续）</h2><ul id="1d5a551d-035a-8044-b1b5-f5af1e7d1dd4" class="toggle"><li><details open=""><summary>houduan.py</summary><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="1d5a551d-035a-8079-9830-dbd1bb846518" class="code"><code class="language-Python" style="white-space:pre-wrap;word-break:break-all">from abc import ABCMeta, abstractmethod
import requests
import openai
import json
import os
from zhipuai import ZhipuAI
from dashscope import Generation

class ZhipuAI_Res(metaclass = ABCMeta):
    @abstractmethod
    def get_response_withtool(self, input: str) -&gt; str:
        pass
class OpenAI_Res(metaclass = ABCMeta):
    @abstractmethod
    def get_response_withtool(self, input: str, temperature) -&gt; str:
        pass
# 这回细了，把不一样的有区别的都提前准备在构造函数里或单独的函数里
# 或者找其他方式分割出来
# 其他尽可能全保持不变，这样改起来不用到处找,再改成继承的形式
class Model_Fun_Tools:
    def __init__(self):
        self.common_tool_descriptions = [
        {
            &quot;name&quot;: &quot;get_stock_info&quot;,
            &quot;description&quot;: &quot;根据股票编号, 查询该股票信息&quot;,
            &quot;parameters&quot;: {
                &quot;type&quot;: &quot;object&quot;,
                &quot;properties&quot;: {
                    &quot;gid&quot;: {
                        &quot;type&quot;: &quot;string&quot;,
                        &quot;description&quot;: &quot;股票编号，上海股市以sh开头，深圳股市以sz开头&quot;
                    }
                },
                &quot;required&quot;: [&quot;gid&quot;]
            }
        },
        {
            &quot;name&quot;: &quot;get_nba_company&quot;,
            &quot;description&quot;: &quot;根据NBA球队名称，查询该球队头号球星的信息&quot;,
            &quot;parameters&quot;: {
                &quot;type&quot;: &quot;object&quot;,
                &quot;properties&quot;: {
                    &quot;nba_name&quot;: {
                        &quot;type&quot;: &quot;string&quot;,
                        &quot;description&quot;: &quot;NBA球队名称&quot;
                    }
                },
                &quot;required&quot;: [&quot;nba_name&quot;]
            }
        },
        {
            &quot;name&quot;: &quot;get_weather&quot;,
            &quot;description&quot;: &quot;获取指定城市的当前实时天气，或未来的天气&quot;,
            &quot;parameters&quot;: {
                &quot;type&quot;: &quot;object&quot;,
                &quot;properties&quot;: {
                    &quot;city&quot;: {
                        &quot;type&quot;: &quot;string&quot;,
                        &quot;description&quot;: &quot;中文格式的城市名称，如北京、上海等&quot;  # 城市参数描述
                    },
                    &quot;is_future&quot;: {
                        &quot;type&quot;: &quot;boolean&quot;,
                        &quot;description&quot;: &quot;是否获取未来天气，False 表示当前天气，True 表示未来天气&quot;  # 是否获取未来天气参数描述
                    }
                },
                &quot;required&quot;: [&quot;city&quot;, &quot;is_future&quot;]  # 必填参数
            }
        },
    ]

    def get_stock_info(self, gid):
        url = &quot;http://web.juhe.cn/finance/stock/hs&quot;
        key = os.getenv(&quot;JUHE_API_KEY&quot;)
        url = url + f&quot;?key={key}&amp;gid={gid}&quot;

        headers = {
            &#x27;User-Agent&#x27;: &#x27;Apifox/1.0.0 (https://apifox.com)&#x27;,
        }
        # response = requests.request(&quot;GET&quot;, url, headers=headers)
        response = requests.get(url=url, headers=headers)
        return response.text
    def get_nba_company(self, nba_name):
        return {&quot;nba_company&quot;: &quot;勒布朗詹姆斯&quot;}
    def get_weather(self, city, is_future = False):
        try:
            print(f&quot;已传入城市参数{city}&quot;)
            url = &quot;http://apis.juhe.cn/simpleWeather/query&quot;
            request_params = {
                &quot;key&quot;: os.getenv(&quot;JUHE_WEATHER_API_KEY&quot;),
                &quot;city&quot;: city,
            }
            response = requests.get(url, params = request_params)
            response.raise_for_status()
            data = response.json()
            print(f&quot;天气接口get返回的响应为{data}&quot;)

            weather = {
                &quot;temperature&quot;: data[&quot;result&quot;][&quot;realtime&quot;][&quot;temperature&quot;],
                &quot;description&quot;: data[&quot;result&quot;][&quot;realtime&quot;][&quot;direct&quot;] + data[&quot;result&quot;][&quot;realtime&quot;][&quot;info&quot;],
                &quot;humidity&quot;: data[&quot;result&quot;][&quot;realtime&quot;][&quot;humidity&quot;],
                &quot;city&quot;: data[&quot;result&quot;][&quot;city&quot;],
                &quot;aqi&quot;: data[&quot;result&quot;][&quot;realtime&quot;][&quot;aqi&quot;],
            }

            if is_future:
                weather[&quot;future&quot;] = data[&quot;result&quot;][&quot;future&quot;][1:]
            return weather
        except Exception as e:
            print(f&quot;get_weather函数发生错误{e}&quot;)


class Zhipu_Manager(ZhipuAI_Res, Model_Fun_Tools):
    def __init__(self):
        super().__init__()
        self.client = ZhipuAI(api_key=os.getenv(&quot;ZHIPU_API_KEY&quot;))
        self.tools_data = [{&quot;type&quot;: &quot;function&quot;, &quot;function&quot;: tool_openai} for tool_openai in self.common_tool_descriptions]

    def get_response_withtool(self,input: str, temperature):
        messages = [{
            &quot;role&quot;: &quot;system&quot;,
            &quot;content&quot;: &quot;不要假设或猜测传入函数的参数值。&quot;
        },
        {
            &quot;role&quot;: &quot;user&quot;,
            &quot;content&quot;: input
        }]

        response = self.client.chat.completions.create(
            model=&quot;glm-4&quot;,
            messages=messages,
            tools=self.tools_data,
            temperature = temperature,
        )
        # 模型响应,接下来解析出模型响应中的给工具函数的参数和对应使用的哪个工具函数
        print(response)

        messages.append(response.choices[0].message.model_dump())

        if response.choices[0].message.tool_calls:
            tool_call = response.choices[0].message.tool_calls[0]
            args = tool_call.function.arguments
            tool_result = {}

            # 根据解析到的函数名+待传入的参数数据，解析出工具函数调用的结果
            if tool_call.function.name == &quot;get_stock_info&quot;:
                tool_result = self.get_stock_info(**json.loads(args))
            elif tool_call.function.name == &quot;get_nba_company&quot;:
                tool_result = self.get_nba_company(**json.loads(args))
            elif tool_call.function.name == &quot;get_weather&quot;:
                tool_result = self.get_weather(**json.loads(args))

            print(f&quot;大模型解析了我们提出的问题，并自动调用{tool_call.function.name}函数获取工具调用结果:\n&quot;, tool_result)
            messages.append({
                &quot;role&quot;: &quot;tool&quot;,
                &quot;content&quot;: json.dumps(tool_result),
                &quot;tool_call_id&quot;: tool_call.id,
            })

            final_response = self.client.chat.completions.create(
                model=&quot;glm-4&quot;,
                messages=messages,
                tools=self.tools_data,
                tool_choice=&quot;auto&quot;,
                temperature = temperature,
            )
            if final_response.choices[0].message.content is None:
                return &quot;未找到相关信息&quot;  # 避免None报错
            return final_response.choices[0].message.content
        # 这个一会儿加 messages.append(response.choices[0].message.model_dump())

class OpenAI_Manager(OpenAI_Res, Model_Fun_Tools):
    def __init__(self):
        super().__init__()
        self.tools_data = self.common_tool_descriptions
        openai.base_url = os.getenv(&quot;GPT-API-free_BASE_URL&quot;)
        openai.api_key = os.getenv(&quot;GPT-API-free_API_KEY&quot;)
    def get_response_withtool(self, input: str, temperature):
        messages = [{
            &quot;role&quot;: &quot;system&quot;,
            &quot;content&quot;: &quot;不要假设或猜测传入函数的参数值。&quot;
        },
        {
            &quot;role&quot;: &quot;user&quot;,
            &quot;content&quot;: input
        }]

        response = openai.chat.completions.create(
            model=&quot;gpt-3.5-turbo&quot;,
            messages=messages,
            functions=self.tools_data,
            function_call=&quot;auto&quot;,
            temperature=temperature,
        )
        # 模型响应,接下来解析出模型响应中的给工具函数的参数和对应使用的哪个工具函数
        print(response)

        message = response.choices[0].message

        if message.function_call:
            function_name = message.function_call.name
            arguments = message.function_call.arguments
            tool_result = None

            # 根据解析到的函数名 + 待传入的参数数据，解析出工具函数调用的结果
            if function_name == &quot;get_stock_info&quot;:
                tool_result = self.get_stock_info(json.loads(arguments)[&quot;gid&quot;])
            elif function_name == &quot;get_nba_company&quot;:
                tool_result = self.get_nba_company(json.loads(arguments)[&quot;nba_name&quot;])
            elif function_name == &quot;get_weather&quot;:
                tool_result = self.get_weather(json.loads(arguments)[&quot;city&quot;], json.loads(arguments)[&quot;is_future&quot;])

            print(f&quot;大模型解析了我们提出的问题，并自动调用{function_name}函数获取工具调用结果:\n&quot;, tool_result)
            messages.append({
                &quot;role&quot;: &quot;function&quot;,
                &quot;name&quot;: function_name,
                &quot;content&quot;: str(tool_result)
            })
            final_response = openai.chat.completions.create(
                model=&quot;gpt-3.5-turbo&quot;,
                messages=messages,
                temperature=temperature,
            )
            if final_response is None:
                return &quot;未找到相关信息&quot;  # 避免None报错
            return final_response.choices[0].message.content

class BaiLian_Manager(OpenAI_Res, Model_Fun_Tools):
    def __init__(self):
        super().__init__()
        self.tools_data = [{&quot;type&quot;: &quot;function&quot;, &quot;function&quot;: tool_openai} for tool_openai in self.common_tool_descriptions]
        self.key = os.getenv(&quot;BAILIAN_API_KEY&quot;)
    def get_response_withtool(self, input: str, temperature):
        messages = [{
            &quot;role&quot;: &quot;system&quot;,
            &quot;content&quot;: &quot;不要假设或猜测传入函数的参数值。&quot;
        },
        {
            &quot;role&quot;: &quot;user&quot;,
            &quot;content&quot;: input
        }]

        response = Generation.call(
            api_key = self.key,
            model=&#x27;qwen2.5-72b-instruct&#x27;,
            messages=messages,
            function_call=&quot;auto&quot;,
            tools = self.tools_data,
            result_format=&#x27;message&#x27;,  # 设置结果格式为消息
            temperature=temperature,
        )
        # 模型响应,接下来解析出模型响应中的给工具函数的参数和对应使用的哪个工具函数
        print(response)

        # 百炼模型独有的取response消息结构，为解析做准备
        message = response.output.choices[0].message

        if hasattr(message, &#x27;tool_calls&#x27;) and message.tool_calls:
            tool_call = message.tool_calls[0]
            # function_name, weather_info =
            function_name = tool_call[&#x27;function&#x27;][&#x27;name&#x27;]
            arguments = tool_call[&#x27;function&#x27;][&#x27;arguments&#x27;]

            # function_name = message.function_call.name
            # arguments = message.function_call.arguments
            tool_result = None

            # 根据解析到的函数名 + 待传入的参数数据，解析出工具函数调用的结果
            if function_name == &quot;get_stock_info&quot;:
                tool_result = self.get_stock_info(json.loads(arguments)[&quot;gid&quot;])
            elif function_name == &quot;get_nba_company&quot;:
                tool_result = self.get_nba_company(json.loads(arguments)[&quot;nba_name&quot;])
            elif function_name == &quot;get_weather&quot;:
                tool_result = self.get_weather(json.loads(arguments)[&quot;city&quot;], json.loads(arguments)[&quot;is_future&quot;])

            print(f&quot;大模型解析了我们提出的问题，并自动调用{function_name}函数获取工具调用结果:\n&quot;, tool_result)
            # messages.append({
            #     &quot;role&quot;: &quot;function&quot;,
            #     &quot;name&quot;: function_name,
            #     &quot;content&quot;: f&quot;根据可使用的工具提供的资料为： {str(tool_result)}&quot;
            # })
            # 这部分百炼识别不到，改成下面的message格式了
            messages_bailian = [{
                &quot;role&quot;: &quot;user&quot;,
                &quot;content&quot;: input
            },
            {
                &quot;role&quot;: &quot;assistant&quot;,
                &quot;content&quot;: str(tool_result)
            }]
            final_response = Generation.call(
                api_key = self.key,
                model=&#x27;qwen2.5-72b-instruct&#x27;,
                messages = messages_bailian,
                function_call=&quot;auto&quot;,
                # tools=self.tools_data,
                result_format=&#x27;message&#x27;,  # 设置结果格式为消息
                temperature=temperature,
            )
            if final_response is None:
                return &quot;未找到相关信息&quot;  # 避免None报错
            return final_response.output.choices[0].message.content

if __name__ == &quot;__main__&quot;:
    # client = OpenAI_Manager()
    # print(client.get_response_withtool(&quot;今天北京天气怎么样&quot;))
    # print(client.get_response_withtool(&quot;请帮我查询下NBA洛杉矶湖人队有哪些球星?&quot;))
    # print(client.get_response_withtool(&quot;请帮我查询下股票编号为sh601009的信息&quot;))

    # zhipuai_client = Zhipu_Manager()
    # print(zhipuai_client.get_response_withtool(&quot;今天北京天气怎么样&quot;))
    # print(zhipuai_client.get_response_withtool(&quot;请帮我查询下NBA洛杉矶湖人队有哪些球星?&quot;))
    # print(zhipuai_client.get_response_withtool(&quot;请帮我查询下股票编号为sh601009的信息&quot;))

    # client = BaiLian_Manager()
    # print(client.get_response_withtool(&quot;今天北京天气怎么样&quot;))
    # print(client.get_response_withtool(&quot;请帮我查询下NBA洛杉矶湖人队有哪些球星?&quot;))
    # print(client.get_response_withtool(&quot;请帮我查询下股票编号为sh601009的信息&quot;))
    pass</code></pre></details></li></ul><ul id="1d5a551d-035a-802a-aea8-ee257bd16a7e" class="toggle"><li><details open=""><summary>main.py</summary><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="1d5a551d-035a-8054-ac50-c1fa704b69bc" class="code"><code class="language-Python" style="white-space:pre-wrap;word-break:break-all">import gradio as gr
# 这俩manager是已经写好的类，可以选择平台，只需调用一个方法；如果需要选模型，可再做
from school_workbook.houduan import Zhipu_Manager, OpenAI_Manager, BaiLian_Manager

# 暂时没把模型写活，所以用不到

global client
client = Zhipu_Manager()  # 初始化默认值，这个直接跟随单选框的默认值了

def manager_choice(name: str):
    global client  # 声明全局变量
    if name == &#x27;ZhipuAI&#x27;:
        client = Zhipu_Manager()
    elif name == &#x27;OpenAI&#x27;:
        client = OpenAI_Manager()
    elif name == &#x27;BaiLian&#x27;:
        client = BaiLian_Manager()

def user(user_message, history: list):
    history.append({&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: user_message})
    return &quot;&quot;, history

def get_response(temperature, history: list):
    global client
    input_text = history[-1][&quot;content&quot;]
    res = client.get_response_withtool(input_text, temperature)
    history.append({&quot;role&quot;: &quot;assistant&quot;, &quot;content&quot;: &quot;&quot;})
    history[-1][&quot;content&quot;] += res
    print(res)
    return history
    # for chunk in res:
    #     history[-1][&quot;content&quot;] += chunk
    #     yield history

with gr.Blocks() as demo:
    with gr.Row():
        gr.Markdown(&quot;# 工具调用demo（nba球星查询，api股票查询，当前和未来天气查询）&quot;)
    with gr.Row():
        with gr.Column(scale = 2):
            chatbot = gr.Chatbot(type = &quot;messages&quot;, label = &quot;对话框&quot;, height = 500)
            question = gr.Textbox(label = &quot;说词儿啊&quot;)
            clear_btn = gr.Button(&quot;clear_query&quot;)
            gr.Examples([&quot;请帮我查询下NBA洛杉矶湖人队有哪些球星?&quot;, &quot;请帮我查询下股票编号为sh601009的信息&quot;, &quot;今天北京天气怎么样&quot;, &quot;未来北京天气怎么样&quot;], inputs = [question])
        with gr.Column(scale = 1):
            gr.Markdown(&quot;### 平台选择&quot;)
            pingtai_name = gr.Radio(label = &quot;平台名称/协议类型&quot;, choices = [&quot;ZhipuAI&quot;, &quot;OpenAI&quot;, &quot;BaiLian&quot;], value = &quot;ZhipuAI&quot;)
            temperature = gr.Slider(minimum = 0, maximum=1,label = &quot;temperature&quot;, value = 0.8, step = 0.1)


            # 这个outputs后面可以加上平台提供的模型名称列表，然后根据选择的平台一起更新
            pingtai_name.change(fn = manager_choice,inputs = pingtai_name, outputs = [])


    question.submit(fn = user, inputs = [question,chatbot], outputs = [question,chatbot]).then(fn = get_response, inputs = [temperature, chatbot], outputs = chatbot)
    clear_btn.click(lambda: None, None, question, queue=False)


demo.launch()</code></pre></details></li></ul><h2 id="1d5a551d-035a-8019-9d91-f57e1506b2f6" class="">Day02  医疗项目练习（加上上一个项目，没用到的就只剩召回重排和agent和其他webui界面库了，之后拆底层记得把功能加进去）</h2><p id="1d6a551d-035a-804d-b1a1-ea050df80208" class="">本项目仍未设计完整</p><ul id="1d6a551d-035a-8058-bcfc-fb912205cedb" class="toggle"><li><details open=""><summary>计划待用代码</summary><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="1d6a551d-035a-80e0-b378-ff1c6f32e88e" class="code"><code class="language-Python" style="white-space:pre-wrap;word-break:break-all"># 先转成面向对象，再写ui（功能分齐全），再拆底层
from langchain.agents import AgentExecutor, create_tool_calling_agent, create_self_ask_with_search_agent, create_react_agent
from langchain.prompts import PromptTemplate
from langchain_community.document_loaders import PDFMinerLoader
from langchain_community.tools.tavily_search import TavilySearchResults
import os
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_openai import OpenAIEmbeddings
from langchain_community.vectorstores import FAISS
from langchain.tools.retriever import create_retriever_tool

# 自己写了个外部的api轮流查看api能否可用，方便动态更新免费openai协议的api资源
from openai_apikey import try_api_bd_model

# 用自己写的外部，这个类算是接口类？网上学到的不清楚
# chat_manager = TryChat()

# 轮流检查api是否连通，函数写在openai_apikey
model = try_api_bd_model(&quot;gpt-3.5-turbo&quot;)


os.environ[&quot;TAVILY_API_KEY&quot;] = os.getenv(&quot;TAVILY_API_KEY&quot;)

loader = PDFMinerLoader(r&quot;C:\Users\ding\Desktop\Python-master\school_1\text_pdf.pdf&quot;)
docs = loader.load()
documents = RecursiveCharacterTextSplitter(chunk_size = 1000, chunk_overlap = 200).split_documents(docs)
vector = FAISS.from_documents(documents, OpenAIEmbeddings())
retriever = vector.as_retriever()
retriever_tools = create_retriever_tool(
    retriever,
    &quot;RAG_search&quot;,
    &quot;搜索关于RAG的信息，任何有关了解RAG的信息都可以使用该工具&quot;,
)

search_tool = TavilySearchResults(max_results = 2)
tools = [search_tool, retriever_tools]


template = &quot;&quot;&quot;
你是一个智能助手，擅长借助工具回答用户的问题。当你需要获取额外信息时，可调用提供的工具.
问题:{input}
可用工具:{tools_names}
{agent_scratchpad}
&quot;&quot;&quot;

prompt = PromptTemplate(
    input_variables = [&quot;input&quot;, &quot;tools_names&quot;, &quot;agent_scratchpad&quot;],
    template = template,
)

# 用agent_executor接受用户输入，顺便准备agent其他需要的环境
# 然后再由agent决定是否使用工具，把是否使用的结果再给agent_executor
# 然后agent_executor再按照agent的要求使用工具查找，结果给agent
# 然后agent再整合工具结果和自身的答案，给agent_executor
# 然后agent_executor再输出最终结果
agent_executor = AgentExecutor(agent = create_tool_calling_agent(model, tools, prompt), tools = tools)
resp = agent_executor.invoke({&quot;input&quot;: &quot;c语言入门，教授c语言的郝斌老师现在人在哪里？怎么样了？&quot;, &quot;tools_names&quot;: &quot;联网搜索&quot;})
print(resp[&quot;output&quot;])

resp = agent_executor.invoke({&quot;input&quot;: &quot;幻觉效应是什么？&quot;, &quot;tools_names&quot;: &quot;RAG向量数据库检索&quot;})
print(resp[&quot;output&quot;])

</code></pre></details></li></ul><ul id="1d6a551d-035a-80f8-8f54-f13daeb35ef5" class="toggle"><li><details open=""><summary>chroma</summary><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="1d6a551d-035a-80e2-802f-cd11b04ea92a" class="code"><code class="language-Python" style="white-space:pre-wrap;word-break:break-all"># 导入 os 模块，用于与操作系统进行交互，例如操作文件路径、环境变量等
import os
# 导入 streamlit 库并别名为 st，Streamlit 用于快速创建数据应用程序和交互式界面
import streamlit as st
# 从 robot 模块中导入 Robot 类，Robot 类可能是用于处理聊天机器人相关的逻辑
from robot import Robot
# 从 chroma 模块中导入 MyChroma 类，MyChroma 类可能与向量数据库操作有关
from chroma import MyChroma

# 检查会话状态中是否存在名为&#x27;started&#x27;的键
# 如果不存在，说明这是会话的首次加载或初始化
if &#x27;started&#x27; not in st.session_state:
    # 在会话状态中设置&#x27;started&#x27;键的值为 True，表示会话已开始初始化
    st.session_state.started = True
    # 从指定文件夹（&#x27;./files/rag&#x27;）加载数据到名为&#x27;rag_collection&#x27;的集合中，
    # 创建 MyChroma 实例并存储在会话状态的&#x27;chroma&#x27;键下
    st.session_state.chroma = MyChroma.from_folder(&#x27;./files/rag&#x27;, &#x27;rag_collection&#x27;)
    # 使用指定的模型配置（&#x27;gpt-3.5-turbo&#x27;）和从会话状态中获取的检索器（来自 MyChroma）
    # 创建 Robot 实例，并存储在会话状态的&#x27;robot&#x27;键下
    st.session_state.robot = Robot({&#x27;model&#x27;: &#x27;gpt-3.5-turbo&#x27;}, st.session_state.chroma.as_retriever(k=3))
    # 在会话状态中设置&#x27;session_id&#x27;键的值为 1，用于标识当前会话的 ID
    st.session_state.session_id = 1

# 导入自定义的 funcs 模块并别名为 funcs，该模块可能包含了一些业务逻辑函数
import funcs as funcs

# 定义 init_interface 函数，用于初始化和展示应用程序的界面
def init_interface():
    # 设置 Streamlit 应用程序的页面配置，包括页面标题为&quot;Medical Chatbot&quot;和布局为宽屏
    st.set_page_config(page_title=&quot;Medical Chatbot&quot;, layout=&quot;wide&quot;)
    # 在应用程序页面上显示标题&quot;Molly 医疗精灵&quot;
    st.title(&quot;Molly 医疗精灵&quot;)
    # 调用 funcs 模块中的 get_session_messages 函数获取当前会话的聊天消息列表
    messages = funcs.get_session_messages()
    # 遍历获取到的聊天消息列表
    for message in messages:
        # 解包消息元组，分别获取消息的角色（如&quot;Human&quot;或&quot;AI&quot;）和内容
        role, content = message
        # 根据消息的角色，在 Streamlit 聊天界面中显示相应的聊天消息
        with st.chat_message(role):
            st.write(content)
    # 在应用程序中创建一个聊天输入框，提示用户输入问题，并将用户输入的内容赋值给 question 变量
    question = st.chat_input(&quot;输入问题提问....&quot;)
    # 如果用户输入了问题（question 不为 None）
    if question is not None:
        # 调用 funcs 模块中的 create_response 函数，根据用户问题生成回答
        response = funcs.create_response(question)
        # 在聊天界面中显示用户输入的问题，角色标识为&quot;Human&quot;
        st.chat_message(&quot;Human&quot;).write(question)
        # 在聊天界面中以流式输出的方式显示 AI 的回答，角色标识为&quot;AI&quot;
        st.chat_message(&quot;AI&quot;).write_stream(response)
    # 在应用程序的侧边栏中进行以下操作
    with st.sidebar:
        # 在侧边栏中显示当前对话的 ID，ID 从会话状态的&#x27;session_id&#x27;键获取
        st.header(f&quot;当前对话ID：{st.session_state.session_id}&quot;)
        # 在侧边栏中创建一个按钮，按钮文本为&quot;开始新对话&quot;，
        # 点击按钮时调用 funcs 模块中的 start_session 函数
        st.button(&quot;开始新对话&quot;, on_click=funcs.start_session)
        # 遍历 funcs 模块中获取的所有会话 ID 列表
        for session_id in funcs.get_all_session_ids():
            # 在侧边栏中创建一个可展开的区域，标题为当前会话 ID
            with st.expander(f&quot;对话ID：{session_id}&quot;):
                # 在展开区域中创建两列布局
                col1, col2 = st.columns(2)
                # 在第一列中创建一个按钮，按钮文本为&quot;继续对话&quot;，
                # 点击按钮时调用 funcs 模块中的 continue_session 函数，并传递当前会话 ID 作为参数
                col1.button(&quot;继续对话&quot;, on_click=funcs.continue_session, args=(session_id,), key=f&quot;Restart_{session_id}&quot;)
                # 在第二列中创建一个按钮，按钮文本为&quot;删除对话&quot;，
                # 点击按钮时调用 funcs 模块中的 delete_session 函数，并传递当前会话 ID 作为参数
                col2.button(&quot;删除对话&quot;, on_click=funcs.delete_session, args=(session_id,), key=f&quot;Delete_{session_id}&quot;)
                # 调用 funcs 模块中的 get_session_messages 函数获取指定会话 ID 的聊天消息列表
                messages = funcs.get_session_messages(session_id)
                # 遍历获取到的聊天消息列表
                for message in messages:
                    # 解包消息元组，分别获取消息的角色（如&quot;Human&quot;或&quot;AI&quot;）和内容
                    role, content = message
                    # 根据消息的角色，在侧边栏的聊天界面中显示相应的聊天消息
                    with st.chat_message(role):
                        st.write(content)

# 如果当前脚本是主程序入口
if __name__ == &#x27;__main__&#x27;:
    # 设置环境变量&quot;OPENAI_API_KEY&quot;的值，用于访问 OpenAI 的 API
    os.environ[&quot;OPENAI_API_KEY&quot;] = &#x27;sk-WsX5IosThRA057bUxujAHKSH9YMRtG0Rgq7IRBsYKMfCECFP&#x27;
    os.environ[&quot;OPENAI_API_BASE&quot;] = &#x27;https://api.siliconflow.cn/v1/chat/completions&#x27;
    os.environ[&quot;OPENAI_API_BASE&quot;] = &#x27;https://api.chatanywhere.tech&#x27;
    # 调用 init_interface 函数，初始化并展示应用程序的界面
    init_interface()</code></pre></details></li></ul><ul id="1e0a551d-035a-809e-bb61-d1a0c1f05f66" class="toggle"><li><details open=""><summary>robot</summary><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="1e0a551d-035a-80b8-a24d-f753bf6b984b" class="code"><code class="language-Python" style="white-space:pre-wrap;word-break:break-all">import os
#向量库
import sqlite3
#类型提示
from typing import Iterator
#创建聊天提示模板
from langchain.prompts import ChatPromptTemplate
#系统消息和AI消息
from langchain_core.messages import SystemMessage,AIMessage
# 构建可运行的链
from langchain_core.runnables import RunnableWithMessageHistory,RunnableLambda,RunnablePassthrough
# 聊天历史记录的基类
from langchain_core.chat_history import BaseChatMessageHistory
#openai聊天进行交互
from langchain_openai import ChatOpenAI
#将聊天历史记录存储到sql数据库中
from  langchain_community.chat_message_histories.sql import SQLChatMessageHistory

class Robot:
    #定义系统提示，告诉AI如何回答用户的问题
    system_propmt = &quot;你是一个名叫Molly的医学专家，\
           对于用户提问的医学相关问题，你需要按照给出的参考文献资料对问题进行回答。\
           你的回答需要按照以下步骤：\
               1. 分析用户问题、对话历史以及参考文献，判断参考资料的哪些内容可以解答用户的问题，并将这一过程进行说明。\
               2. 如果参考文献可以解答用户的问题，则根据文献内容对问题进行解答。\
               3. 如果参考文献不能解答用户问题，告诉用户信息不足，无法回答，建议用户寻求专业人士帮助，不要自行发挥。\
           你的回答需要注意以下几点： \
               1. 保证你的回答是清晰的、明确的。如果你参考了参考资料，应该指出参考资料的标题等。\
               2. 结合用户的对话历史，分析用户的问题意图。但不要复述问题。\
               2. 回复用户时，使用对话的口吻，有礼貌地称呼用户为”您“，不要使用“用户”来称呼！\
               3. 如果用户的问题与医学无关，判断用户的目的，并温柔地提示其回到医学话题。\
           再次提醒：请严格遵守以上规则，当参考资料不足时，拒绝回答问题，不要自行发挥！&quot;
    #问候语提示，AI与用户对话开始时候的欢迎语
    greeting_prompt = &quot;你好！我是Molly医疗精灵，专注解决你的医疗问题。请问你需要什么帮助？&quot;
    #定义提示模版，包含用户的问题，参考资料和对话历史
    prompt_template = &quot;##用户问题：{input}\n\n ##参考资料：\n\n ##本地知识库：{rag_results}\n\n ##对话历史：{chat_history}&quot;

    def __init__(self,model_config,retriever=None):
        os.environ[&quot;OPENAI_API_KEY&quot;] = &#x27;hk-78s05w100005368902b62ce2a6216ecad6e94ed207fab3f7&#x27;
        os.environ[&quot;OPENAI_API_BASE&quot;] = &#x27;https://api.openai-hk.com/v1&#x27;
        #如果没有提供检索器，则创建一个默认的检索器，返回空的参考资料和对话历史
        if retriever is None:
            retriever =RunnableLambda(lambda input:{&quot;input&quot;:input,&quot;rag_results&quot;:&quot;&quot;,&quot;chat_history&quot;:&quot;&quot;})
        #初始化模型，传入模型配置信息
        self.model =ChatOpenAI(**model_config)
        #保存检索器
        self.retriever=retriever
        #根据模版创建聊天提示
        self.prompt=ChatPromptTemplate.from_messages([(&quot;human&quot;,self.prompt_template)])
        #创建一个带有历史记录功能的运行链，赋值给model_with_message_history
        self.model_with_message_history =RunnableWithMessageHistory(
            #链式操作符， 提示模版 和语言模型 连接起来
            self.prompt | self.model,
            #可调用的函数，获取当前会话的聊天历史记录
            get_session_history=self.get_session,
            #输入消息在输入字典中的键名
            input_messages_key=&#x27;input&#x27;,
            history_messages_key=&#x27;chat_history&#x27;
        )
        #构建最终链； 将输入、检索器、历史聊天记录组合在一起
        self.chain={&quot;input&quot;:RunnablePassthrough(),&quot;rag_results&quot;:self.retriever,&quot;chat_history&quot;:RunnablePassthrough()} | self.model_with_message_history
        #加载会话数据
        self.load_session_data()

    def get_session(self,session_id:int)-&gt;BaseChatMessageHistory:
        #如果会话id不在会话数据中，则创建一个新的会话记录
        if session_id not in self.session_data.keys():
            #session_id 存储聊天记录的会话id
            #connection_string： 指定数据库的连接字符串
            self.session_data[session_id]=SQLChatMessageHistory(session_id,connection_string=&#x27;sqlite:///files///chat_history.db&#x27;)
            #系统提示消息
            self.session_data[session_id].add_message(SystemMessage(self.system_propmt))
            #添加问候语消息
            self.session_data[session_id].add_message(AIMessage(self.greeting_prompt))

        return self.session_data[session_id]

    #加载会话记录
    def load_session_data(self):
        #连接到SQLite数据库
        with sqlite3.connect(&quot;files/chat_history.db&quot;) as client:
            #创建游标对象，执行sql语句
            cur =client.cursor()
            cur.execute(&quot;select count(*) from sqlite_master where type=&#x27;table&#x27; and name=&#x27;message_store&#x27;&quot;)
            #如果表不存在
            if cur.fetchall()[0][0] == 0:
                #初始化会话数据为空字典
                self.session_data={}
            else:
                #如果存在，查询所有不同的会话id
                cur.execute(&quot;select distinct session_id from message_store&quot;)
                session_ids=cur.fetchall()
                #创建一个字典，存储每个会话的聊天历史记录对象
                self.session_data={
                    #字典的键是会话id，将会话id转换为整数类型
                    int(session_id[0]):
                    #字典的值:SQLChatMessageHistory的实例,管理会话的聊天历史记录
                    SQLChatMessageHistory(
                        int(session_id[0]),
                        connection_string=&quot;sqlite:///files/chat_history.db&quot;
                    )
                    #字典推导式，遍历session_ids 列表中的每个会话id
                    for session_id in session_ids
                }

    def chat(self,input:str,session_id:int)-&gt;AIMessage:
        #配置会话
        config ={&quot;configurable&quot;:{&#x27;session_id&#x27;:session_id}}
        #调用链处理输入，获取响应
        response=self.chain.invoke(input,config=config)

        return response.content

if __name__ == &#x27;__main__&#x27;:
    rebot =Robot(model_config={&quot;model&quot;:&quot;gpt-3.5-turbo&quot;})
    question=&quot;你好，请问你是谁?&quot;
    print(&quot;问题:&quot;,question)
    response=rebot.chat(question,1)
    print(&quot;答复：&quot;,response)















</code></pre></details></li></ul><h2 id="1d6a551d-035a-809e-99b7-f4e671efcf5d" class="">Day03  医疗精灵项目（完整）</h2><ul id="1e0a551d-035a-8050-aecd-df269ce33a42" class="toggle"><li><details open=""><summary>chroma.py</summary><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="1e0a551d-035a-807f-b918-f8e73372e935" class="code"><code class="language-Python" style="white-space:pre-wrap;word-break:break-all"># 导入 os 模块，用于与操作系统进行交互，例如文件路径操作等
import os

# 从 langchain_chroma 模块导入 Chroma 类，Chroma 是一个向量数据库，用于存储和检索文档向量
from langchain_chroma import Chroma
# 从 langchain_openai 模块导入 OpenAIEmbeddings 类，用于将文本转换为向量表示
from langchain_openai import OpenAIEmbeddings
# 从 langchain_community.document_loaders 模块导入 PDFMinerLoader 类，用于加载 PDF 文件并提取文本内容
from langchain_community.document_loaders import PDFMinerLoader
# 从 langchain_text_splitters 模块导入 RecursiveCharacterTextSplitter 类，用于将文本分割成较小的块
from langchain_text_splitters import RecursiveCharacterTextSplitter
# 设置 OpenAI API 密钥
os.environ[&quot;OPENAI_API_KEY&quot;]=&#x27;hk-78s05w100005368902b62ce2a6216ecad6e94ed207fab3f7&#x27;
os.environ[&quot;OPENAI_API_BASE&quot;]=&#x27;https://api.openai-hk.com/v1&#x27;

# 定义自定义的 MyChroma 类，继承自 Chroma 类
class MyChroma(Chroma):
    # 类属性，使用 OpenAIEmbeddings 作为嵌入函数，用于将文本转换为向量
    embedding_function = OpenAIEmbeddings()

    # 定义 add_file 方法，用于将指定文件名的 PDF 文件添加到向量数据库中
    def add_file(self, filename):
        # 使用 PDFMinerLoader 加载指定的 PDF 文件，将其内容转换为文档对象
        document = PDFMinerLoader(filename).load()
        # 使用 RecursiveCharacterTextSplitter 对文档进行分割，每个块大小为 200 字符，重叠部分为 40 字符
        splits = RecursiveCharacterTextSplitter(chunk_size=200, chunk_overlap=40).split_documents(document)
        # 将分割后的文档块添加到向量数据库中
        self.add_documents(splits)

    # 定义类方法 from_folder，用于从指定文件夹加载 PDF 文件并添加到向量数据库中
    @classmethod
    def from_folder(cls, persist_directory, collection_name, folder_path=None):
        # 创建 MyChroma 类的实例，传入集合名称、嵌入函数和持久化目录
        # cls：在类方法中，cls 是一个约定俗成的参数名，代表类本身，类似于实例方法中的 self 代表实例对象。
        # 通过 cls 可以调用类的属性和方法，也可以用来创建类的实例。
        self = cls(collection_name, cls.embedding_function, persist_directory)
        # 如果指定了文件夹路径
        if folder_path:
            # 获取文件夹中所有以 .pdf 结尾的文件的完整路径
            #1.os.listdir(folder_path)
            #os.listdir(path) 这个函数能够返回指定路径 path 下的所有文件和文件夹名称所构成的列表。

            #2. f.endswith(&#x27;.pdf&#x27;)
            #endswith(suffix) 是字符串对象的一个方法，其作用是判断字符串是否以指定的后缀 suffix 结尾。
            # 若以该后缀结尾，就返回 True；反之则返回 False。
            #在这段代码里，f 代表 os.listdir(folder_path) 返回列表中的每个元素（也就是文件名）。
            # f.endswith(&#x27;.pdf&#x27;) 会检查文件名是否以 .pdf 结尾，以此筛选出 PDF 文件。

            #3. os.path.join(folder_path, f)
            #os.path.join(folder_path, f) 会把文件夹路径和文件名组合成该文件的完整路径。

            files = [os.path.join(folder_path, f) for f in os.listdir(folder_path) if f.endswith(&#x27;.pdf&#x27;)]
            # 遍历所有 PDF 文件
            for f in files:
                # 调用 add_file 方法将每个 PDF 文件添加到向量数据库中
                self.add_file(f)
        # 返回创建好的 MyChroma 实例
        return self


if __name__ == &#x27;__main__&#x27;:
    # 创建向量数据库，从指定文件夹加载 PDF 文件并添加到名为 &#x27;rag_collection&#x27; 的集合中，持久化目录为 &#x27;./files/rag&#x27;
    chroma = MyChroma.from_folder(&#x27;./files/rag&#x27;, &#x27;rag_collection&#x27;, folder_path=&#x27;./files/docs&#x27;)

    # # 读取现有向量数据库
    chroma = MyChroma.from_folder(&#x27;./files/rag&#x27;, &#x27;rag_collection&#x27;)

    # # 打印向量数据库数据
    # documents = chroma.get()
    # n_documents = len(documents[&#x27;ids&#x27;])
    # for i in range(n_documents):
    #     text = documents[&#x27;documents&#x27;][i].replace(&#x27;\n&#x27;, &#x27;&#x27;).replace(&#x27; &#x27;, &#x27;&#x27;)
    #     #:&lt;.10s 是格式化说明符，: 用于分隔变量和格式化选项，&lt; 表示左对齐，.10s 表示截取字符串的前 10 个字符。
    #     # 所以这部分的作用是输出文档 ID 的前 10 个字符，并且左对齐。
    #     print(f&quot;Document {i}: {documents[&#x27;ids&#x27;][i]:&lt;.10s}... 内容：{text[:20]:&lt;20s}...{text[-20:]:&lt;20s}&quot;)

    # # 删除向量数据库的数据
    #chroma.delete(...)

    # # 删除整个向量数据库
    #chroma.reset_collection()

    # # 创建检索器
    # retriver = chroma.as_retriver()</code></pre></details></li></ul><ul id="1e0a551d-035a-800b-8b16-ed08f2e334d9" class="toggle"><li><details open=""><summary>funcs.py</summary><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="1e0a551d-035a-8091-9fd9-f5c521430afc" class="code"><code class="language-Python" style="white-space:pre-wrap;word-break:break-all"># 导入 streamlit 库，用于快速创建 Web 应用程序
import streamlit as st
# 从 langchain_core.messages 模块导入 HumanMessage 类，用于处理人类消息
from langchain_core.messages import HumanMessage

# 定义一个函数，用于获取指定会话 ID 的聊天消息列表
# session_id 为可选参数，若未提供则使用当前会话 ID
def get_session_messages(session_id: int=None):
    # 初始化一个空列表，用于存储消息元组
    messages = []
    # 如果 session_id 未提供，则使用 streamlit 会话状态中的 session_id
    session_id = session_id or st.session_state.session_id
    # 通过会话 ID 获取对应的会话对象
    session = st.session_state.robot.get_session(session_id)
    # 遍历会话对象中的消息列表，从第二个消息开始（跳过第一个可能的系统消息）
    for message in session.messages[1:]:
        # 判断消息是否为人类消息，如果是则将角色标记为 &quot;Human&quot;，否则标记为 &quot;AI&quot;
        message = (&quot;Human&quot;, message.content) if isinstance(message, HumanMessage) else (&quot;AI&quot;, message.content)
        # 将处理后的消息元组添加到消息列表中
        messages.append(message)
    # 返回消息列表
    return messages

# 定义一个函数，用于获取所有会话的 ID 列表
def get_all_session_ids():
    # 使用列表推导式从机器人的会话数据字典中提取所有会话 ID
    return [session_id for session_id in st.session_state.robot.session_data.keys()]

# 定义一个函数，用于根据用户问题生成 AI 回复
# question 为用户提出的问题，session_id 为可选的会话 ID，默认为当前会话 ID
def create_response(question: str, session_id: int = None):
    # 如果 session_id 未提供，则使用 streamlit 会话状态中的 session_id
    session_id = session_id or st.session_state.session_id
    # 调用机器人的 stream 方法，传入问题和会话 ID，获取 AI 回复
    response = st.session_state.robot.stream(question, session_id)
    # 返回 AI 回复
    return response

# 定义一个函数，用于启动一个新的会话
def start_session():
    # 计算新的会话 ID，取当前所有会话 ID 中的最大值加 1
    st.session_state.session_id = max(st.session_state.robot.session_data.keys(), default=0) + 1
    # 根据新的会话 ID 获取对应的会话对象
    st.session_state.robot.get_session(st.session_state.session_id)

# 定义一个函数，用于继续指定会话 ID 的会话
# session_id 为要继续的会话的 ID
def continue_session(session_id: int):
    # 将 streamlit 会话状态中的 session_id 更新为指定的会话 ID
    st.session_state.session_id = session_id

# 定义一个函数，用于删除指定会话 ID 的会话
# session_id 为要删除的会话的 ID
def delete_session(session_id: int):
    # 从机器人的会话数据字典中移除指定会话 ID 对应的会话对象，并获取该对象
    chathistory = st.session_state.robot.session_data.pop(session_id)
    # 清空该会话对象的聊天历史记录
    chathistory.clear()
    # 更新 streamlit 会话状态中的 session_id 为剩余会话 ID 中的最大值，若没有则默认为 1
    st.session_state.session_id = max(st.session_state.robot.session_data.keys(), default=1)</code></pre></details></li></ul><ul id="1e0a551d-035a-8042-b099-e5334a4c2b63" class="toggle"><li><details open=""><summary>main.py</summary><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="1e0a551d-035a-8037-b122-f9a69202f2b9" class="code"><code class="language-Python" style="white-space:pre-wrap;word-break:break-all"># 导入 os 模块，用于与操作系统进行交互，例如操作文件路径、环境变量等
import os
# 导入 streamlit 库并别名为 st，Streamlit 用于快速创建数据应用程序和交互式界面
import streamlit as st
# 从 robot 模块中导入 Robot 类，Robot 类可能是用于处理聊天机器人相关的逻辑
from robot import Robot
# 从 chroma 模块中导入 MyChroma 类，MyChroma 类可能与向量数据库操作有关
from chroma import MyChroma

# 检查会话状态中是否存在名为&#x27;started&#x27;的键
# 如果不存在，说明这是会话的首次加载或初始化
if&#x27;started&#x27; not in st.session_state:
    # 在会话状态中设置&#x27;started&#x27;键的值为 True，表示会话已开始初始化
    st.session_state.started = True
    # 从指定文件夹（&#x27;./files/rag&#x27;）加载数据到名为&#x27;rag_collection&#x27;的集合中，
    # 创建 MyChroma 实例并存储在会话状态的&#x27;chroma&#x27;键下
    st.session_state.chroma = MyChroma.from_folder(&#x27;./files/rag&#x27;, &#x27;rag_collection&#x27;)
    # 使用指定的模型配置（&#x27;gpt-3.5-turbo&#x27;）和从会话状态中获取的检索器（来自 MyChroma）
    # 创建 Robot 实例，并存储在会话状态的&#x27;robot&#x27;键下
    st.session_state.robot = Robot({&#x27;model&#x27;: &#x27;gpt-3.5-turbo&#x27;}, st.session_state.chroma.as_retriever(k=3))
    # 在会话状态中设置&#x27;session_id&#x27;键的值为 1，用于标识当前会话的 ID
    st.session_state.session_id = 1

# 导入自定义的 funcs 模块并别名为 funcs，该模块可能包含了一些业务逻辑函数
import funcs as funcs

# 定义 init_interface 函数，用于初始化和展示应用程序的界面
def init_interface():
    # 设置 Streamlit 应用程序的页面配置，包括页面标题为&quot;Medical Chatbot&quot;和布局为宽屏
    st.set_page_config(page_title=&quot;Medical Chatbot&quot;, layout=&quot;wide&quot;)
    # 在应用程序页面上显示标题&quot;Molly 医疗精灵&quot;
    st.title(&quot;Molly 医疗精灵&quot;)
    # 调用 funcs 模块中的 get_session_messages 函数获取当前会话的聊天消息列表
    messages = funcs.get_session_messages()
    # 遍历获取到的聊天消息列表
    for message in messages:
        # 解包消息元组，分别获取消息的角色（如&quot;Human&quot;或&quot;AI&quot;）和内容
        role, content = message
        # 根据消息的角色，在 Streamlit 聊天界面中显示相应的聊天消息
        with st.chat_message(role):
            st.write(content)
    # 在应用程序中创建一个聊天输入框，提示用户输入问题，并将用户输入的内容赋值给 question 变量
    question = st.chat_input(&quot;输入问题提问....&quot;)
    # 如果用户输入了问题（question 不为 None）
    if question is not None:
        # 调用 funcs 模块中的 create_response 函数，根据用户问题生成回答
        response = funcs.create_response(question)
        # 在聊天界面中显示用户输入的问题，角色标识为&quot;Human&quot;
        st.chat_message(&quot;Human&quot;).write(question)
        # 在聊天界面中以流式输出的方式显示 AI 的回答，角色标识为&quot;AI&quot;
        st.chat_message(&quot;AI&quot;).write_stream(response)
    # 在应用程序的侧边栏中进行以下操作
    with st.sidebar:
        # 在侧边栏中显示当前对话的 ID，ID 从会话状态的&#x27;session_id&#x27;键获取
        st.header(f&quot;当前对话ID：{st.session_state.session_id}&quot;)
        # 在侧边栏中创建一个按钮，按钮文本为&quot;开始新对话&quot;，
        # 点击按钮时调用 funcs 模块中的 start_session 函数
        st.button(&quot;开始新对话&quot;, on_click=funcs.start_session)
        # 遍历 funcs 模块中获取的所有会话 ID 列表
        for session_id in funcs.get_all_session_ids():
            # 在侧边栏中创建一个可展开的区域，标题为当前会话 ID
            with st.expander(f&quot;对话ID：{session_id}&quot;):
                # 在展开区域中创建两列布局
                col1, col2 = st.columns(2)
                # 在第一列中创建一个按钮，按钮文本为&quot;继续对话&quot;，
                # 点击按钮时调用 funcs 模块中的 continue_session 函数，并传递当前会话 ID 作为参数
                col1.button(&quot;继续对话&quot;, on_click=funcs.continue_session, args=(session_id,), key=f&quot;Restart_{session_id}&quot;)
                # 在第二列中创建一个按钮，按钮文本为&quot;删除对话&quot;，
                # 点击按钮时调用 funcs 模块中的 delete_session 函数，并传递当前会话 ID 作为参数
                col2.button(&quot;删除对话&quot;, on_click=funcs.delete_session, args=(session_id,), key=f&quot;Delete_{session_id}&quot;)
                # 调用 funcs 模块中的 get_session_messages 函数获取指定会话 ID 的聊天消息列表
                messages = funcs.get_session_messages(session_id)
                # 遍历获取到的聊天消息列表
                for message in messages:
                    # 解包消息元组，分别获取消息的角色（如&quot;Human&quot;或&quot;AI&quot;）和内容
                    role, content = message
                    # 根据消息的角色，在侧边栏的聊天界面中显示相应的聊天消息
                    with st.chat_message(role):
                        st.write(content)

# 如果当前脚本是主程序入口
if __name__ == &#x27;__main__&#x27;:
    # 设置环境变量&quot;OPENAI_API_KEY&quot;的值，用于访问 OpenAI 的 API
    os.environ[&quot;OPENAI_API_KEY&quot;] = &#x27;hk-78s05w100005368902b62ce2a6216ecad6e94ed207fab3f7&#x27;
    os.environ[&quot;OPENAI_API_BASE&quot;] = &#x27;https://api.openai-hk.com/v1&#x27;
    # 调用 init_interface 函数，初始化并展示应用程序的界面
    init_interface()</code></pre></details></li></ul><ul id="1e0a551d-035a-8043-9e27-dac6ffb3a79c" class="toggle"><li><details open=""><summary>robot.py</summary><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="1e0a551d-035a-8063-90a8-dfeb28571e62" class="code"><code class="language-Python" style="white-space:pre-wrap;word-break:break-all">import os
# 导入 sqlite3 库，用于与 SQLite 数据库进行交互
import sqlite3
# 从 typing 模块导入 Iterator 类型，用于类型提示
from typing import Iterator
# 从 langchain.prompts.chat 模块导入 ChatPromptTemplate 类，用于创建聊天提示模板
from langchain.prompts.chat import ChatPromptTemplate
# 从 langchain_core.messages 模块导入 SystemMessage 和 AIMessage 类，分别表示系统消息和 AI 消息
from langchain_core.messages import SystemMessage, AIMessage
# 从 langchain_core.runnables 模块导入 RunnableWithMessageHistory、RunnableLambda 和 RunnablePassthrough 类，用于构建可运行的链
from langchain_core.runnables import RunnableWithMessageHistory, RunnableLambda, RunnablePassthrough
# 从 langchain_core.chat_history 模块导入 BaseChatMessageHistory 类，作为聊天历史记录的基类
from langchain_core.chat_history import BaseChatMessageHistory

# 从 langchain_openai 模块导入 ChatOpenAI 类，用于与 OpenAI 的聊天模型进行交互
from langchain_openai import ChatOpenAI
# 从 langchain_community.chat_message_histories.sql 模块导入 SQLChatMessageHistory 类，用于将聊天历史记录存储到 SQL 数据库中
from langchain_community.chat_message_histories.sql import SQLChatMessageHistory


class Robot:
    # 定义系统提示，告知 AI 如何回答用户的问题
    system_propmt = &quot;你是一个名叫Molly的医学专家，\
        对于用户提问的医学相关问题，你需要按照给出的参考文献资料对问题进行回答。\
        你的回答需要按照以下步骤：\
            1. 分析用户问题、对话历史以及参考文献，判断参考资料的哪些内容可以解答用户的问题，并将这一过程进行说明。\
            2. 如果参考文献可以解答用户的问题，则根据文献内容对问题进行解答。\
            3. 如果参考文献不能解答用户问题，告诉用户信息不足，无法回答，建议用户寻求专业人士帮助，不要自行发挥。\
        你的回答需要注意以下几点： \
            1. 保证你的回答是清晰的、明确的。如果你参考了参考资料，应该指出参考资料的标题等。\
            2. 结合用户的对话历史，分析用户的问题意图。但不要复述问题。\
            2. 回复用户时，使用对话的口吻，有礼貌地称呼用户为”您“，不要使用“用户”来称呼！\
            3. 如果用户的问题与医学无关，判断用户的目的，并温柔地提示其回到医学话题。\
        再次提醒：请严格遵守以上规则，当参考资料不足时，拒绝回答问题，不要自行发挥！&quot;
    # 定义问候语提示，作为 AI 与用户对话开始时的欢迎语
    greeting_prompt = &quot;你好！我是Molly医疗精灵，专注解决你的医疗问题。请问你需要什么帮助？&quot;
    # 定义提示模板，包含用户问题、参考资料和对话历史
    prompt_template = &quot;##用户问题：{input}\n\n ##参考资料：\n\n ##本地知识库：{rag_results}\n\n ##对话历史：{chat_history}&quot;

    def __init__(self, model_config, retriever=None):
        # 设置 OpenAI API 密钥，用于访问 OpenAI 的聊天模型
        os.environ[&quot;OPENAI_API_KEY&quot;] = &#x27;hk-78s05w100005368902b62ce2a6216ecad6e94ed207fab3f7&#x27;
        os.environ[&quot;OPENAI_API_BASE&quot;] = &#x27;https://api.openai-hk.com/v1&#x27;
        # 如果没有提供检索器，则创建一个默认的检索器，返回空的参考资料和对话历史
        if retriever is None:
            retriever = RunnableLambda(lambda input: {&quot;input&quot;: input, &quot;rag_results&quot;: &quot;&quot;, &quot;chat_history&quot;: &quot;&quot;})
        # 初始化 ChatOpenAI 模型，传入模型配置
        self.model = ChatOpenAI(**model_config)
        # 保存检索器
        self.retriever = retriever
        # 根据提示模板创建聊天提示
        self.prompt = ChatPromptTemplate.from_messages([(&quot;human&quot;, self.prompt_template)])
        # 创建一个带有消息历史记录处理功能的可运行链，将其赋值给 self.model_with_message_history 这个实例属性
        self.model_with_message_history = RunnableWithMessageHistory(
            # 这里使用链式操作符 | 将提示模板（self.prompt）和语言模型（self.model）连接起来。
            # 意味着先将输入数据经过提示模板处理，然后将处理后的结果作为语言模型的输入
            self.prompt | self.model,
            # get_session_history 是一个可调用对象（通常是函数），它用于获取当前会话的聊天历史记录。
            # self.get_session 是自定义的方法，用于返回当前会话对应的聊天历史记录列表
            get_session_history=self.get_session,
            # input_messages_key 指定了输入消息在输入字典中的键名。
            # 当调用这个可运行链时，需要传入一个字典，其中用户输入的消息应该以 &#x27;input&#x27; 作为键名
            input_messages_key=&#x27;input&#x27;,
            # history_messages_key 指定了聊天历史记录在输入字典中的键名。
            # 调用时，聊天历史记录应以 &#x27;chat_history&#x27; 作为键名存于输入字典中
            history_messages_key=&#x27;chat_history&#x27;
        )
        # 构建最终的链，将输入、检索器和聊天历史记录组合在一起
        self.chain = {&quot;input&quot;: RunnablePassthrough(), &quot;rag_results&quot;: self.retriever,
                      &quot;chat_history&quot;: RunnablePassthrough()} | self.model_with_message_history
        # 加载会话数据
        self.load_session_data()

    def get_session(self, session_id: int) -&gt; BaseChatMessageHistory:
        # 如果会话 ID 不在会话数据中，则创建一个新的会话记录
        if session_id not in self.session_data.keys():
            #session_id：用于指定要存储聊天记录的会话 ID。
            #connection_string：用于指定数据库的连接字符串。
            # 这里使用的是 SQLite 数据库，sqlite:///files///chat_history.db 表明数据库文件名为 chat_history.db，并且存放在 files 目录下
            self.session_data[session_id] = SQLChatMessageHistory(session_id,
                                                                  connection_string=&quot;sqlite:///files///chat_history.db&quot;)
            # 添加系统提示消息
            self.session_data[session_id].add_message(SystemMessage(self.system_propmt))
            # 添加问候语消息
            self.session_data[session_id].add_message(AIMessage(self.greeting_prompt))
        # 返回会话记录
        return self.session_data[session_id]

    def load_session_data(self):
        # 连接到 SQLite 数据库
        with sqlite3.connect(&quot;files/chat_history.db&quot;) as client:
            # 创建游标对象，用于执行 SQL 语句
            cur = client.cursor()
            # 执行查询语句，检查名为 message_store 的表是否存在
            cur.execute(
                &quot;select count(*)  from sqlite_master where type=&#x27;table&#x27; and name = &#x27;message_store&#x27;&quot;)
            # 如果表不存在
            if cur.fetchall()[0][0] == 0:
                # 初始化会话数据为空字典
                self.session_data = {}
            else:
                # 如果表存在，查询所有不同的会话 ID
                cur.execute(&quot;SELECT DISTINCT session_id FROM message_store&quot;)
                session_ids = cur.fetchall()
                # 创建一个字典 self.session_data，用于存储每个会话的聊天历史记录对象
                self.session_data = {
                    # 字典的键是会话 ID，这里将会话 ID 转换为整数类型
                    int(session_id[0]):
                    # 字典的值是 SQLChatMessageHistory 类的实例，用于管理会话的聊天历史记录
                    SQLChatMessageHistory(
                        # 第一个参数是会话 ID，同样转换为整数类型，用于标识特定的会话
                        int(session_id[0]),
                        # 第二个参数是数据库连接字符串，指定了聊天历史记录存储的数据库位置
                        # 这里使用 SQLite 数据库，数据库文件存储在 files 目录下的 chat_history.db 文件中
                        connection_string=&quot;sqlite:///files/chat_history.db&quot;
                    )
                    # 这是一个字典推导式，遍历 session_ids 列表中的每个会话 ID
                    for session_id in session_ids
                }

    def chat(self, input: str, session_id: int) -&gt; AIMessage:
        # 配置会话 ID
        config = {&#x27;configurable&#x27;: {&#x27;session_id&#x27;: session_id}}
        # 调用链处理输入，并获取响应
        response = self.chain.invoke(input, config=config)
        # 返回响应的内容
        return response.content

    #机器人的 stream 方法，传入问题和会话 ID，获取 AI 回复
    def stream(self, inputs, session_id) -&gt; Iterator:
        # 配置会话 ID
        config = {&#x27;configurable&#x27;: {&#x27;session_id&#x27;: session_id}}
        # 调用链以流式方式处理输入，并返回迭代器
        response = self.chain.stream(inputs, config=config)
        return response


if __name__ == &quot;__main__&quot;:
    # 初始化 Robot 类的实例，传入模型配置
    robot = Robot(model_config={&quot;model&quot;: &quot;gpt-3.5-turbo&quot;})
    # 定义用户问题
    question = &quot;你好，请问你是谁？&quot;
    # 打印问题
    print(&quot;问题：&quot;, question)
    # 调用 chat 方法获取回答
    response = robot.chat(question, 1)
    # 打印回答
    print(&quot;答复：&quot;, response)</code></pre></details></li></ul><h2 id="1e0a551d-035a-8036-9a53-f5da0ea6051f" class="">Day04  LangSmith+LangGraph+Llama_Index</h2><ul id="1dba551d-035a-806e-b3fa-fbbd8c7df20f" class="toggle"><li><details open=""><summary>langsmith</summary><ul id="1e0a551d-035a-80ca-8cc5-ed2dfacd124d" class="toggle"><li><details open=""><summary>langsmith_quickstart.py</summary><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="1e0a551d-035a-809b-aaad-cc8c70160fa0" class="code"><code class="language-Python" style="white-space:pre-wrap;word-break:break-all">import os
# langsmith 环境变量
os.environ[&#x27;LANGCHAIN_TRACING_V2&#x27;]=&quot;true&quot;
os.environ[&#x27;LANGCHAIN_ENDPOINT&#x27;]=&quot;https://api.smith.langchain.com&quot;
os.environ[&#x27;LANGCHAIN_API_KEY&#x27;]=&quot;你的key&quot;
os.environ[&#x27;LANGCHAIN_PROJECT&#x27;]=&quot;langsmith_learn&quot; #项目名
# openai key
os.environ[&quot;OPENAI_API_KEY&quot;]=&#x27;你的key&#x27;
os.environ[&quot;OPENAI_API_BASE&quot;]=&#x27;https://api.openai-hk.com/v1&#x27;
os.environ[&quot;OPENAI_BASE_URL&quot;] = &quot;https://api.openai-hk.com/v1&quot;

import openai
from langsmith.wrappers import wrap_openai
from langsmith import traceable

# 自动跟踪模型的调用情况
client = wrap_openai(openai.Client())

@traceable # 自动跟踪模型的调用情况,装饰器
def pipeline(user_input: str):
    result = client.chat.completions.create(
        messages=[{&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: user_input}],
        model=&quot;gpt-3.5-turbo&quot;
    )
    return result.choices[0].message.content

pipeline(&quot;你好&quot;)</code></pre><p id="1e0a551d-035a-8031-a8ac-fe05e3ebea28" class="">
</p></details></li></ul><ul id="1e0a551d-035a-8016-9782-e84cb633c4b1" class="toggle"><li><details open=""><summary>langsmith_with_langchain.py</summary><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="1e0a551d-035a-807f-b7f8-cde0150e3b79" class="code"><code class="language-Python" style="white-space:pre-wrap;word-break:break-all">import os
# langsmith 环境变量
os.environ[&#x27;LANGCHAIN_TRACING_V2&#x27;]=&quot;true&quot;
os.environ[&#x27;LANGCHAIN_ENDPOINT&#x27;]=&quot;https://api.smith.langchain.com&quot;
os.environ[&#x27;LANGCHAIN_API_KEY&#x27;]=&quot;你的key&quot;
os.environ[&#x27;LANGCHAIN_PROJECT&#x27;]=&quot;langsmith_learn2&quot;
# openai key
os.environ[&quot;OPENAI_API_KEY&quot;]=&#x27;你的key&#x27;
os.environ[&quot;OPENAI_API_BASE&quot;]=&#x27;https://api.openai-hk.com/v1&#x27;
os.environ[&quot;OPENAI_BASE_URL&quot;] = &quot;https://api.openai-hk.com/v1&quot;

from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser

#提示词模版
prompt = ChatPromptTemplate.from_messages([
    #帮助机器人
    (&quot;system&quot;, &quot;You are a helpful assistant. Please respond to the user&#x27;s request only based on the given context.&quot;),
    (&quot;user&quot;, &quot;Question: {question}\nContext: {context}&quot;)
])
#模型
model = ChatOpenAI(model=&quot;gpt-3.5-turbo&quot;)
#解析输出
output_parser = StrOutputParser()

#链的组件
chain = prompt | model | output_parser

#问题  总结一下早上会议内容
question = &quot;Can you summarize this morning&#x27;s meetings?&quot;
#早上的会议内容，解决世界冲突
context = &quot;During this morning&#x27;s meeting, we solved all world conflict.&quot;
#放入模型提问
chain.invoke({&quot;question&quot;: question, &quot;context&quot;: context})</code></pre></details></li></ul></details></li></ul><ul id="1e0a551d-035a-8086-b1cc-f764df41675a" class="toggle"><li><details open=""><summary>LangGraph</summary><ul id="1e0a551d-035a-807d-9f73-f0bf128f0fec" class="toggle"><li><details open=""><summary>custom_tools.py</summary><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="1e0a551d-035a-8068-b5fc-dbb12d8fc146" class="code"><code class="language-Python" style="white-space:pre-wrap;word-break:break-all">from langchain_core.tools import tool

@tool
def multiply(a: int, b: int) -&gt; int:
    &quot;&quot;&quot;两数相乘&quot;&quot;&quot;
    return a * b

tools = [multiply]


# 1.导包
from langchain_openai import ChatOpenAI
from langchain_core.messages import HumanMessage,SystemMessage
import os

# langsmith 环境变量
os.environ[&#x27;LANGCHAIN_TRACING_V2&#x27;]=&quot;true&quot;
os.environ[&#x27;LANGCHAIN_ENDPOINT&#x27;]=&quot;https://api.smith.langchain.com&quot;
os.environ[&#x27;LANGCHAIN_API_KEY&#x27;]=&quot;你的key&quot;
os.environ[&#x27;LANGCHAIN_PROJECT&#x27;]=&quot;langsmith_learn3&quot;
# openai key
os.environ[&quot;OPENAI_API_KEY&quot;]=&#x27;你的key&#x27;
os.environ[&quot;OPENAI_API_BASE&quot;]=&#x27;https://api.openai-hk.com/v1&#x27;
os.environ[&quot;OPENAI_BASE_URL&quot;] = &quot;https://api.openai-hk.com/v1&quot;

# 2.创建模型
model = ChatOpenAI(model=&quot;gpt-3.5-turbo&quot;)

# 绑定Tools
model_bind_tools = model.bind_tools(tools)

# 3.对话
input_text = input(&quot;请输入两数相乘的问题:&quot;)
messages = [
    HumanMessage(content=input_text)
]

# 4.启动模型
response = model_bind_tools.invoke(messages)

# print(response)
for tool_call in response.tool_calls:

    output = multiply.invoke(tool_call[&#x27;args&#x27;])

    print(output)

# 使用工具计算的结果
result_with_tool = model.invoke(f&quot;{input_text}的计算结果是:{output}&quot;)
print(f&quot;使用工具计算的结果是:{result_with_tool.content}&quot;)

# 不使用工具计算的结果
result_without_tool = model.invoke(input_text)
print(f&quot;不使用工具计算的结果:{result_without_tool.content}&quot;)</code></pre></details></li></ul><ul id="1e0a551d-035a-80dc-8f46-c3757b7ae8ce" class="toggle"><li><details open=""><summary>langgraph_with_tools.py</summary><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="1e0a551d-035a-8048-80b3-c385d7e910b7" class="code"><code class="language-Python" style="white-space:pre-wrap;word-break:break-all">import os

# langsmith 环境变量
os.environ[&#x27;LANGCHAIN_TRACING_V2&#x27;]=&quot;true&quot;
os.environ[&#x27;LANGCHAIN_ENDPOINT&#x27;]=&quot;https://api.smith.langchain.com&quot;
os.environ[&#x27;LANGCHAIN_API_KEY&#x27;]=&quot;你的key&quot;
os.environ[&#x27;LANGCHAIN_PROJECT&#x27;]=&quot;langsmith_learn5&quot;
# openai key
os.environ[&quot;OPENAI_API_KEY&quot;]=&#x27;你的key&#x27;
os.environ[&quot;OPENAI_API_BASE&quot;]=&#x27;https://api.openai-hk.com/v1&#x27;
os.environ[&quot;OPENAI_BASE_URL&quot;] = &quot;https://api.openai-hk.com/v1&quot;

# 设置 Tavily 的 API 密钥，Tavily 是一个搜索工具，用于获取外部信息
os.environ[&quot;TAVILY_API_KEY&quot;] = &quot;你的key&quot;

# 从 langchain_community 库中导入 TavilySearchResults 工具类
from langchain_community.tools.tavily_search import TavilySearchResults

# 创建一个 TavilySearchResults 工具实例，设置最大返回结果数量为 2
tool = TavilySearchResults(max_results=2)
# 将工具实例存储在列表中，方便后续使用
tools = [tool]

# 导入相关类型注解模块，用于增强代码的可读性和类型检查
from typing import Annotated
from typing_extensions import TypedDict
# 从 langgraph 库中导入状态图相关类和常量
from langgraph.graph import StateGraph, START, END
# 从 langgraph 库中导入消息处理相关函数
from langgraph.graph.message import add_messages
# 从 langgraph 预构建模块中导入工具节点和工具条件函数
from langgraph.prebuilt import ToolNode, tools_condition

# 定义一个名为 State 的类型字典，用于表示状态图中的状态
# 其中 messages 字段是一个列表，使用 add_messages 进行注解，用于处理消息
class State(TypedDict):
    messages: Annotated[list, add_messages]

# 创建一个 StateGraph 实例，用于构建状态图，传入 State 类型作为状态定义
graph_builder = StateGraph(State)

# 从 langchain_openai 库中导入 ChatOpenAI 类，用于与 OpenAI 的聊天模型进行交互
from langchain_openai import ChatOpenAI

# 创建一个 ChatOpenAI 实例，指定使用 gpt-3.5-turbo 模型
llm = ChatOpenAI(model=&quot;gpt-3.5-turbo&quot;)
# 将之前创建的工具列表绑定到语言模型上，使模型可以调用工具
llm_with_tools = llm.bind_tools(tools)

# 定义一个名为 chatbot 的函数，用于处理聊天逻辑
# 接收一个 State 类型的参数 state，返回一个包含处理后消息的字典
def chatbot(state: State):
    return {&quot;messages&quot;: [llm_with_tools.invoke(state[&quot;messages&quot;])]}

# 在状态图构建器中添加一个名为 chatbot 的节点，使用 chatbot 函数作为节点的处理逻辑
graph_builder.add_node(&quot;chatbot&quot;, chatbot)
# 创建一个 ToolNode 实例，传入工具列表，用于处理工具调用逻辑
tool_node = ToolNode(tools=tools)
# 在状态图构建器中添加一个名为 tools 的节点，使用 tool_node 作为节点的处理逻辑
graph_builder.add_node(&quot;tools&quot;, tool_node)

# 在状态图中添加从 chatbot 节点到 tools 节点的条件边，根据工具条件进行跳转
graph_builder.add_conditional_edges(&quot;chatbot&quot;, tools_condition)
# 在状态图中添加从 tools 节点到 chatbot 节点的边，形成循环调用逻辑
graph_builder.add_edge(&quot;tools&quot;, &quot;chatbot&quot;)
# 设置状态图的入口点为 chatbot 节点
graph_builder.set_entry_point(&quot;chatbot&quot;)

# 编译状态图构建器，生成可执行的状态图
graph = graph_builder.compile()

# 定义一个名为 stream_graph_updates 的函数，用于流式处理状态图的更新
# 接收用户输入作为参数，将用户输入封装成消息列表传入状态图进行处理
# 遍历状态图的流事件，打印出助手的回复内容
def stream_graph_updates(user_input: str):
    for event in graph.stream({&quot;messages&quot;: [(&quot;user&quot;, user_input)]}):
        for value in event.values():
            print(&quot;Assistant:&quot;, value[&quot;messages&quot;][-1].content)

# 进入一个无限循环，用于持续接收用户输入并处理
while True:
    try:
        # 提示用户输入信息，并读取用户输入
        user_input = input(&quot;User: &quot;)
        # 检查用户输入是否为退出指令，如果是则打印再见信息并退出循环
        if user_input.lower() in [&quot;quit&quot;, &quot;exit&quot;, &quot;q&quot;]:
            print(&quot;Goodbye!&quot;)
            break

        # 调用 stream_graph_updates 函数处理用户输入
        stream_graph_updates(user_input)
    except:
        # 如果输入过程中出现异常（如无法获取输入），使用默认输入进行处理
        user_input = &quot;What do you know about LangGraph?&quot;
        print(&quot;User: &quot; + user_input)
        # 调用 stream_graph_updates 函数处理默认输入
        stream_graph_updates(user_input)
        # 出现异常后退出循环
        break</code></pre></details></li></ul><ul id="1e0a551d-035a-8046-b8d5-e3f015771092" class="toggle"><li><details open=""><summary>langgraph_quickstart.py</summary><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="1e0a551d-035a-80e7-a928-d76c1de01107" class="code"><code class="language-Python" style="white-space:pre-wrap;word-break:break-all">import os
# langsmith 环境变量
os.environ[&#x27;LANGCHAIN_TRACING_V2&#x27;]=&quot;true&quot;
os.environ[&#x27;LANGCHAIN_ENDPOINT&#x27;]=&quot;https://api.smith.langchain.com&quot;
os.environ[&#x27;LANGCHAIN_API_KEY&#x27;]=&quot;你的key&quot;
os.environ[&#x27;LANGCHAIN_PROJECT&#x27;]=&quot;langsmith_learn4&quot;
# openai key
os.environ[&quot;OPENAI_API_KEY&quot;]=&#x27;你的key&#x27;
os.environ[&quot;OPENAI_API_BASE&quot;]=&#x27;https://api.openai-hk.com/v1&#x27;
os.environ[&quot;OPENAI_BASE_URL&quot;] = &quot;https://api.openai-hk.com/v1&quot;

#导入数据类型 2个
from typing import Annotated
from typing_extensions import TypedDict

from langgraph.graph import StateGraph, START, END
from langgraph.graph.message import add_messages

#参数：字典类型dict
class State(TypedDict):
    #把消息messages 通过add_messages方法放在list里面
    messages: Annotated[list, add_messages]

#构造了一个图
graph_builder = StateGraph(State)

from langchain_openai import ChatOpenAI

llm = ChatOpenAI(model=&quot;gpt-3.5-turbo&quot;)

def chatbot(state: State):
    #key：messages，value：模型返回结果
    return {&quot;messages&quot;: [llm.invoke(state[&quot;messages&quot;])]}

#图里面添加了一个节点（任务或叫操作）
graph_builder.add_node(&quot;chatbot&quot;, chatbot)

#图里面添加了一个起始边
graph_builder.add_edge(START, &quot;chatbot&quot;)
#图里面添加了一个结束边
graph_builder.add_edge(&quot;chatbot&quot;, END)

graph = graph_builder.compile()

def stream_graph_updates(user_input: str):
    #一次性提问
    for event in graph.stream({&quot;messages&quot;: [(&quot;user&quot;, user_input)]}):
        for value in event.values():
            print(&quot;Assistant:&quot;, value[&quot;messages&quot;][-1].content)


while True:
    try:
        user_input = input(&quot;User: &quot;)
        if user_input.lower() in [&quot;quit&quot;, &quot;exit&quot;, &quot;q&quot;]:
            print(&quot;Goodbye!&quot;)
            break

        stream_graph_updates(user_input)
    except:
        user_input = &quot;What do you know about LangGraph?&quot;
        print(&quot;User: &quot; + user_input)
        stream_graph_updates(user_input)
        break</code></pre></details></li></ul></details></li></ul><ul id="1e0a551d-035a-801d-8d56-e3460419f910" class="toggle"><li><details open=""><summary>Llama_Index</summary><ul id="1e0a551d-035a-80f4-ba15-f8ff6eb9768f" class="toggle"><li><details open=""><summary>llamaindex_load_directory.py</summary><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="1e0a551d-035a-80b4-bbf6-f70c7204b8b1" class="code"><code class="language-Python" style="white-space:pre-wrap;word-break:break-all"># 导入 os 模块，用于与操作系统进行交互，比如设置环境变量
import os
# 从 llama_index.core 模块导入 load_index_from_storage 和 StorageContext 类
# load_index_from_storage 用于从存储中加载索引，StorageContext 用于管理索引的存储上下文
from llama_index.core import load_index_from_storage, StorageContext

# 设置 OpenAI 的 API 密钥，这里的密钥是一个示例值
os.environ[&quot;OPENAI_API_KEY&quot;] = &#x27;你的key&#x27;
# 设置 OpenAI 的 API 基础地址，这里指定了自定义的 API 地址
os.environ[&quot;OPENAI_API_BASE&quot;] = &#x27;https://api.openai-hk.com/v1&#x27;

# 使用 StorageContext 类的 from_defaults 方法创建一个存储上下文对象
# persist_dir 参数指定了索引数据持久化存储的目录
storage_context = StorageContext.from_defaults(persist_dir=&quot;index_persist&quot;)
# 调用 load_index_from_storage 函数，根据之前创建的存储上下文对象加载索引
index = load_index_from_storage(storage_context)

# 调用索引对象的 as_query_engine 方法，将索引转换为查询引擎对象
# 查询引擎用于向索引中发起查询请求
query_engine = index.as_query_engine()
# 调用查询引擎对象的 query 方法，发起一个关于“脑卒中怎么治疗?”的查询
# 并将查询结果存储在 response 变量中
response = query_engine.query(&quot;脑卒中怎么治疗?&quot;)

# 打印查询结果
print(response)</code></pre></details></li></ul><ul id="1e0a551d-035a-8027-a4a7-fdc2a4bb8a40" class="toggle"><li><details open=""><summary>llamaindex_quickstart.py</summary><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="1e0a551d-035a-8012-b400-d38ac9f8d490" class="code"><code class="language-Python" style="white-space:pre-wrap;word-break:break-all"># 导入 os 模块，该模块提供了与操作系统进行交互的功能，比如设置环境变量
import os
# 从 llama_index.core 模块导入 SimpleDirectoryReader 和 VectorStoreIndex 类
# SimpleDirectoryReader 用于从指定目录读取文档数据
# VectorStoreIndex 用于根据文档数据创建向量存储索引
from llama_index.core import SimpleDirectoryReader, VectorStoreIndex

# 设置 OpenAI 的 API 密钥，这是与 OpenAI 服务进行交互时的身份验证凭证
# 注意：这里的密钥可能是示例，实际使用时需要替换为有效的密钥
os.environ[&quot;OPENAI_API_KEY&quot;] = &#x27;你的key&#x27;
# 设置 OpenAI 的 API 基础地址，指定了与 OpenAI 服务进行通信的服务器地址
os.environ[&quot;OPENAI_API_BASE&quot;] = &#x27;https://api.openai-hk.com/v1&#x27;

# 使用 SimpleDirectoryReader 类创建一个文档读取器对象，指定从 &quot;files&quot; 目录读取文档数据
# 调用 load_data 方法将该目录下的文档数据加载到内存中，并存储在 documents 变量中
documents = SimpleDirectoryReader(&quot;files&quot;).load_data()
# 利用 VectorStoreIndex 类的 from_documents 方法，根据加载的文档数据创建一个向量存储索引
# 这个索引可以用于后续的查询操作
index = VectorStoreIndex.from_documents(documents)

# 持久化index到目录中
# 调用索引对象的 storage_context 属性的 persist 方法，将索引数据持久化到 &quot;index_persist&quot; 目录中
# 这样在后续可以从该目录重新加载索引，避免重复创建索引的开销
#index.storage_context.persist(&quot;index_persist&quot;)

# 调用索引对象的 as_query_engine 方法，将索引转换为查询引擎对象
# 查询引擎可以接受用户的查询请求，并从索引中查找相关信息
query_engine = index.as_query_engine()
# 调用查询引擎对象的 query 方法，发起一个关于“脑卒中是什么”的查询
# 并将查询结果存储在 response 变量中
response = query_engine.query(&quot;脑卒中是什么？&quot;)

# 打印查询结果，将关于“脑卒中是什么”的查询结果输出到控制台
print(response)</code></pre></details></li></ul></details></li></ul><h1 id="1e0a551d-035a-80ff-91e4-ecf875c11f22" class="">WK6  项目答辩周</h1><h2 id="1e0a551d-035a-80fd-b2b4-ca3e108285e8" class="">Day02  matplot按自然语言要求生成可视化图表</h2><ul id="1e0a551d-035a-800a-8934-db969ec79d71" class="toggle"><li><details open=""><summary>matplot+pandas+langchain</summary><ul id="1e0a551d-035a-80ff-be43-e32fb10dba89" class="toggle"><li><details open=""><summary>待读取csv文件</summary><figure id="1e0a551d-035a-80a0-acd7-dd3aef2c5dbb"><div class="source"><a href="sales_data.csv">sales_data.csv</a></div></figure></details></li></ul><ul id="1e0a551d-035a-805d-a7d0-f3e225831ac5" class="toggle"><li><details open=""><summary>automated_data_analysis.py</summary><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="1e0a551d-035a-806a-ae20-ebce277e68c3" class="code"><code class="language-Python" style="white-space:pre-wrap;word-break:break-all"># 导入 contextlib 模块，用于上下文管理，这里主要用于重定向标准输出
import contextlib
# 导入 os.path 模块，用于处理文件路径相关操作
import os.path
# 导入 re 模块，用于正则表达式操作，可进行字符串的匹配和替换
import re
# 导入 sys 模块，提供对 Python 解释器使用或维护的一些变量的访问，以及与解释器进行交互的函数
import sys

# 导入 gradio 库，用于快速创建交互式的 Web 界面
import gradio as gr
# 导入 matplotlib.pyplot 模块，用于绘制图表
import matplotlib.pyplot as plt
# 导入 pandas 库，用于数据处理和分析
import pandas as pd
# 从 langchain_experimental.agents 模块导入 create_pandas_dataframe_agent 函数，用于创建基于 Pandas 数据框的智能代理
from langchain_experimental.agents import create_pandas_dataframe_agent
# 从 langchain_openai 模块导入 ChatOpenAI 类，用于与 OpenAI 模型进行交互
from langchain_openai import ChatOpenAI

# 设置 OpenAI API 密钥，用于访问 OpenAI 服务
os.environ[&quot;OPENAI_API_KEY&quot;] = &#x27;sk-19a6e044d00c4962b62434ec87302a87&#x27;
# 设置 OpenAI API 的基础地址，指定使用的 API 服务地址
os.environ[&#x27;OPENAI_API_BASE&#x27;] = &#x27;https://api.deepseek.com/v1&#x27;

# 设置 matplotlib 的中文字体，确保中文能正常显示
plt.rcParams[&#x27;font.sans-serif&#x27;] = [&#x27;Arial Unicode MS&#x27;, &#x27;PingFang SC&#x27;, &#x27;STHeiti&#x27;, &#x27;SimHei&#x27;, &#x27;Microsoft YaHei&#x27;]
# 解决 matplotlib 中负号显示问题
plt.rcParams[&#x27;axes.unicode_minus&#x27;] = False


# 定义 CodeCapture 类，用于捕获代码和输出信息
class CodeCapture:
    # 类的初始化方法，创建一个空的代码列表和输出列表
    def __init__(self):
        self.code = []
        self.output = []

    # 清理 ANSI 颜色代码和其他特殊字符的方法
    def clean_ansi_codes(self, text):
        # 定义一个正则表达式模式，用于匹配 ANSI 转义序列
        ansi_escape = re.compile(r&#x27;\x1B(?:[@-Z\\-_]|\[[0-?]*[ -/]*[@-~])&#x27;)
        # 使用正则表达式替换方法，将匹配到的 ANSI 转义序列替换为空字符串
        return ansi_escape.sub(&#x27;&#x27;, text)

    # 清理代码中的特殊字符和格式的方法
    def clean_code(self, code):
        # 调用 clean_ansi_codes 方法清理代码中的 ANSI 颜色代码
        code = self.clean_ansi_codes(code)
        # 去除代码前后的空白字符
        code = code.strip()
        # 如果代码以 ```python 开头，去掉这部分内容
        if code.startswith(&quot;```python&quot;):
            code = code[9:].strip()
        # 如果代码以 ``` 开头，去掉这部分内容
        if code.startswith(&quot;```&quot;):
            code = code[3:].strip()
        # 如果代码以 ``` 结尾，去掉这部分内容
        if code.endswith(&quot;```&quot;):
            code = code[:-3].strip()
        return code

    # 重写 write 方法，用于捕获所有输出信息
    def write(self, text):
        # 将输出信息添加到输出列表中
        self.output.append(text)
        # 检查输出中是否包含 &quot;Action:&quot; 和 &quot;Action Input:&quot;
        if &quot;Action:&quot; in text and &quot;Action Input:&quot; in text:
            # 使用正则表达式搜索 &quot;Action Input:&quot; 后面的代码内容
            code_match = re.search(r&quot;Action Input:(.*?)(?=\nObservation:|$)&quot;, text, re.DOTALL)
            if code_match:
                # 获取匹配到的代码内容并去除前后空白字符
                code = code_match.group(1).strip()
                # 调用 clean_code 方法清理代码
                code = self.clean_code(code)
                # 如果代码不为空，则添加到代码列表中
                if code:
                    self.code.append(code)

    # 获取捕获的代码，将代码列表中的元素用换行符连接成一个字符串
    def get_code(self):
        return &#x27;\n&#x27;.join(self.code)

    # 获取捕获的输出，将输出列表中的元素连接成一个字符串
    def get_output(self):
        return &#x27;&#x27;.join(self.output)


# 定义 DataAnalysisApp 类，用于实现数据分析应用的主要功能
class DataAnalysisApp:
    # 类的初始化方法，初始化数据框、智能代理和代码捕获对象
    def __init__(self):
        self.df = None
        self.agent = None
        self.code_capture = None

    # 创建智能代理的方法
    def create_agent(self):
        # 检查数据框是否已初始化
        if self.df is not None:
            # 使用 create_pandas_dataframe_agent 函数创建智能代理
            self.agent = create_pandas_dataframe_agent(
                # 创建 ChatOpenAI 实例，设置温度为 0，使用 deepseek-chat 模型，并设置系统消息
                ChatOpenAI(
                    temperature=0,
                    model=&#x27;deepseek-chat&#x27;,
                    messages=[{&quot;role&quot;: &quot;system&quot;, &quot;content&quot;: &quot;你是一个专业的数据分析师, 请使用中文回答所有问题&quot;}]
                ),
                # 传入数据框
                self.df,
                # 指定代理类型为零样本反应描述
                agent_type=&quot;zero-shot-react-description&quot;,
                # 开启详细输出模式
                verbose=True,
                # 允许执行危险代码
                allow_dangerous_code=True
            )

    # 处理上传文件的方法
    def process_file(self, file):
        # 检查文件是否为空
        if file is None:
            return &quot;请上传文件&quot;, None
        # 获取文件的扩展名
        file_ext = os.path.splitext(file.name)[1].lower()
        try:
            # 如果文件扩展名是 .csv
            if file_ext == &#x27;.csv&#x27;:
                # 使用 pandas 的 read_csv 函数读取 CSV 文件
                self.df = pd.read_csv(file.name)
            # 如果文件扩展名是 .xlsx 或 .xls
            elif file_ext in [&#x27;.xlsx&#x27;, &#x27;.xls&#x27;]:
                # 使用 pandas 的 read_excel 函数读取 Excel 文件
                self.df = pd.read_excel(file.name)
            else:
                return &#x27;不支持的文件格式, 请上传 CSV 或 EXCEL 文件&#x27;, None

            # 调用 create_agent 方法创建智能代理
            self.create_agent()
            # 返回成功信息和数据的前几行预览
            return f&quot;文件已成功加载。数据形状: {self.df.shape}&quot;, self.df.head().to_html()
        except Exception as e:
            # 如果处理文件时出现异常，返回错误信息
            return f&#x27;处理文件时出错: {str(e)}&#x27;, None

    # 分析数据的方法
    def analyze_data(self, question):
        # 检查数据框是否未初始化
        if self.df is None:
            return &#x27;请先上传数据文件&#x27;, None, &quot;&quot;

        # 检查智能代理是否未初始化
        if self.agent is None:
            return &#x27;Agent 未初始化&#x27;, None, &quot;&quot;

        try:
            # 创建一个新的 CodeCapture 实例
            self.code_capture = CodeCapture()

            # 使用 contextlib.redirect_stdout 重定向标准输出到 CodeCapture 实例
            with contextlib.redirect_stdout(self.code_capture):
                # 调用智能代理的 invoke 方法执行分析，并传入问题
                result = self.agent.invoke({&quot;input&quot;: question})

            # 获取执行的代码
            executed_code = self.code_capture.get_code()

            # 打印捕获的输出信息到实际的终端
            sys.__stdout__.write(f&quot;Captured Output: {self.code_capture.get_output()}\n&quot;)
            # 打印执行的代码信息到实际的终端
            sys.__stdout__.write(f&quot;Executed Code: {executed_code}\n&quot;)
            # 打印分析结果信息到实际的终端
            sys.__stdout__.write(f&quot;Result: {result}\n&quot;)

            # 检查是否生成了新的图表
            if plt.get_fignums():
                # 将图表保存为临时文件
                plt.savefig(&#x27;tmp_plot.png&#x27;)
                # 关闭当前图表
                plt.close()
                # 返回分析结果、图表文件路径和执行的代码
                return result[&#x27;output&#x27;], &#x27;tmp_plot.png&#x27;, executed_code

            # 返回分析结果、无图表和执行的代码
            return result[&#x27;output&#x27;], None, executed_code
        except Exception as e:
            # 如果分析过程中出现异常，返回错误信息
            return f&#x27;分析过程中出错: {str(e)}&#x27;, None, &quot;&quot;


# 创建 DataAnalysisApp 类的实例
app = DataAnalysisApp()

# 使用 gradio 的 Blocks 类创建一个界面块
with gr.Blocks() as interface:
    # 创建一个行布局
    with gr.Row():
        # 在界面上显示标题
        gr.Markdown(&#x27;# 智能数据分析助手&#x27;)

    # 创建一个行布局
    with gr.Row():
        # 创建一个列布局
        with gr.Column():
            # 创建一个文件输入组件，用于上传数据文件
            file_input = gr.File(label=&#x27;上传数据文件(CSV或EXCEL)&#x27;)
            # 创建一个文本框组件，用于显示文件上传状态
            upload_output = gr.Textbox(label=&#x27;上传状态&#x27;)

        # 创建一个列布局
        with gr.Column():
            # 创建一个 HTML 组件，用于显示数据的预览信息
            data_preview = gr.HTML(label=&#x27;数据预览&#x27;)

    # 为文件输入组件的上传事件绑定处理函数
    file_input.upload(
        # 绑定的处理函数为 app.process_file
        app.process_file,
        # 输入参数为文件输入组件
        inputs=[file_input],
        # 输出参数为上传状态文本框和数据预览 HTML 组件
        outputs=[upload_output, data_preview]
    )

    # 创建一个行布局
    with gr.Row():
        # 创建一个文本框组件，用于输入分析问题
        question_input = gr.Textbox(label=&#x27;输入您的分析问题&#x27;, placeholder=&#x27;例如: 计算每列的基本统计信息&#x27;)

    # 创建一个行布局
    with gr.Row():
        # 创建一个列布局
        with gr.Column():
            # 创建一个文本框组件，用于显示分析结果
            analysis_output = gr.Textbox(label=&#x27;分析结果&#x27;)
            # 创建一个代码显示组件，用于显示执行的代码
            code_output = gr.Code(label=&#x27;执行的代码&#x27;, language=&#x27;python&#x27;, interactive=False)
        # 创建一个列布局
        with gr.Column():
            # 创建一个图像组件，用于显示可视化结果
            plot_output = gr.Image(label=&#x27;可视化结果&#x27;)

    # 为问题输入组件的提交事件绑定处理函数
    question_input.submit(
        # 绑定的处理函数为 app.analyze_data
        app.analyze_data,
        # 输入参数为问题输入组件
        inputs=[question_input],
        # 输出参数为分析结果文本框、可视化结果图像组件和执行的代码显示组件
        outputs=[analysis_output, plot_output, code_output]
    )
# if __name__ == &#x27;__main__&#x27;:
#     app = DataAnalysisApp()
#     path = f&#x27;C:\Users\xupengcheng\PycharmProjects\pyDataAnalysis\demo\sales_data.csv&#x27;
#     with open(path,&#x27;r+&#x27;) as file:
#         app.process_file(file)
#

# 当脚本作为主程序运行时
if __name__ == &#x27;__main__&#x27;:
    # 启动界面
    interface.launch()</code></pre></details></li></ul></details></li></ul><h2 id="1e0a551d-035a-80e4-8f92-fe409470a804" class="">Day05  最终答辩项目</h2><ul id="1e0a551d-035a-8074-92e1-ec16c98002d5" class="toggle"><li><details open=""><summary>提效项目压缩包</summary><figure id="1e0a551d-035a-80a7-b0bc-c939062fd644"><div class="source"><a href="%E6%8F%90%E6%95%88%E9%A1%B9%E7%9B%AE-%E4%B8%81%E5%AD%90%E5%81%A5.zip">提效项目-丁子健.zip</a></div></figure></details></li></ul><h1 id="1e0a551d-035a-80ed-86e6-dec83d320578" class="">WK7  langchain_graph</h1><h2 id="1e3a551d-035a-8089-8b35-c68f6c912165" class="">Day01  langchain_graph</h2><ul id="1e3a551d-035a-80ad-abae-d7b648eb01ca" class="toggle"><li><details open=""><summary>test01.py</summary><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="1e3a551d-035a-80d0-855f-e077f1995bba" class="code"><code class="language-Python" style="white-space:pre-wrap;word-break:break-all">#聊天机器人
from typing import Annotated
from langchain_deepseek import ChatDeepSeek
#一定类型化字典
from typing_extensions import TypedDict
#构建状态图
from langgraph.graph import StateGraph
#处理消息添加
from langgraph.graph.message import add_messages
import os

os.environ[&#x27;DEEPSEEK_API_KEY&#x27;] = &#x27;你的key&#x27;

#类型化字典，存储状态
class State(TypedDict):
    #messages 列表， 使用add_messages进行注解
    messages:Annotated[list,add_messages]

#创建实例
graph_builder =StateGraph(State)

llm =ChatDeepSeek(model=&quot;deepseek-chat&quot;)

#接收一个参数State类型
#作用：调用Deepseek模型处理输入的消息，返回处理结果的字典
def chatbot(state:State):
    return {&quot;messages&quot;:[llm.invoke(state[&quot;messages&quot;])]}

#图添加节点,该节点对应chatbot函数
graph_builder.add_node(&quot;chatbot&quot;,chatbot)
#设置状态图的入口点
graph_builder.set_entry_point(&quot;chatbot&quot;)
#设置状态图的结束点
graph_builder.set_finish_point(&quot;chatbot&quot;)
#生成最终状态图
graph =graph_builder.compile()

#显示图像
from IPython.display import Image,display

try:
    #获取状态图的图形表示，转换为PNG图像
    img =Image(graph.get_graph().draw_mermaid_png())
    #将图像保存为base_image.png的文件
    with open(&quot;base_image.png&quot;,&quot;wb&quot;) as f:
        f.write(img.data)
except Exception:
    pass

#处理用户输入并流式输出聊天机器人
def stream_graph_updates(user_input:str):
    #遍历状态图流式处理用户输入，产生的事件
    for event in graph.stream({&quot;messages&quot;:[{&quot;role&quot;:&quot;user&quot;,&quot;content&quot;:user_input}]}):
        #遍历事件中的值
        for value in event.values():
            #打印聊天机器人的回复
            print(&quot;Assistant&quot;,value[&quot;messages&quot;][-1].content)

#循环，接收用户输入，并进行交互
while True:
    try:
        user_input =input(&quot;User:&quot;)
        if user_input.lower() in [&quot;quit&quot;,&quot;exit&quot;,&quot;q&quot;]:
            print(&quot;bye&quot;)
            break
        stream_graph_updates(user_input)
    except:
        user_input =&quot;你对 LangGraph 了解多少?&quot;
        print(&quot;User:&quot;+ user_input)
        stream_graph_updates(user_input)
        break









</code></pre></details></li></ul><ul id="1e3a551d-035a-8007-a068-d53ec8fb305f" class="toggle"><li><details open=""><summary>test02.py</summary><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="1e3a551d-035a-8053-a01b-fd6271f4b1bd" class="code"><code class="language-Python" style="white-space:pre-wrap;word-break:break-all"># 从 typing 模块导入 Annotated，用于类型注解（标记类型元数据）
from typing import Annotated
from langchain_deepseek import ChatDeepSeek
# 导入 Tavily 搜索工具的结果类，用于处理搜索结果
from langchain_community.tools.tavily_search import TavilySearchResults
# 导入 LangChain 的基础消息类，用于定义消息结构
from langchain_core.messages import BaseMessage
from typing_extensions import TypedDict

# 导入 LangGraph 的状态图构建类
from langgraph.graph import StateGraph
from langgraph.graph.message import add_messages
# 导入 LangGraph 预构建的工具节点和工具调用条件
from langgraph.prebuilt import ToolNode, tools_condition
import os

os.environ[&#x27;DEEPSEEK_API_KEY&#x27;] = &#x27;你的key&#x27;
os.environ[&quot;TAVILY_API_KEY&quot;] =&quot;你的key&quot;

class State(TypedDict):
    messages: Annotated[list, add_messages]

#创建状态图
graph_builder=StateGraph(State)
#初始化搜索工具
tool =TavilySearchResults(max_results=2)
tools=[tool]
llm =ChatDeepSeek(model=&#x27;deepseek-chat&#x27;)
#将语言模型与工具绑定，使模型能调用工具
llm_with_tools=llm.bind_tools(tools)
def chatbot(state:State):
    return {&quot;messages&quot;:[llm_with_tools.invoke(state[&quot;messages&quot;])]}

#添加聊天机器人节点
graph_builder.add_node(&quot;chatbot&quot;,chatbot)
#创建工具节点(封装Tavily搜索工具)
tool_node=ToolNode(tools=[tool])
#添加工具节点
graph_builder.add_node(&quot;tools&quot;,tool_node)

#添加条件 边： 当聊天机器人节点需要调用工具时(通过 tools_condtion判断),触发工具节点
graph_builder.add_conditional_edges(
    &quot;chatbot&quot;, #源节点，聊天机器人
    tools_condition, #条件函数：判断是否需要调用工具
)
#添加固定边，工具节点处理完成后，返回聊天机器人节点继续处理
graph_builder.add_edge(&quot;tools&quot;,&quot;chatbot&quot;)
#状态图入口为聊天机器人
graph_builder.set_entry_point(&quot;chatbot&quot;)
#编译状态图，生成可执行的图结构
graph =graph_builder.compile()

# 导入 IPython 的图像显示工具（用于在 Jupyter 中展示流程图）
from IPython.display import Image, display

try:
    # 生成状态图的 Mermaid 格式 PNG 图像
    img = Image(graph.get_graph().draw_mermaid_png())
    # 保存图像到文件
    with open(&quot;add_tools_image.png&quot;, &quot;wb&quot;) as f:
        f.write(img.data)
except Exception:
    # 忽略图像生成过程中的异常（如依赖缺失）
    pass

# 定义流式处理函数：接收用户输入，触发状态图运行并流式输出回复
def stream_graph_updates(user_input: str):
    # 调用状态图的 stream 方法，传入用户消息（初始状态为包含用户输入的消息列表）
    for event in graph.stream({&quot;messages&quot;: [{&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: user_input}]}):
        # 遍历状态图更新事件中的值（可能包含多节点输出）
        for value in event.values():
            # 提取最新的一条回复消息并打印
            print(&quot;Assistant:&quot;, value[&quot;messages&quot;][-1].content)

# 主循环：持续接收用户输入并进行交互
while True:
    try:
        # 读取用户输入
        user_input = input(&quot;User: &quot;)
        # 判断是否为退出指令
        if user_input.lower() in [&quot;quit&quot;, &quot;exit&quot;, &quot;q&quot;]:
            print(&quot;Goodbye!&quot;)
            break  # 退出循环

        # 调用流式处理函数处理用户输入
        stream_graph_updates(user_input)
    except:
        # 捕获异常（如输入格式错误），自动触发默认问题
        user_input = &quot;你对 LangGraph 了解多少？&quot;
        print(&quot;User: &quot; + user_input)
        stream_graph_updates(user_input)
        break  # 处理完默认问题后退出循环
</code></pre></details></li></ul><ul id="1e3a551d-035a-8024-ad58-f46630434f8f" class="toggle"><li><details open=""><summary>test03.py</summary><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="1e3a551d-035a-80ea-8b1e-d84245b3a82b" class="code"><code class="language-Python" style="white-space:pre-wrap;word-break:break-all">from typing import Annotated
from langchain_deepseek import ChatDeepSeek
# 从 langchain_community.tools.tavily_search 模块导入 TavilySearchResults，它是一个搜索工具
from langchain_community.tools.tavily_search import TavilySearchResults
# 从 langchain_core.messages 模块导入 BaseMessage，是消息的基类
from langchain_core.messages import BaseMessage
# 从 typing_extensions 模块导入 TypedDict，用于定义类型化字典
from typing_extensions import TypedDict

# 从 langgraph.checkpoint.memory 模块导入 MemorySaver，用于保存和恢复状态图的内存状态
from langgraph.checkpoint.memory import MemorySaver
# 从 langgraph.graph 模块导入 StateGraph，用于构建和管理状态图
from langgraph.graph import StateGraph
# 从 langgraph.graph.message 模块导入 add_messages，用于处理消息添加操作
from langgraph.graph.message import add_messages
# 从 langgraph.prebuilt 模块导入 ToolNode 和 tools_condition，ToolNode 用于创建工具节点，tools_condition 用于判断是否需要调用工具
from langgraph.prebuilt import ToolNode, tools_condition
# 导入 os 模块，用于与操作系统交互，比如设置环境变量
import os

os.environ[&#x27;LANGSMITH_TRACING&#x27;] = &#x27;true&#x27;
os.environ[&#x27;LANGCHAIN_API_KEY&#x27;]=&quot;你的key&quot;
os.environ[&#x27;DEEPSEEK_API_KEY&#x27;] = &#x27;你的key&#x27;
os.environ[&quot;TAVILY_API_KEY&quot;] =&quot;你的key&quot;
os.environ[&#x27;LANGSMITH_PROJECT&#x27;] = &quot;0428&quot;

# 定义一个名为 State 的类型化字典，用于存储状态信息
# messages 字段是一个列表，并且使用 add_messages 进行注解，表明该列表用于存储消息
class State(TypedDict):
    messages: Annotated[list, add_messages]

# 创建一个 StateGraph 实例，以 State 类型作为状态图的状态类型
graph_builder = StateGraph(State)

# 创建一个 TavilySearchResults 工具实例，最多返回 2 条搜索结果
tool = TavilySearchResults(max_results=2)
# 将工具存储在列表中，方便后续使用
tools = [tool]
# 创建一个 ChatDeepSeek 实例，指定使用 deepseek-chat 模型
llm = ChatDeepSeek(model=&quot;deepseek-chat&quot;)
# 将语言模型与工具绑定，使模型可以调用工具
llm_with_tools = llm.bind_tools(tools)

# 定义一个名为 chatbot 的函数，该函数接收一个 State 类型的参数
# 函数的作用是调用绑定了工具的语言模型处理输入的消息，并返回包含处理结果的字典
def chatbot(state: State):
    return {&quot;messages&quot;: [llm_with_tools.invoke(state[&quot;messages&quot;])]}

# 向状态图构建器中添加一个名为 &quot;chatbot&quot; 的节点，该节点对应 chatbot 函数
graph_builder.add_node(&quot;chatbot&quot;, chatbot)

# 创建一个 ToolNode 实例，将之前创建的工具包含在内
tool_node = ToolNode(tools=[tool])
# 向状态图构建器中添加一个名为 &quot;tools&quot; 的节点，该节点对应 tool_node
graph_builder.add_node(&quot;tools&quot;, tool_node)

# 向状态图中添加条件边，当 &quot;chatbot&quot; 节点满足 tools_condition 条件时，会触发相关操作
graph_builder.add_conditional_edges(
    &quot;chatbot&quot;,
    tools_condition,
)
# 向状态图中添加一条边，使 &quot;tools&quot; 节点处理完后可以回到 &quot;chatbot&quot; 节点
graph_builder.add_edge(&quot;tools&quot;, &quot;chatbot&quot;)
# 设置状态图的入口点为 &quot;chatbot&quot; 节点
graph_builder.set_entry_point(&quot;chatbot&quot;)

#聊天机器人现在可以使用工具回答用户问题，但无法记住之前交互的上下文，限制连续多轮对话能力
#LangGraph持久化检查点解决这个问题。 在编译图时，提供“检查器”
#在调用图，提供 线程_id，将每一步自动保存状态
#当使用相同的线程_id调用图时，图将加载保存的状态
#&quot;检查点&quot; 比简单的聊天记录记忆强大很多，允许在任何时间保存和回复复杂状态
#内存检查点器，全部保存在内存中
#生产环境中，可以修改为SqlliteSvaer 连接自己的数据库

memory=MemorySaver()
#编译状态图构建器，指定使用memory，作为检查点保存器
graph =graph_builder.compile(checkpointer=memory)
# 导入 IPython 的图像显示工具（用于在 Jupyter 中展示流程图）
from IPython.display import Image, display

try:
    # 生成状态图的 Mermaid 格式 PNG 图像
    img = Image(graph.get_graph().draw_mermaid_png())
    # 保存图像到文件
    with open(&quot;add_memory_image.png&quot;, &quot;wb&quot;) as f:
        f.write(img.data)
except Exception:
    # 忽略图像生成过程中的异常（如依赖缺失）
    pass
# 定义流式处理函数：接收用户输入，触发状态图运行并流式输出回复
def stream_graph_updates(user_input: str):
    # 调用状态图的 stream 方法，传入用户消息（初始状态为包含用户输入的消息列表）
    for event in graph.stream({&quot;messages&quot;: [{&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: user_input}]}):
        # 遍历状态图更新事件中的值（可能包含多节点输出）
        for value in event.values():
            # 提取最新的一条回复消息并打印
            print(&quot;Assistant:&quot;, value[&quot;messages&quot;][-1].content)


#定义配置字典，包含线程ID信息
config={&quot;configurable&quot;:{&quot;thread_id&quot;:&quot;1&quot;}}
user_input=&quot;你好，我是徐老师&quot;
#调用状态图，传入用户输入的消息和配置信息，以流的方式处理消息
events =graph.stream(
    {&quot;messages&quot;:[{&quot;role&quot;:&quot;user&quot;,&quot;content&quot;:user_input}]},
    config,
    stream_mode=&quot;values&quot;,
)
#遍历处理过程中产生的时间，打印聊天机器人的回复
for event in events:
    event[&quot;messages&quot;][-1].pretty_print()

user_input=&quot;你还记得我的名字吗?&quot;

events =graph.stream(
    {&quot;messages&quot;:[{&quot;role&quot;:&quot;user&quot;,&quot;content&quot;:user_input}]},
    config,
    stream_mode=&quot;values&quot;,
)

for event in events:
    event[&quot;messages&quot;][-1].pretty_print()

config={&quot;configurable&quot;:{&quot;thread_id&quot;:&quot;2&quot;}}

#换了线程id，再问，是否记得名字
user_input=&quot;你还记得我的名字吗?&quot;

events =graph.stream(
    {&quot;messages&quot;:[{&quot;role&quot;:&quot;user&quot;,&quot;content&quot;:user_input}]},
    config,
    stream_mode=&quot;values&quot;,
)

for event in events:
    event[&quot;messages&quot;][-1].pretty_print()











</code></pre></details></li></ul><ul id="1e3a551d-035a-8085-95c3-c9bcc487f492" class="toggle"><li><details open=""><summary>test04.py</summary><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="1e3a551d-035a-8056-9505-f518e32d43ff" class="code"><code class="language-Python" style="white-space:pre-wrap;word-break:break-all">from typing import Annotated
from langchain_deepseek import ChatDeepSeek
from langchain_community.tools.tavily_search import TavilySearchResults
# 从 langchain_core.tools 模块导入 tool 装饰器，用于将函数转换为工具
from langchain_core.tools import tool
from typing_extensions import TypedDict
from langgraph.checkpoint.memory import MemorySaver
# 从 langgraph.graph 模块导入 StateGraph 用于构建状态图，START 和 END 作为状态图的起始和结束标识
from langgraph.graph import StateGraph, START, END
# 从 langgraph.graph.message 模块导入 add_messages，用于消息处理相关操作
from langgraph.graph.message import add_messages
# 从 langgraph.prebuilt 模块导入 ToolNode 和 tools_condition，ToolNode 用于创建工具节点，tools_condition 用于判断是否需要调用工具
from langgraph.prebuilt import ToolNode, tools_condition
# 从 langgraph.types 模块导入 Command 和 interrupt，Command 用于封装命令，interrupt 用于中断当前流程并获取响应
from langgraph.types import Command, interrupt
import os

os.environ[&#x27;DEEPSEEK_API_KEY&#x27;] = &#x27;sk-19a6e044d00c4962b62434ec87302a87&#x27;
os.environ[&quot;TAVILY_API_KEY&quot;] =&quot;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&quot;

# 定义一个类型化字典 State，用于存储状态信息
# messages 字段是一个列表，并且使用 add_messages 进行注解，表明该列表用于存储消息
class State(TypedDict):
    messages: Annotated[list, add_messages]

# 创建一个 StateGraph 实例，指定状态类型为 State
graph_builder = StateGraph(State)

# 使用 tool 装饰器将 human_assistance 函数转换为工具，并指定其描述
@tool(description=&quot;处理需要人工干预的复杂问题，传入用户查询后获取人工回复&quot;)
# 定义 human_assistance 函数，接收一个字符串类型的查询参数，返回一个字符串类型的回复
def human_assistance(query: str) -&gt; str:
    # 调用 interrupt 函数，传入包含查询的字典，获取人工回复
    human_response = interrupt({&quot;query&quot;: query})
    # 返回人工回复中的数据
    return human_response[&quot;data&quot;]

# 创建一个 TavilySearchResults 工具实例，设置最多返回 2 条搜索结果
tool = TavilySearchResults(max_results=2)
# 将 Tavily 搜索工具和人工辅助工具组合成一个工具列表
tools = [tool, human_assistance]
# 创建一个 ChatDeepSeek 实例，指定使用 deepseek - chat 模型
llm = ChatDeepSeek(model=&quot;deepseek-chat&quot;)
# 将语言模型与工具列表绑定，使模型可以调用这些工具
llm_with_tools = llm.bind_tools(tools)

# 定义 chatbot 函数，接收一个 State 类型的参数
def chatbot(state: State):
    # 调用绑定了工具的语言模型处理输入的消息列表
    message = llm_with_tools.invoke(state[&quot;messages&quot;])
    # 断言消息中的工具调用数量不超过 1 个
    assert(len(message.tool_calls) &lt;= 1)
    # 返回包含处理后消息的字典
    return {&quot;messages&quot;: [message]}

# 向状态图构建器中添加一个名为 &quot;chatbot&quot; 的节点，该节点对应 chatbot 函数
graph_builder.add_node(&quot;chatbot&quot;, chatbot)

# 创建一个 ToolNode 实例，包含之前定义的工具列表
tool_node = ToolNode(tools=tools)
# 向状态图构建器中添加一个名为 &quot;tools&quot; 的节点，该节点对应 tool_node
graph_builder.add_node(&quot;tools&quot;, tool_node)

# 向状态图中添加条件边，当 &quot;chatbot&quot; 节点满足 tools_condition 条件时，会触发相关操作
graph_builder.add_conditional_edges(
    &quot;chatbot&quot;,
    tools_condition,
)
# 向状态图中添加一条边，使 &quot;tools&quot; 节点处理完后可以回到 &quot;chatbot&quot; 节点
graph_builder.add_edge(&quot;tools&quot;, &quot;chatbot&quot;)
# 向状态图中添加一条从起始标识到 &quot;chatbot&quot; 节点的边
graph_builder.add_edge(START, &quot;chatbot&quot;)

# 创建一个 MemorySaver 实例，用于保存和恢复状态
memory = MemorySaver()
# 编译状态图构建器，生成最终的状态图，并指定使用 memory 作为检查点保存器
graph = graph_builder.compile(checkpointer=memory)

# 从 IPython.display 模块导入 Image 和 display，用于显示图像
from IPython.display import Image, display

try:
    # 获取状态图的图形表示，并将其转换为 PNG 图像
    img = Image(graph.get_graph().draw_mermaid_png())
    # 将图像数据保存为名为 Human-in-the-loop_image.png 的文件
    with open(&quot;Human-in-the-loop_image.png&quot;, &quot;wb&quot;) as f:
        f.write(img.data)
except Exception:
    # 如果在生成或保存图像时发生异常，忽略该异常
    pass

# 定义用户输入的消息
user_input = &quot;我需要一些专家指导来构建 AI Agent。你能帮助我吗？&quot;
# 定义配置字典，包含线程 ID 信息
config = {&quot;configurable&quot;: {&quot;thread_id&quot;: &quot;1&quot;}}

# 调用状态图的 stream 方法，传入用户输入的消息和配置信息，以流式方式处理消息
events = graph.stream(
    {&quot;messages&quot;: [{&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: user_input}]},
    config,
    stream_mode=&quot;values&quot;,
)
# 遍历处理过程中产生的事件
for event in events:
    # 如果事件中包含 &quot;messages&quot; 字段
    if &quot;messages&quot; in event:
        # 打印消息列表中最后一条消息
        event[&quot;messages&quot;][-1].pretty_print()

# 以下两行代码被注释掉，若取消注释可获取状态图的状态快照并打印下一个状态
# snapshot = graph.get_state(config)
# print(snapshot.next)

# 定义人工回复的内容
human_response = (
    &quot;我们，随时为您提供帮助！我们建议您查看 LangGraph 来构建您的Agent。&quot;
    &quot;它比简单的自主代理更可靠、可扩展得多。&quot;
)

# 创建一个 Command 实例，用于恢复处理流程，并传入人工回复的数据
human_command = Command(resume={&quot;data&quot;: human_response})

# 调用状态图的 stream 方法，传入人工命令和配置信息，以流式方式处理
events = graph.stream(human_command, config, stream_mode=&quot;values&quot;)
# 遍历处理过程中产生的事件
for event in events:
    # 如果事件中包含 &quot;messages&quot; 字段
    if &quot;messages&quot; in event:
        # 打印消息列表中最后一条消息
        event[&quot;messages&quot;][-1].pretty_print()</code></pre></details></li></ul><ul id="1e3a551d-035a-804e-b1ca-f1f422889c69" class="toggle"><li><details open=""><summary>test05.py</summary><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="1e3a551d-035a-80d1-9091-fc48799bb7dc" class="code"><code class="language-Python" style="white-space:pre-wrap;word-break:break-all"># 从 typing 模块导入 Annotated，用于类型注解（标记类型元数据）
from typing import Annotated
from langchain_deepseek import ChatDeepSeek
# 导入 Tavily 搜索工具的结果类，用于处理搜索结果
from langchain_community.tools.tavily_search import TavilySearchResults
# 导入 LangChain 的基础消息类，用于定义消息结构
from langchain_core.messages import BaseMessage
from typing_extensions import TypedDict

# 导入 LangGraph 的状态图构建类
from langgraph.graph import StateGraph
from langgraph.graph.message import add_messages
# 导入 LangGraph 预构建的工具节点和工具调用条件
from langgraph.prebuilt import ToolNode, tools_condition
import os

os.environ[&#x27;DEEPSEEK_API_KEY&#x27;] = &#x27;你的key&#x27;
os.environ[&quot;TAVILY_API_KEY&quot;] =&quot;你的key&quot;

class State(TypedDict):
    messages: Annotated[list, add_messages]

#创建状态图
graph_builder=StateGraph(State)
#初始化搜索工具
tool =TavilySearchResults(max_results=2)
tools=[tool]
llm =ChatDeepSeek(model=&#x27;deepseek-chat&#x27;)
#将语言模型与工具绑定，使模型能调用工具
llm_with_tools=llm.bind_tools(tools)
def chatbot(state:State):
    return {&quot;messages&quot;:[llm_with_tools.invoke(state[&quot;messages&quot;])]}

#添加聊天机器人节点
graph_builder.add_node(&quot;chatbot&quot;,chatbot)
#创建工具节点(封装Tavily搜索工具)
tool_node=ToolNode(tools=[tool])
#添加工具节点
graph_builder.add_node(&quot;tools&quot;,tool_node)

#添加条件 边： 当聊天机器人节点需要调用工具时(通过 tools_condtion判断),触发工具节点
graph_builder.add_conditional_edges(
    &quot;chatbot&quot;, #源节点，聊天机器人
    tools_condition, #条件函数：判断是否需要调用工具
)
#添加固定边，工具节点处理完成后，返回聊天机器人节点继续处理
graph_builder.add_edge(&quot;tools&quot;,&quot;chatbot&quot;)
#状态图入口为聊天机器人
graph_builder.set_entry_point(&quot;chatbot&quot;)
#编译状态图，生成可执行的图结构
graph =graph_builder.compile()

# 导入 IPython 的图像显示工具（用于在 Jupyter 中展示流程图）
from IPython.display import Image, display

try:
    # 生成状态图的 Mermaid 格式 PNG 图像
    img = Image(graph.get_graph().draw_mermaid_png())
    # 保存图像到文件
    with open(&quot;add_tools_image.png&quot;, &quot;wb&quot;) as f:
        f.write(img.data)
except Exception:
    # 忽略图像生成过程中的异常（如依赖缺失）
    pass

# 定义流式处理函数：接收用户输入，触发状态图运行并流式输出回复
def stream_graph_updates(user_input: str):
    # 调用状态图的 stream 方法，传入用户消息（初始状态为包含用户输入的消息列表）
    for event in graph.stream({&quot;messages&quot;: [{&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: user_input}]}):
        # 遍历状态图更新事件中的值（可能包含多节点输出）
        for value in event.values():
            # 提取最新的一条回复消息并打印
            print(&quot;Assistant:&quot;, value[&quot;messages&quot;][-1].content)

# 主循环：持续接收用户输入并进行交互
while True:
    try:
        # 读取用户输入
        user_input = input(&quot;User: &quot;)
        # 判断是否为退出指令
        if user_input.lower() in [&quot;quit&quot;, &quot;exit&quot;, &quot;q&quot;]:
            print(&quot;Goodbye!&quot;)
            break  # 退出循环

        # 调用流式处理函数处理用户输入
        stream_graph_updates(user_input)
    except:
        # 捕获异常（如输入格式错误），自动触发默认问题
        user_input = &quot;你对 LangGraph 了解多少？&quot;
        print(&quot;User: &quot; + user_input)
        stream_graph_updates(user_input)
        break  # 处理完默认问题后退出循环
</code></pre></details></li></ul><h2 id="1e3a551d-035a-80ed-86b5-ea8a3f8325bf" class="">Day02 langgraph实现旅行助手</h2><ul id="1eba551d-035a-8012-8447-d9cf15f46c86" class="toggle"><li><details open=""><summary>项目文件</summary><figure id="1eba551d-035a-80d6-9ea6-ea8ee3393b8c"><div class="source"><a href="%E6%97%85%E8%A1%8C%E5%8A%A9%E6%89%8B.rar">旅行助手.rar</a></div></figure></details></li></ul><h2 id="1eba551d-035a-80e0-b378-d794e1f13d9c" class="">Day03 LoRA微调流程</h2><ul id="1eca551d-035a-80b6-89e5-e917e1b94870" class="toggle"><li><details open=""><summary>流程文件</summary><figure id="1eca551d-035a-8020-8f09-f463cf421093"><div class="source"><a href="LoRA%E5%BE%AE%E8%B0%83%E6%A1%88%E4%BE%8B.rar">LoRA微调案例.rar</a></div></figure></details></li></ul><p id="1eca551d-035a-8019-9ebb-c1f9f8caac79" class="">
</p></div></article><span class="sans" style="font-size:14px;padding-top:2em"></span></body></html>